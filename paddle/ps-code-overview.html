<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ps - Aller au boulot</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="../index.html"><strong aria-hidden="true">1.</strong> welcome</a></li><li class="chapter-item "><a href="../chronicles/2024mar.html"><strong aria-hidden="true">2.</strong> chronicles</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../chronicles/2024feb.html"><strong aria-hidden="true">2.1.</strong> feb 2024</a></li></ol></li><li class="chapter-item "><a href="../survey/papers.html"><strong aria-hidden="true">3.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../survey/pollux.html"><strong aria-hidden="true">3.1.</strong> pollux</a></li><li class="chapter-item "><a href="../survey/adasum.html"><strong aria-hidden="true">3.2.</strong> adasum</a></li><li class="chapter-item "><a href="../survey/adaptation_learning.html"><strong aria-hidden="true">3.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="../survey/gradient_descent.html"><strong aria-hidden="true">3.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="../survey/auto_parallel.html"><strong aria-hidden="true">3.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="../survey/scheduling.html"><strong aria-hidden="true">3.6.</strong> scheduling</a></li><li class="chapter-item "><a href="../survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">3.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../survey/gradient_compression/dgc.html"><strong aria-hidden="true">3.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="../survey/gradient_compression/csc.html"><strong aria-hidden="true">3.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="../survey/flash_attention.html"><strong aria-hidden="true">3.8.</strong> flash attention</a></li><li class="chapter-item "><a href="../survey/lora.html"><strong aria-hidden="true">3.9.</strong> LoRA</a></li></ol></li><li class="chapter-item "><a href="../mathematics/overview.html"><strong aria-hidden="true">4.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../mathematics/topics.html"><strong aria-hidden="true">4.1.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../mathematics/basic.html"><strong aria-hidden="true">4.1.1.</strong> basic</a></li><li class="chapter-item "><a href="../mathematics/entropy.html"><strong aria-hidden="true">4.1.2.</strong> entropy</a></li><li class="chapter-item "><a href="../mathematics/newton.html"><strong aria-hidden="true">4.1.3.</strong> newton</a></li><li class="chapter-item "><a href="../mathematics/regression.html"><strong aria-hidden="true">4.1.4.</strong> regression</a></li><li class="chapter-item "><a href="../mathematics/conjugate_descent.html"><strong aria-hidden="true">4.1.5.</strong> conjugate descent</a></li><li class="chapter-item "><a href="../mathematics/gradient_descent.html"><strong aria-hidden="true">4.1.6.</strong> gradient descent</a></li><li class="chapter-item "><a href="../mathematics/pca.html"><strong aria-hidden="true">4.1.7.</strong> pca</a></li><li class="chapter-item "><a href="../mathematics/support_vector.html"><strong aria-hidden="true">4.1.8.</strong> support vector</a></li><li class="chapter-item "><a href="../mathematics/differentiation.html"><strong aria-hidden="true">4.1.9.</strong> differentiation</a></li><li class="chapter-item "><a href="../mathematics/fourier.html"><strong aria-hidden="true">4.1.10.</strong> fourier</a></li><li class="chapter-item "><a href="../mathematics/kmeans_cos.html"><strong aria-hidden="true">4.1.11.</strong> kmeans</a></li></ol></li><li class="chapter-item "><a href="../wavelets/plan.html"><strong aria-hidden="true">4.2.</strong> wavelets</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../wavelets/plan.html"><strong aria-hidden="true">4.2.1.</strong> plan</a></li><li class="chapter-item "><a href="../wavelets/preliminary.html"><strong aria-hidden="true">4.2.2.</strong> preliminary</a></li><li class="chapter-item "><a href="../wavelets/haar.html"><strong aria-hidden="true">4.2.3.</strong> haar wavelet</a></li><li class="chapter-item "><a href="../wavelets/fourier.html"><strong aria-hidden="true">4.2.4.</strong> fourier analysis</a></li><li class="chapter-item "><a href="../wavelets/uncertainty_principle.html"><strong aria-hidden="true">4.2.5.</strong> uncertainty principle</a></li><li class="chapter-item "><a href="../wavelets/multiresolution.html"><strong aria-hidden="true">4.2.6.</strong> multiresolution</a></li></ol></li></ol></li><li class="chapter-item "><a href="../llm/models.html"><strong aria-hidden="true">5.</strong> models</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../llm/llm.html"><strong aria-hidden="true">5.1.</strong> llm</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../llm/falcon.html"><strong aria-hidden="true">5.1.1.</strong> falcon</a></li><li class="chapter-item "><a href="../llm/llama.html"><strong aria-hidden="true">5.1.2.</strong> llama</a></li></ol></li><li class="chapter-item "><a href="../nlp/nlp.html"><strong aria-hidden="true">5.2.</strong> nlp</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../nlp/transformer.html"><strong aria-hidden="true">5.2.1.</strong> transformer</a></li><li class="chapter-item "><a href="../nlp/models.html"><strong aria-hidden="true">5.2.2.</strong> models</a></li></ol></li><li class="chapter-item "><a href="../llm/peft.html"><strong aria-hidden="true">5.3.</strong> peft</a></li><li class="chapter-item "><a href="../somewhat/github.html"><strong aria-hidden="true">5.4.</strong> trends</a></li></ol></li><li class="chapter-item expanded "><a href="../frameworks/overview.html"><strong aria-hidden="true">6.</strong> frameworks</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../megatron/megatron.html"><strong aria-hidden="true">6.1.</strong> megatron</a></li><li class="chapter-item "><a href="../deepspeed/deepspeed.html"><strong aria-hidden="true">6.2.</strong> deepspeed</a></li><li class="chapter-item "><a href="../pytorch/overview.html"><strong aria-hidden="true">6.3.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../pytorch/tensor.html"><strong aria-hidden="true">6.3.1.</strong> tensor</a></li><li class="chapter-item "><a href="../pytorch/autograd.html"><strong aria-hidden="true">6.3.2.</strong> autograd</a></li><li class="chapter-item "><a href="../pytorch/operator.html"><strong aria-hidden="true">6.3.3.</strong> operator</a></li><li class="chapter-item "><a href="../pytorch/profiler.html"><strong aria-hidden="true">6.3.4.</strong> profiler</a></li><li class="chapter-item "><a href="../pytorch/hook.html"><strong aria-hidden="true">6.3.5.</strong> hook</a></li><li class="chapter-item "><a href="../pytorch/elastic.html"><strong aria-hidden="true">6.3.6.</strong> elastic</a></li><li class="chapter-item "><a href="../pytorch/patch.html"><strong aria-hidden="true">6.3.7.</strong> patch</a></li><li class="chapter-item "><a href="../pytorch/misc.html"><strong aria-hidden="true">6.3.8.</strong> misc</a></li></ol></li><li class="chapter-item expanded "><a href="../paddle/paddle.html"><strong aria-hidden="true">6.4.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../paddle/ps-code-overview.html" class="active"><strong aria-hidden="true">6.4.1.</strong> ps</a></li><li class="chapter-item "><a href="../paddle/framework.html"><strong aria-hidden="true">6.4.2.</strong> framework</a></li><li class="chapter-item "><a href="../paddle/cinn.html"><strong aria-hidden="true">6.4.3.</strong> cinn</a></li><li class="chapter-item "><a href="../paddle/dataloader.html"><strong aria-hidden="true">6.4.4.</strong> dataloader</a></li></ol></li><li class="chapter-item "><a href="../horovod/horovod.html"><strong aria-hidden="true">6.5.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../horovod/run.html"><strong aria-hidden="true">6.5.1.</strong> run</a></li><li class="chapter-item "><a href="../horovod/workflow.html"><strong aria-hidden="true">6.5.2.</strong> workflow</a></li><li class="chapter-item "><a href="../horovod/object.html"><strong aria-hidden="true">6.5.3.</strong> object</a></li><li class="chapter-item "><a href="../horovod/develop.html"><strong aria-hidden="true">6.5.4.</strong> develop</a></li><li class="chapter-item "><a href="../horovod/pytorch.html"><strong aria-hidden="true">6.5.5.</strong> pytorch</a></li><li class="chapter-item "><a href="../horovod/tensorflow.html"><strong aria-hidden="true">6.5.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="../horovod/elastic.html"><strong aria-hidden="true">6.5.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="../ray/ray.html"><strong aria-hidden="true">6.6.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../ray/overview.html"><strong aria-hidden="true">6.6.1.</strong> overview</a></li><li class="chapter-item "><a href="../ray/gcs.html"><strong aria-hidden="true">6.6.2.</strong> gcs</a></li><li class="chapter-item "><a href="../ray/raylet.html"><strong aria-hidden="true">6.6.3.</strong> raylet</a></li><li class="chapter-item "><a href="../ray/api.html"><strong aria-hidden="true">6.6.4.</strong> api</a></li><li class="chapter-item "><a href="../ray/survey.html"><strong aria-hidden="true">6.6.5.</strong> survey</a></li></ol></li><li class="chapter-item "><a href="../somewhat/jax.html"><strong aria-hidden="true">6.7.</strong> jax</a></li><li class="chapter-item "><a href="../somewhat/tvm.html"><strong aria-hidden="true">6.8.</strong> tvm</a></li></ol></li><li class="chapter-item "><a href="../programming/overview.html"><strong aria-hidden="true">7.</strong> programming</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../python/python.html"><strong aria-hidden="true">7.1.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../python/concurrent.html"><strong aria-hidden="true">7.1.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="../python/multiprocessing.html"><strong aria-hidden="true">7.1.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="../python/decorator.html"><strong aria-hidden="true">7.1.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="../tips/tips.html"><strong aria-hidden="true">7.2.</strong> tips</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../tips/enable_shared_from_this.html"><strong aria-hidden="true">7.2.1.</strong> enable_shared_from_this</a></li><li class="chapter-item "><a href="../tips/ip_local_port_range.html"><strong aria-hidden="true">7.2.2.</strong> ip_local_port_range</a></li><li class="chapter-item "><a href="../tips/golang_error.html"><strong aria-hidden="true">7.2.3.</strong> golang error</a></li></ol></li></ol></li><li class="chapter-item "><a href="../infra/overview.html"><strong aria-hidden="true">8.</strong> infrastructure</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../kubernetes/kubernetes.html"><strong aria-hidden="true">8.1.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../kubernetes/scheduler.html"><strong aria-hidden="true">8.1.1.</strong> scheduler</a></li><li class="chapter-item "><a href="../kubernetes/operator.html"><strong aria-hidden="true">8.1.2.</strong> operator</a></li><li class="chapter-item "><a href="../kubernetes/device_plugin.html"><strong aria-hidden="true">8.1.3.</strong> device plugin</a></li><li class="chapter-item "><a href="../kubernetes/docker.html"><strong aria-hidden="true">8.1.4.</strong> docker</a></li><li class="chapter-item "><a href="../kubernetes/install.html"><strong aria-hidden="true">8.1.5.</strong> install</a></li></ol></li><li class="chapter-item "><a href="../nvidia/nvidia.html"><strong aria-hidden="true">8.2.</strong> cuda</a></li><li class="chapter-item "><a href="../infra/ccl.html"><strong aria-hidden="true">8.3.</strong> communication library</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../nvidia/nccl.html"><strong aria-hidden="true">8.3.1.</strong> nccl</a></li><li class="chapter-item "><a href="../somewhat/gloo.html"><strong aria-hidden="true">8.3.2.</strong> gloo</a></li><li class="chapter-item "><a href="../somewhat/mpi.html"><strong aria-hidden="true">8.3.3.</strong> mpi</a></li></ol></li></ol></li><li class="chapter-item "><a href="../notes/index.html"><strong aria-hidden="true">9.</strong> notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../notes/influence_and_persuasion.html"><strong aria-hidden="true">9.1.</strong> influence and persuasion</a></li><li class="chapter-item "><a href="../notes/feynman_technique.html"><strong aria-hidden="true">9.2.</strong> freynman technique</a></li><li class="chapter-item "><a href="../notes/wavelet_tour_signal_processing_sparse.html"><strong aria-hidden="true">9.3.</strong> wavelet signal processing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paddle-ps-代码分析"><a class="header" href="#paddle-ps-代码分析">paddle ps 代码分析</a></h1>
<h2 id="python-前端"><a class="header" href="#python-前端">python 前端</a></h2>
<h3 id="api"><a class="header" href="#api">API</a></h3>
<pre><code class="language-python">import paddle.distributed.fleet as fleet

fleet.init()

if fleet.is_server():
    fleet.init_server()
    fleet.run_server()
elif fleet.is_worker():
    run_worker()
    fleet.stop_worker()

def run_worker():
    # paddle.static.Executor(place)
    exe.run(paddle.static.default_startup_program())
    fleet.init_worker()
    exe.train_from_dataset(...)

# Fin, fleet is optimizer
</code></pre>
<h3 id="fleet-initoptimizer"><a class="header" href="#fleet-initoptimizer">fleet init/optimizer</a></h3>
<pre><code class="language-python"># python/paddle/distributed/fleet/base/fleet_base.py

def init(self, role_maker=None, is_collective=False, strategy=None):
    # 配置之集大成者，就是各种配置，细到训练参数，粗到训练模式，开关
    strategy = DistributedStrategy()
    # role maker 包含分布式信息，基本上对接 launch 信息
    # 也负责初始化如 gloo 之类的工具
    self._role_maker._generate_role()

def minimize(...)
    def _minimize_impl(...)
        # runtime handle 做映射 init_server/_init_server, run_server/_run_server
        self._runtime_handle = RuntimeFactory()._create_runtime(context)
</code></pre>
<h3 id="runtime"><a class="header" href="#runtime">runtime</a></h3>
<pre><code class="language-python"># 使用实例
# python/paddle/distributed/ps/the_one_ps.py
class TheOnePSRuntime(RuntimeBase):
    def __init__(self):
        super(TheOnePSRuntime, self).__init__()
        self._communicator = None
        self._server = None
        self._worker = fluid.core.DistFleetWrapper()
        self._server_sub_program = []
        self._heter_client = None
        self._send_ctx = None
</code></pre>
<h3 id="pybind"><a class="header" href="#pybind">pybind</a></h3>
<pre><code class="language-cpp">void BindDistFleetWrapper(py::module* m) {
  py::class_&lt;FleetWrapper, std::shared_ptr&lt;FleetWrapper&gt;&gt;(*m,
                                                          "DistFleetWrapper")
      .def(py::init([]() { return FleetWrapper::GetInstance(); }))
      .def("load_sparse", &amp;FleetWrapper::LoadSparseOnServer)
      .def("load_model", &amp;FleetWrapper::LoadModel)
      .def("load_one_table", &amp;FleetWrapper::LoadModelOneTable)
      .def("init_server", &amp;FleetWrapper::InitServer)
      .def("run_server",
           (uint64_t (FleetWrapper::*)(void)) &amp; FleetWrapper::RunServer)
      .def("run_server", (uint64_t (FleetWrapper::*)(          // NOLINT
                             const std::string&amp;, uint32_t)) &amp;  // NOLINT
                             FleetWrapper::RunServer)
      .def("init_worker", &amp;FleetWrapper::InitWorker)
      .def("push_dense_params", &amp;FleetWrapper::PushDenseParamSync)
      .def("pull_dense_params", &amp;FleetWrapper::PullDenseVarsSync)
      .def("save_all_model", &amp;FleetWrapper::SaveModel)
      .def("save_one_model", &amp;FleetWrapper::SaveModelOneTable)
      .def("recv_and_save_model", &amp;FleetWrapper::RecvAndSaveTable)
      .def("sparse_table_stat", &amp;FleetWrapper::PrintTableStat)
      .def("stop_server", &amp;FleetWrapper::StopServer)
      .def("stop_worker", &amp;FleetWrapper::FinalizeWorker)
      .def("barrier", &amp;FleetWrapper::BarrierWithTable)
      .def("shrink_sparse_table", &amp;FleetWrapper::ShrinkSparseTable)
      .def("set_clients", &amp;FleetWrapper::SetClients)
      .def("get_client_info", &amp;FleetWrapper::GetClientsInfo)
      .def("create_client2client_connection",
           &amp;FleetWrapper::CreateClient2ClientConnection);
}
</code></pre>
<h2 id="fleet-run_server"><a class="header" href="#fleet-run_server">fleet run_server</a></h2>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_server(self, dirname=None, var_names=None, **kwargs):
        # cpp instance
        self._server = fluid.core.DistFleetWrapper()
        self._server.init_server(server_desc, self.string_hosts, role_id,
                                 trainers, self._server_sub_program)
        # load_sparse 
        for var_name in load_varnames:
            table_id = sparse_table_maps[var_name]
            self._server.load_sparse(dirname, "0", table_id)

    def _run_server(self):
        self._server.run_server(host, int(port))
</code></pre>
<h3 id="fleetwrapper"><a class="header" href="#fleetwrapper">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
// FleetWrapper 层
void FleetWrapper::InitServer(...){
    pserver_ptr_ = std::shared_ptr&lt;paddle::distributed::PSCore&gt;(
        new paddle::distributed::PSCore());
    pserver_ptr_-&gt;init_server(...)
}

uint64_t FleetWrapper::RunServer(...){
    auto ret = pserver_ptr_-&gt;run_server(ip, port);
}

void FleetWrapper::LoadSparseOnServer(...){
    // _server_ptr is PSServer
    pserver_ptr_-&gt;_server_ptr-&gt;table(table_id)-&gt;load(path, meta);
}
</code></pre>
<h3 id="pscore"><a class="header" href="#pscore">PSCore</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_service/service.cc
// PSCore layer

int PSCore::init_server(...){
  _ps_env = paddle::distributed::PaddlePSEnvironment();
  _ps_env.set_ps_servers(host_sign_list, node_num);
  _ps_env.set_trainers(trainers); // 没啥用
  _server_ptr = std::shared_ptr&lt;paddle::distributed::PSServer&gt;(
      paddle::distributed::PSServerFactory::create(_ps_param));
  ret = _server_ptr-&gt;configure(_ps_param, _ps_env, index, server_sub_program);
}

uint64_t PSCore::run_server(const std::string&amp; ip, uint32_t port) {
  return _server_ptr-&gt;start(ip, port);
}
</code></pre>
<h3 id="psserver"><a class="header" href="#psserver">PSServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/server.cc
// PSServer layer

PSServer *PSServerFactory::create(const PSParameter &amp;ps_config) {
    PSServer *server =
      CREATE_PSCORE_CLASS(PSServer, service_param.server_class());
    TableManager::instance().initialize();
}

int32_t PSServer::configure(...){
    // for i in downpour_param.downpour_table_param_size()
    auto *table = CREATE_PSCORE_CLASS(
        Table, downpour_param.downpour_table_param(i).table_class());
    table-&gt;set_program_env(scope_.get(), place_, &amp;server_sub_program);
    table-&gt;set_shard(_rank, shard_num);
    table-&gt;initialize(downpour_param.downpour_table_param(i),
                      config.fs_client_param());
    _table_map[downpour_param.downpour_table_param(i).table_id()].reset(table);

    return initialize();
}
</code></pre>
<h3 id="brpcpsserver"><a class="header" href="#brpcpsserver">BrpcPsServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_server.h
class BrpcPsServer : public PSServer {
    brpc::Server _server;
}

// paddle/fluid/distributed/ps/service/brpc_ps_server.cc
int32_t BrpcPsServer::initialize() {
    auto *service =
      CREATE_PSCORE_CLASS(PsBaseService, service_config.service_class());
    _server.AddService(service, brpc::SERVER_DOESNT_OWN_SERVICE)
}
uint64_t BrpcPsServer::start(const std::string &amp;ip, uint32_t port) {
    auto trainers = _environment-&gt;get_trainers(); // 可以去掉
    _server.Start(ip_port.c_str(), &amp;options)
    _environment-&gt;registe_ps_server(ip, port, _rank);
}
</code></pre>
<h3 id="brpcpsservice"><a class="header" href="#brpcpsservice">BrpcPsService</a></h3>
<pre><code class="language-cpp">class BrpcPsService : public PsBaseService {
  int32_t initialize_shard_info(...)
  int32_t pull_dense(...)
  int32_t push_dense(...)
  int32_t push_dense_param(...)
  int32_t push_sparse_param(...)
  int32_t pull_sparse(...)
  int32_t pull_geo_param(...)
  int32_t barrier(...)
  int32_t push_sparse(...)
  int32_t load_one_table(...)
  int32_t load_all_table(...)
  int32_t save_one_table(...)
  int32_t save_all_table(...)
  int32_t shrink_table(...)
  int32_t clear_one_table(...)
  int32_t clear_all_table(...)
  int32_t stop_server(...)
  int32_t start_profiler(...)
  int32_t stop_profiler(...)
  int32_t print_table_stat(...)
  int32_t push_global_step(...)
}
</code></pre>
<h2 id="fleet-run_worker"><a class="header" href="#fleet-run_worker">fleet run_worker</a></h2>
<h3 id="runtime-1"><a class="header" href="#runtime-1">runtime</a></h3>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_worker(self, scopes=None):
        # in init
        # self._worker = fluid.core.DistFleetWrapper()
        self._worker.init_worker(proto_txt, self.string_hosts, role_id)
        # GEO mode
        self._communicator = Communicator(...)
        self._communicator.init_with_ctx(...)
        # 
        info = self._worker.get_client_info()
        self._worker.set_clients(all_info) # _all_gather info is all_info
        self._worker.create_client2client_connection()
        #
        self._pull_all_dense(scopes, send_ctx, dense_map)
        # GEO mode
        self._communicator.start()    

    def _pull_all_dense(self, scopes, send_ctx, recv_map):
        for name, ctx in send_ctx.items():
            self._worker.pull_dense_params(scope, table_id, var_names)
</code></pre>
<h3 id="init-worker"><a class="header" href="#init-worker">init worker</a></h3>
<h3 id="fleetwrapper-1"><a class="header" href="#fleetwrapper-1">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
void FleetWrapper::InitWorker(...){
    ps_env_.set_ps_servers(&amp;host_sign_list, servers);
    worker_ptr_ = std::shared_ptr&lt;paddle::distributed::PSClient&gt;(
          paddle::distributed::PSClientFactory::create(ps_param));
    worker_ptr_-&gt;configure(ps_param, dense_pull_regions, ps_env_, index);
}

void FleetWrapper::PullDenseVarsSync(...){
    auto status = worker_ptr_-&gt;pull_dense(regions.data(), regions.size(), tid);
    status.wait();
}

int FleetWrapper::SetClients(std::vector&lt;uint64_t&gt;&amp; host_sign_list) {
    return ps_env_.set_ps_clients(host_sign_list.data(), node);
}
void FleetWrapper::CreateClient2ClientConnection() {
    worker_ptr_-&gt;create_client2client_connection(...)
}
</code></pre>
<h3 id="psclient"><a class="header" href="#psclient">PSClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_client.cc

PSClient *PSClientFactory::create(const PSParameter &amp;ps_config) {
    PSClient *client = CREATE_PSCORE_CLASS(PSClient, service_param.client_class());
    TableManager::instance().initialize();
}

int32_t PSClient::configure(...){
    // for i in work_param.downpour_table_param_size()
    auto *accessor = CREATE_PSCORE_CLASS(
        ValueAccessor,
        work_param.downpour_table_param(i).accessor().accessor_class());
    accessor-&gt;configure(work_param.downpour_table_param(i).accessor());
    accessor-&gt;initialize();
    _table_accessors[work_param.downpour_table_param(i).table_id()].reset(accessor);
    return initialize();
}
</code></pre>
<h3 id="brpcpsclient"><a class="header" href="#brpcpsclient">BrpcPsClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc
class BrpcPsClient : public PSClient {
    brpc::Server _server;
    DownpourPsClientService _service;
}

int32_t BrpcPsClient::initialize() {
    // for i in server_list.size()
    _server_channels[i][j].reset(new brpc::Channel());
    _server_channels[i][j]-&gt;Init(server_ip_port.c_str(), "", &amp;options)
    // 启动client探听接口, 并相互建立连接
    start_client_service();
    // 异步push 请求队列初始化
    _push_dense_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;DenseAsyncTask *&gt;();
    _push_sparse_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;SparseAsyncTask *&gt;();
    // 启动异步push线程
    _async_push_sparse_thread = std::thread(std::bind(&amp;BrpcPsClient::push_sparse_task_consume, this));
    // _async_push_sparse_thread.detach();
    _async_push_dense_thread = std::thread(std::bind(&amp;BrpcPsClient::push_dense_task_consume, this));
}

// 启动client端RpcService 用于数据互发等操作
int32_t BrpcPsClient::start_client_service() {
    _service.configure(this, _client_id)
    _server.AddService(&amp;_service, brpc::SERVER_DOESNT_OWN_SERVICE);
    _server.Start(butil::my_ip_cstr(), brpc::PortRange(start_port, max_port), &amp;options)
    _env-&gt;registe_ps_client(...)
}

// how 弹性？？？
int32_t BrpcPsClient::create_client2client_connection(...){
    // for i in client_list.size()
    _client_channels[i].reset(new brpc::Channel());
    _client_channels[i]-&gt;Init(server_ip_port.c_str(), "", &amp;options)
}
</code></pre>
<h3 id="downpourpsclientservice"><a class="header" href="#downpourpsclientservice">DownpourPsClientService</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc

class DownpourPsClientService : public PsService {
    PSClient *_client;
    void service(...)
}
</code></pre>
<h2 id="communicator"><a class="header" href="#communicator">communicator</a></h2>
<pre><code class="language-python"># python/paddle/fluid/communicator.py

class Communicator(object):
    def init_with_ctx(self,...):
        self.communicator_ = core.DistCommunicator(self.mode,...)
    def start(self):
        # Start communicator. Should call before training process.
        self.communicator_.start()
</code></pre>
<h3 id="bind"><a class="header" href="#bind">bind</a></h3>
<pre><code class="language-cpp">// paddle/fluid/pybind/communicator_py.cc
void BindCommunicator(py::module* m) {
  // Communicator is already used by nccl, change to DistCommunicator
  py::class_&lt;Communicator, std::shared_ptr&lt;Communicator&gt;&gt;(*m, "DistCommunicator")
  .def(py::init([](...){Communicator::InitInstance&lt;GeoCommunicator&gt;(...)}
  .def("start", &amp;Communicator::Start)
// paddle/fluid/distributed/ps/service/communicator/communicator.h
static Communicator *InitInstance(...){
    std::call_once(init_flag_, &amp;Communicator::InitWithRpcCtx&lt;T&gt;,...);
}
static void InitWithRpcCtx(...){
    communicator_.reset(new T(std::ref(envs)));
    communicator_-&gt;InitEnvs();
    communicator_-&gt;InitBrpcClient(dist_desc, host_sign_list);
    communicator_-&gt;InitImpl(send_ctx, recv_ctx, recv_scope);
}
</code></pre>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/communicator/communicator.cc
void Communicator::InitBrpcClient(...){
    auto fleet = paddle::distributed::FleetWrapper::GetInstance();
    _worker_ptr = fleet-&gt;worker_ptr_;
}
void AsyncCommunicator::InitImpl(...){
    // for varnames
    send_varname_to_queue_[var_name] = std::make_shared&lt;BlockingQueue&lt;std::shared_ptr&lt;Variable&gt;&gt;&gt;(send_queue_size_);
    send_threadpool_.reset(new ::ThreadPool(thread_pool_size_));
    }

void AsyncCommunicator::Start() {
    main_thread_.reset(new std::thread(std::bind(&amp;AsyncCommunicator::MainThread, this))); // MainThread/RecvThread
}

void AsyncCommunicator::MainThread() {
    while (running_) {
        SendByCommunicator();
        RpcProfilerControl();
    }
}
void AsyncCommunicator::RecvThread() {
    while (running_) {
        RecvByCommunicator();
    }
}
</code></pre>
<h2 id="train_from_dataset"><a class="header" href="#train_from_dataset">train_from_dataset</a></h2>
<pre><code class="language-python"># dataset
dataset = paddle.distributed.InMemoryDataset() # "MultiSlotInMemoryDataFeed"
dataset.load_into_memory()
dataset.init(...)
dataset.set_filelist(train_files_list)

# InMemoryDataset -- MultiSlotInMemoryDataFeed  -- InMemoryDataFeed -- DataFeed
# QueueDataset -- MultiSlotDataFeed -- PrivateQueueDataFeed -- DataFeed
# python/paddle/fluid/executor.py
</code></pre>
<pre><code class="language-python"># class Executor(object):
def train_from_dataset(self,...):
    return self._run_from_dataset(...)

def _run_from_dataset(self,...):
    # dataset
    dataset = paddle.fluid.DatasetFactory().create_dataset(...)
    dataset.set_xxx(...)
    dataset._prepare_to_run()
    # trainer
    scope, trainer = self._prepare_trainer(...)
    trainer._gen_trainer_desc()
    # self._default_executor = core.Executor(p)
    trainer_instance = self._default_executor.init_for_dataset(
                    program.desc, trainer._desc(), scope, dataset.dataset)
    # run
    self._default_executor.run_from_dataset(trainer_instance)

def _prepare_trainer(self,...):
    trainer = TrainerFactory()._create_trainer(program.program._fleet_opt)
    # trainer._set_thread(thread)
</code></pre>
<h3 id="excutor"><a class="header" href="#excutor">excutor</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/executor.cc

std::shared_ptr&lt;TrainerBase&gt; Executor::InitForDataset(...){
  // MultiTrainer
  std::shared_ptr&lt;TrainerBase&gt; trainer;
  trainer = TrainerFactory::CreateTrainer(trainer_desc.class_name());
  // initialize trainer
  trainer-&gt;Initialize(trainer_desc, dataset);
  trainer-&gt;SetScope(scope);
  // prepare training environment and helper environment
  trainer-&gt;InitTrainerEnv(main_program, place_);
  // Try to init other environment
  trainer-&gt;InitOtherEnv(main_program);
}

void Executor::RunFromDataset(std::shared_ptr&lt;TrainerBase&gt; trainer) {
    trainer-&gt;Run();
}
</code></pre>
<h3 id="multitrainer"><a class="header" href="#multitrainer">MultiTrainer</a></h3>
<pre><code class="language-cpp">//paddle/fluid/framework/trainer.h
class MultiTrainer : public TrainerBase {
    std::vector&lt;DataFeed*&gt; readers_;
    std::vector&lt;std::shared_ptr&lt;DeviceWorker&gt;&gt; workers_;
}

// paddle/fluid/framework/multi_trainer.cc
void MultiTrainer::Initialize(const TrainerDesc&amp; trainer_desc, Dataset* dataset) {
    // Dataset -&gt; DataFeed
    const std::vector&lt;paddle::framework::DataFeed*&gt; readers = dataset-&gt;GetReaders();
    thread_num_ = readers.size(); // !!! thread num
    workers_.resize(thread_num_); 
    // for i in thread_num_
    workers_[i] = DeviceWorkerFactory::CreateDeviceWorker(...)
    workers_[i]-&gt;Setxxx()
    workers_[i]-&gt;Initialize(trainer_desc);
    workers_[i]-&gt;SetDataFeed(readers[i]);
}

void MultiTrainer::Run() {
    // for i in thread_num_
    threads_.push_back(std::thread(&amp;DeviceWorker::TrainFiles, workers_[thidx].get()));
    // for th in threads_
    th.join();
}
</code></pre>
<h3 id="hogwildworker"><a class="header" href="#hogwildworker">HogwildWorker</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/device_worker.cc
void DeviceWorker::SetDataFeed(DataFeed* data_feed) {
  device_reader_ = data_feed;
}

// paddle/fluid/framework/hogwild_worker.cc
void HogwildWorker::Initialize(const TrainerDesc &amp;desc) {
}

void HogwildWorker::TrainFiles() {
    device_reader_-&gt;Start();
    while ((cur_batch = device_reader_-&gt;Next()) &gt; 0) {
        // for op in ops_
        op-&gt;Run(*thread_scope_, place_);
    }
}
</code></pre>
<h3 id="multislotinmemorydatafeed"><a class="header" href="#multislotinmemorydatafeed">MultiSlotInMemoryDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class InMemoryDataFeed : public DataFeed {
    // 下面的 channel 赋值在 DatasetImpl&lt;T&gt;::CreateReaders()
    // input 为全局，output 和 consume 独立
    paddle::framework::ChannelObject&lt;T&gt;* input_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* output_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* consume_channel_;
}

bool InMemoryDataFeed&lt;T&gt;::Start() {
    //  input
    channel
    global channel
    input_channel_-&gt;Read(data); 
    output_channel_-&gt;Write(std::move(data));
}

int InMemoryDataFeed&lt;T&gt;::Next() {
    while (index &lt; this-&gt;default_batch_size_) {
        output_channel_-&gt;Get(instance);
        ins_vec.push_back(instance);
        ++index;
        consume_channel_-&gt;Put(std::move(instance));
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotInMemoryDataFeed : public InMemoryDataFeed&lt;Record&gt; {
}
</code></pre>
<h3 id="multislotdatafeed"><a class="header" href="#multislotdatafeed">MultiSlotDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class PrivateQueueDataFeed : public DataFeed {
    std::shared_ptr&lt;paddle::framework::ChannelObject&lt;T&gt;&gt; queue_;
}

bool PrivateQueueDataFeed&lt;T&gt;::Start() {
    read_thread_ = std::thread(&amp;PrivateQueueDataFeed::ReadThread, this);
}
void PrivateQueueDataFeed&lt;T&gt;::ReadThread() {
    while (PickOneFile(&amp;filename)) {
        fp_ = fs_open_read(filename, &amp;err_no, pipe_command_);
        while (ParseOneInstanceFromPipe(&amp;instance)) {
            queue_-&gt;Put(instance);
        }
    }
}
int PrivateQueueDataFeed&lt;T&gt;::Next() {
    while (index &lt; default_batch_size_) {
        queue_-&gt;Get(instance)
        AddInstanceToInsVec(&amp;ins_vec, instance, index++);
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotDataFeed : public PrivateQueueDataFeed&lt;std::vector&lt;MultiSlotType&gt;&gt; {
}
</code></pre>
<h4 id="misc"><a class="header" href="#misc">Misc</a></h4>
<ol>
<li>InMemoryDataset 流程分析</li>
</ol>
<ul>
<li>
<p>LoadIntoMemory 把文件读取进 input_channel_，注意 input_channel_ 是全局共享，由 GetReaders() 返回时设定；</p>
</li>
<li>
<p>Start() 从 input_channel_ 读取一份数据进 output_channel_</p>
</li>
<li>
<p>Next() 从 output_channel_ 取数据进 consume_channel_</p>
</li>
</ul>
<ol start="2">
<li>input_channel_ 在哪里初始化？
data_set.cc 中 DatasetImpl<T>::CreateChannel()，它是全局的，最终调用在 dataset.py 中 self.dataset.create_channel()，所以 InMemoryDataset 有调用，QueueDataset 没有调用</li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../paddle/paddle.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../paddle/framework.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../paddle/paddle.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../paddle/framework.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
