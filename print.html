<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Aller au boulot</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">1.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">1.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">1.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">1.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">1.4.</strong> api</a></li></ol></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">2.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">2.1.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">2.2.</strong> profiler</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">3.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps/ps-code-overview.html"><strong aria-hidden="true">3.1.</strong> ps</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">4.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">4.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/api.html"><strong aria-hidden="true">4.2.</strong> api</a></li><li class="chapter-item "><a href="horovod/workflow.html"><strong aria-hidden="true">4.3.</strong> workflow</a></li><li class="chapter-item "><a href="horovod/object.html"><strong aria-hidden="true">4.4.</strong> object</a></li><li class="chapter-item "><a href="horovod/develop.html"><strong aria-hidden="true">4.5.</strong> develop</a></li></ol></li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">5.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">5.1.</strong> concurrent execution</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="ray"><a class="header" href="#ray">Ray</a></h1>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<ul>
<li><a href="https://github.com/ray-project/ray">Ray Github</a></li>
<li><a href="https://docs.ray.io/en/latest/">Ray Documentation</a></li>
</ul>
<p>源代码和官方文档永远是最好的学习资料，总结和学习笔记能辅助快速理解，抓住重点，提高效率。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<h2 id="build"><a class="header" href="#build">BUILD</a></h2>
<p>Ray 的编译使用 bazel，其中大量的 proto 编译也依赖于此，
具体定义在 <code>src/ray/protobuf/BUILD</code> 文件中，参考 <a href="https://rules-proto-grpc.com/en/latest/example.html">文档</a>.</p>
<h2 id="启动部署"><a class="header" href="#启动部署">启动部署</a></h2>
<p>安装完成后，可以直接运行的相关命令如下</p>
<pre><code class="language-python"># python/setup.py

entry_points={
    &quot;console_scripts&quot;: [
        &quot;ray=ray.scripts.scripts:main&quot;,
        &quot;rllib=ray.rllib.scripts:cli [rllib]&quot;,
        &quot;tune=ray.tune.scripts:cli&quot;,
        &quot;ray-operator=ray.ray_operator.operator:main&quot;,
        &quot;serve=ray.serve.scripts:cli&quot;,
    ]
}
</code></pre>
<p>常驻模式启动 ray 集群</p>
<pre><code class="language-shell"># 启动 head 节点
ray start --head
# 启动 worker 节点
ray start --address=RAY_HEAD_IP:6379

ray start --head --redis-password=&quot;&quot; --port=6389
</code></pre>
<p>start 命令的解析如下</p>
<pre><code class="language-python"># python/ray/scripts/scripts.py

# args 解析用的是 click

@cli.command()
@click.option(&quot;--head&quot;,...)
def start(...,head,...):
    ray_params = ray._private.parameter.RayParams(...)
    if head:
        # 启动 head 节点
        ray_params.update_if_absent(...)
        node = ray.node.Node(ray_params, head=True,...)
    else:
        # 启动 worker 节点
        bootstrap_address = services.canonicalize_bootstrap_address(address)
        ray_params.gcs_address = bootstrap_address
        node = ray.node.Node(ray_params, head=False,...)

cli.add_command(start)
def main():
    return cli()
</code></pre>
<p>可以看出本质上都是初始化了节点 Node 对象，是否为 head 则通过参数指定。</p>
<h3 id="node"><a class="header" href="#node">Node</a></h3>
<p>Node 节点的初始化</p>
<pre><code class="language-python"># python/ray/node.py

class Node:
    def __init__(self, ray_params, head=False,...):
        self.head = head

        if not head:
            # GCS GRPC client, 确保 gcs 已启动
            self.get_gcs_client()

        # 初始化持久化存储
        storage._init_storage(ray_params.storage, is_head=head) 

        if head:
            self.validate_external_storage()

        if ...:
            # 启动 reaper 进程，负责在主进程意外退出后回收进程
            self.start_reaper_process()

        if head:
            self.start_head_processes()
            # 尝试写入 gcs
            self.get_gcs_client().internal_kv_put(...)

        if not connect_only:
            self.start_ray_processes()
            ray._private.services.wait_for_node(...)
</code></pre>
<p>Node 的初始化包括启动 start_head_processes 和 start_ray_processes 两部分。</p>
<pre><code class="language-python"># python/ray/node.py

class Node:
    def start_head_processes(self):
        # 如果使用外部 redis，需要配置
        # 这里的逻辑目前有点 confuse，external 和 local 不够明确
        if self._ray_params.external_addresses is not None:
            self.start_or_configure_redis()
            self.create_redis_client()

        # 启动 gcs，包含 redis 服务, 默认端口 6379
        self.start_gcs_server()

        self.start_ray_client_server()

    def start_or_configure_redis(self):
        # 如果 external 有配置，并不真正启动
        ray._private.services.start_redis(...)

    def start_gcs_server(self):
        process_info = ray._private.services.start_gcs_server(self.redis_address,...)
        # 等待启动
        self.get_gcs_client()

    def start_ray_client_server(self):
        process_info = ray._private.services.start_ray_client_server(self.address, self._node_ip_address, ...)

    def start_ray_processes(self):
        # 启动节点上所有的进程
        self.destroy_external_storage()
        # 启动 raylet
        self.start_raylet(plasma_directory, object_store_memory)

    def start_raylet(self, ...):
        process_info = ray._private.services.start_raylet(
            self.redis_address,
            self.gcs_address,
            self._node_ip_address,
            ...)

</code></pre>
<ul>
<li>Head 节点启动 gcs 服务和 ray_client 服务</li>
<li>Head 和 Worker 节点都启动 raylet 服务</li>
</ul>
<p>这些服务的具体启动都被封装在 services 里。</p>
<h3 id="services"><a class="header" href="#services">Services</a></h3>
<p>Services 提供多种服务启动的封装，包括 redis 服务启动。</p>
<blockquote>
<p>1.11 之前的版本 ray 通过启动 redis-server 二进制启动 redis，新版本中已经移除。</p>
</blockquote>
<pre><code class="language-python"># python/ray/_private/services.py

def start_gcs_server(redis_address, ...):
    # 调用 gcs 二进制启动服务，包含 redis 服务
    # GCS_SERVER_EXECUTABLE &quot;core/src/ray/gcs/gcs_server&quot;
    command = [GCS_SERVER_EXECUTABLE, &quot;--redis_xxxx=&quot;, ...]
    process_info = start_ray_process(command, ...)

def start_ray_client_server(address, ray_client_server_ip,...):
    command = [
        sys.executable,    # python
        setup_worker_path, # ray/workers/setup_worker.py
        &quot;-m&quot;,
        &quot;ray.util.client.server&quot;,
        ...]
    process_info = start_ray_process(command, ...)

def start_raylet(redis_address, gcs_address, ...):
    # 启动 raylet，包括 local scheduler 和 object manager
    # 支持 python、java 和 cpp
    # RAYLET_EXECUTABLE &quot;core/src/ray/raylet/raylet&quot;
    command = [
        RAYLET_EXECUTABLE,
        f&quot;--python_worker_command={subprocess.list2cmdline(start_worker_command)}&quot;,  # noqa
        f&quot;--java_worker_command={subprocess.list2cmdline(java_worker_command)}&quot;,  # noqa
        f&quot;--cpp_worker_command={subprocess.list2cmdline(cpp_worker_command)}&quot;,  # noqa
        ...]
    command.append(&quot;--agent_command={}&quot;.format(subprocess.list2cmdline(agent_command)))
    process_info = start_ray_process(command, ...)

def start_ray_process(command, ...):
    process = ConsolePopen(
        command,
        env=modified_env,
        cwd=cwd,
        stdout=stdout_file,
        stderr=stderr_file,
        stdin=subprocess.PIPE if pipe_stdin else None,
        preexec_fn=preexec_fn if sys.platform != &quot;win32&quot; else None,
        creationflags=CREATE_SUSPENDED if win32_fate_sharing else 0,
    )

class ConsolePopen(subprocess.Popen):
    pass

</code></pre>
<h3 id="setup-worker--runtime-env-context"><a class="header" href="#setup-worker--runtime-env-context">setup worker &amp; runtime env context</a></h3>
<p>setup worker 的作用是执行不同的程序，</p>
<pre><code class="language-python"># ray/workers/setup_worker.py

parser.add_argument(&quot;--serialized-runtime-env-context&quot;,...)
parser.add_argument(&quot;--language&quot;, ...)

if __name__ == &quot;__main__&quot;:
    args, remaining_args = parser.parse_known_args()
    runtime_env_context = RuntimeEnvContext.deserialize(
        args.serialized_runtime_env_context or &quot;{}&quot;
    )
    runtime_env_context.exec_worker(remaining_args, Language.Value(args.language))
</code></pre>
<pre><code class="language-python"># python/ray/_private/runtime_env/context.py

class RuntimeEnvContext:
    def exec_worker(self, passthrough_args: List[str], language: Language):
        os.environ.update(self.env_vars)

        # exec [python] passthrough_args
        command_str = &quot; &amp;&amp; &quot;.join(...)

        if sys.platform == &quot;win32&quot;:
            os.system(command_str)
        else:
            os.execvp(&quot;bash&quot;, args=[&quot;bash&quot;, &quot;-c&quot;, command_str])
</code></pre>
<h3 id="client-server"><a class="header" href="#client-server">client server</a></h3>
<pre><code class="language-bash">python -m ray.util.client.server --address=x.x.x.x:6379 --host=0.0.0.0 --port=10001 --mode=proxy --redis-password=
</code></pre>
<pre><code class="language-python"># python/ray/util/client/server/__main__.py
# -&gt; server.py main
# python/ray/util/client/server/server.py

def main():
    server = serve(hostport, ray_connect_handler)
    while True:
        ray.experimental.internal_kv._internal_kv_put(..., HEALTHCHECK)

def serve(connection_str, ray_connect_handler=None):
    server = grpc.server(
        futures.ThreadPoolExecutor(
            max_workers=CLIENT_SERVER_MAX_THREADS,
            thread_name_prefix=&quot;ray_client_server&quot;,
        ),
    )
    # mode proxy
    task_servicer = RayletServicerProxy(None, proxy_manager)
    data_servicer = DataServicerProxy(proxy_manager)
    logs_servicer = LogstreamServicerProxy(proxy_manager)
    # else
    task_servicer = RayletServicer(ray_connect_handler)
    data_servicer = DataServicer(task_servicer)
    logs_servicer = LogstreamServicer()
    ray_client_pb2_grpc.add_RayletDriverServicer_to_server(task_servicer, server)
    ray_client_pb2_grpc.add_RayletDataStreamerServicer_to_server(data_servicer, server)
    ray_client_pb2_grpc.add_RayletLogStreamerServicer_to_server(logs_servicer, server)
    server.start()
</code></pre>
<pre><code class="language-python"># python/ray/util/client/server/server.py

# class RayletServicerProxy(ray_client_pb2_grpc.RayletDriverServicer):
class RayletServicer(ray_client_pb2_grpc.RayletDriverServicer):
    def KVPut(self, request, context=None) -&gt; ray_client_pb2.KVPutResponse:
    def KVGet(self, request, context=None) -&gt; ray_client_pb2.KVGetResponse:
    def KVDel(self, request, context=None) -&gt; ray_client_pb2.KVDelResponse:
    def KVList(self, request, context=None) -&gt; ray_client_pb2.KVListResponse:
    def ListNamedActors(...):
    def ClusterInfo(self, request, context=None) -&gt; ray_client_pb2.ClusterInfoResponse:
    def release(self, client_id: str, id: bytes) -&gt; bool:
    def release_all(self, client_id):
    def Terminate(self, req, context=None):
    def GetObject(self, request: ray_client_pb2.GetRequest, context):
    def PutObject( self, request: ray_client_pb2.PutRequest, context=None) -&gt; ray_client_pb2.PutResponse:
    def WaitObject(self, request, context=None) -&gt; ray_client_pb2.WaitResponse:
    def Schedule( self, task: ray_client_pb2.ClientTask, context=None) -&gt; ray_client_pb2.ClientTaskTicket:
    def lookup_or_register_func( self, id: bytes, client_id: str, options: Optional[Dict]) -&gt; ray.remote_function.RemoteFunction:
    def lookup_or_register_actor( self, id: bytes, client_id: str, options: Optional[Dict]):
    def unify_and_track_outputs(self, output, client_id):
</code></pre>
<h3 id="总结"><a class="header" href="#总结">总结</a></h3>
<p><code>ray start [--head]</code>, 将启动以下进程</p>
<pre><code class="language-shell"># 以下进程只在 head 节点运行
# GCS_SERVER_EXECUTABLE 
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/gcs/gcs_server 
/usr/bin/python3.7 -m ray.util.client.server --host=0.0.0.0 --port=10001 --mode=proxy

# worker 进程
# RAYLET_EXECUTABLE
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/raylet/raylet
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/dashboard/agent.py

/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/monitor.py
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/dashboard/dashboard.py
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/_private/log_monitor.py
</code></pre>
<pre><code class="language-shell"># 新版本中的以下进程已被移除
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:6379
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:64712
</code></pre>
<pre><code class="language-shell"># java worker command
python ray/workers/setup_worker.py java -Dx=x -cp xx RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker

#  cpp worker command
cpp/default_worker
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="gcs-server"><a class="header" href="#gcs-server">GCS Server</a></h1>
<p>服务由以下二进制启动</p>
<pre><code class="language-shell">&quot;core/src/ray/gcs/gcs_server&quot;
</code></pre>
<p>程序入口</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_server_main.cc

int main(int argc, char *argv[]) {
  // 初始化配置
  RayConfig::instance().initialize(config_list);
  // 启动 IO service
  // class instrumented_io_context : public boost::asio::io_context {...}
  instrumented_io_context main_service;
  boost::asio::io_service::work work(main_service);
  // 初始化状态模块
  ray::stats::Init(global_tags, metrics_agent_port);

  // 启动 grpc 服务
  ray::gcs::GcsServerConfig gcs_server_config;
  gcs_server_config.grpc_server_name = &quot;GcsServer&quot;;
  ray::gcs::GcsServer gcs_server(gcs_server_config, main_service);
  gcs_server.Start();

  main_service.run();
}
</code></pre>
<p>主服务</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_server.cc

// 服务初始化，根据配置使用外置 redis 存储或者内置存储
GcsServer::GcsServer(...) {
  if (storage_type_ == &quot;redis&quot;) {
    gcs_table_storage_ = std::make_shared&lt;gcs::RedisGcsTableStorage&gt;(GetOrConnectRedis());
  } else if (storage_type_ == &quot;memory&quot;) {
    gcs_table_storage_ = std::make_shared&lt;InMemoryGcsTableStorage&gt;(main_service_);
  }
}

void GcsServer::Start() {
  // 异步加载 gcs tables 数据
  auto gcs_init_data = std::make_shared&lt;GcsInitData&gt;(gcs_table_storage_);
  gcs_init_data-&gt;AsyncLoad([this, gcs_init_data] { DoStart(*gcs_init_data); });
}

void GcsServer::DoStart(const GcsInitData &amp;gcs_init_data) {
  // Init cluster resource scheduler.
  // Init gcs resource manager.
  // Init synchronization service
  // Init gcs node manager.
  // Init gcs heartbeat manager.
  // Init KV Manager
  // Init function manager
  // Init Pub/Sub handler
  // Init RuntimeENv manager
  // Init gcs job manager.
  // Init gcs placement group manager.
  // Init gcs actor manager.
  // Init gcs worker manager.
  // Init stats handler.
  // Install event listeners.

  // 启动 rpc 服务，依赖 tables 数据加载完成
  // rpc::GrpcServer rpc_server_;
  rpc_server_.Run();
  // 心跳服务
  gcs_heartbeat_manager_-&gt;Start();

  RecordMetrics();
}
</code></pre>
<p>Table storage</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_table_storage.h

class GcsTable {}

class GcsTableWithJobId : public GcsTable&lt;Key, Data&gt; {}

class GcsTableStorage {};

class RedisGcsTableStorage : public GcsTableStorage {};

class InMemoryGcsTableStorage : public GcsTableStorage {};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="raylet"><a class="header" href="#raylet">Raylet</a></h1>
<p>服务由以下二进制启动</p>
<pre><code class="language-shell">core/src/ray/raylet/raylet
</code></pre>
<p>程序入口</p>
<pre><code class="language-cpp">// src/ray/raylet/main.cc

int main(int argc, char *argv[]) {

  // 启动 IO service
  // class instrumented_io_context : public boost::asio::io_context {...}
  instrumented_io_context main_service;
  boost::asio::io_service::work main_work(main_service);

  std::unique_ptr&lt;ray::raylet::Raylet&gt; raylet;
  ray::raylet::NodeManagerConfig node_manager_config;
  ray::ObjectManagerConfig object_manager_config;
  raylet = std::make_unique&lt;ray::raylet::Raylet&gt;(main_service,
                                                 raylet_socket_name,
                                                 node_ip_address,
                                                 node_manager_config,
                                                 object_manager_config,
                                                 gcs_client,
                                                 metrics_export_port);
  raylet-&gt;Start();
  main_service.run();
}

</code></pre>
<p>主服务类</p>
<pre><code class="language-cpp">// src/ray/raylet/raylet.cc

class Raylet {
  // 用于和 gcs 连接的客户端
  std::shared_ptr&lt;gcs::GcsClient&gt; gcs_client_;
  NodeManager node_manager_;
}

void Raylet::Start() {
  RAY_CHECK_OK(RegisterGcs());

  // Start listening for clients.
  DoAccept();
}

ray::Status Raylet::RegisterGcs() {
  node_manager_.RegisterGcs();

  gcs_client_-&gt;Nodes().RegisterSelf(self_node_info_, register_callback);
}

void Raylet::DoAccept() {
  acceptor_.async_accept(
      socket_,
      boost::bind(&amp;Raylet::HandleAccept, this, boost::asio::placeholders::error));
}

void Raylet::HandleAccept(const boost::system::error_code &amp;error) {
  // 建立本地连接并分发到 node manager 处理
  auto new_connection = ClientConnection::Create(
      client_handler, // node_manager_.ProcessNewClient(client);
      message_handler, // node_manager_.ProcessClientMessage(client, message_type, message.data());
      std::move(socket_),
      &quot;worker&quot;,
      node_manager_message_enum,
      static_cast&lt;int64_t&gt;(protocol::MessageType::DisconnectClient),
      message_data);
  // 处理连接
  DoAccept();
}
</code></pre>
<p>Node Manager</p>
<p>NodeManager 本身是一个 ServiceHandler，所以在初始化 node_manager_service_ 时，使用 this 作为 handler 传递。</p>
<pre><code class="language-cpp">// src/ray/raylet/node_manager.h

class NodeManager : public rpc::NodeManagerServiceHandler {
  std::shared_ptr&lt;gcs::GcsClient&gt; gcs_client_;
  std::unique_ptr&lt;HeartbeatSender&gt; heartbeat_sender_;
  WorkerPool worker_pool_;
  ObjectManager object_manager_;
  rpc::GrpcServer node_manager_server_;
  rpc::NodeManagerGrpcService node_manager_service_;

  std::unique_ptr&lt;rpc::AgentManagerServiceHandler&gt; agent_manager_service_handler_;
  rpc::AgentManagerGrpcService agent_manager_service_;

  std::shared_ptr&lt;ClusterResourceScheduler&gt; cluster_resource_scheduler_;
  std::shared_ptr&lt;LocalTaskManager&gt; local_task_manager_;

  std::shared_ptr&lt;PlacementGroupResourceManager&gt; placement_group_resource_manager_;
}

// src/ray/raylet/node_manager.cc

// Push
// Pull

NodeManager::NodeManager(...) {
  // 非常多的初始化配置
  node_manager_service_(io_service, *this),
  // 然后注册服务并启动
  node_manager_server_.RegisterService(node_manager_service_);
  node_manager_server_.RegisterService(agent_manager_service_);
  node_manager_server_.Run();
}
</code></pre>
<p>NodeManager 的 rpc 接口</p>
<pre><code class="language-cpp">// src/ray/rpc/node_manager/node_manager_server.h

// `NodeManagerService` 的接口, 对应 `src/ray/protobuf/node_manager.proto`.
class NodeManagerServiceHandler {}
// 目前有以下接口
// UpdateResourceUsage
// RequestResourceReport
// RequestWorkerLease
// ReportWorkerBacklog
// ReturnWorker
// ReleaseUnusedWorkers
// CancelWorkerLease
// PinObjectIDs
// GetNodeStats
// GlobalGC
// FormatGlobalMemoryInfo
// PrepareBundleResources
// CommitBundleResources
// CancelResourceReserve
// RequestObjectSpillage
// ReleaseUnusedBundles
// GetSystemConfig
// GetGcsServerAddress
// ShutdownRaylet

class NodeManagerGrpcService : public GrpcService {
  NodeManagerGrpcService(instrumented_io_context &amp;io_service,
                         NodeManagerServiceHandler &amp;service_handler)
}
</code></pre>
<blockquote>
<p>关于怎么增加新的接口可以参考: <code>src/ray/core_worker/core_worker.h</code>.</p>
</blockquote>
<p>ObjectManager </p>
<pre><code class="language-cpp">// src/ray/object_manager/object_manager.h

class ObjectManager : public ObjectManagerInterface, public rpc::ObjectManagerServiceHandler {
  instrumented_io_context rpc_service_;
  boost::asio::io_service::work rpc_work_;

  rpc::GrpcServer object_manager_server_;
  rpc::ObjectManagerGrpcService object_manager_service_;
}

// 主要接口
// Push
// Pull

// src/ray/object_manager/object_manager.cc

ObjectManager::ObjectManager(...){
  rpc_work_(rpc_service_),
  object_manager_server_(&quot;ObjectManager&quot;,...)
  object_manager_service_(rpc_service_, *this),
  StartRpcService();
}

void ObjectManager::StartRpcService() {
  // for i in config_.rpc_service_threads_number
  rpc_threads_[i] = std::thread(&amp;ObjectManager::RunRpcService, this, i);
  object_manager_server_.RegisterService(object_manager_service_);
  object_manager_server_.Run();
}

void ObjectManager::RunRpcService(int index) {
  rpc_service_.run();
}
</code></pre>
<pre><code class="language-cpp">// src/ray/rpc/object_manager/object_manager_server.h


class ObjectManagerGrpcService : public GrpcService {
  ObjectManagerGrpcService(instrumented_io_context &amp;io_service,
                           ObjectManagerServiceHandler &amp;service_handler)
      : GrpcService(io_service), service_handler_(service_handler){};
</code></pre>
<p>Worker Pool</p>
<pre><code class="language-cpp">// src/ray/raylet/worker_pool.cc
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="api"><a class="header" href="#api">API</a></h1>
<pre><code class="language-python">import ray
ray.init()
# ray.init(address='ray://localhost:10001')

@ray.remote
def f(x):
    return x * x

futures = [f.remote(i) for i in range(4)]
print(ray.get(futures))
</code></pre>
<pre><code class="language-python"># python/ray/__init__.py

from ray.worker import (  # noqa: E402,F401
    get,
    init,
    put,
    remote,
    wait,
)
</code></pre>
<p>API <code>ray.init</code>, <code>ray.remote</code>, <code>ray.get</code> 都来自 <code>ray.worker</code>.</p>
<pre><code class="language-python"># python/ray/worker.py

def init(address: Optional[str] = None, ...):
    # if address
    builder = ray.client(address, _deprecation_warn_enabled=False)
    builder._init_args(**passed_kwargs)
    return builder.connect()

    # if bootstrap_address is None:
    _global_node = ray.node.Node(head=True, shutdown_at_exit=False, spawn_reaper=True, ray_params=ray_params)
    # else
    _global_node = ray.node.Node(ray_params, head=False, shutdown_at_exit=False, spawn_reaper=False, connect_only=True)

    connect(...)
    return RayContext(...)

def connect(node, worker=global_worker, ...):
    worker.node = node
    worker.core_worker = ray._raylet.CoreWorker(...)
</code></pre>
<p>CoreWorker</p>
<pre><code class="language-cpp">// src/ray/core_worker/core_worker.h

class CoreWorker : public rpc::CoreWorkerServiceHandler {
  instrumented_io_context io_service_;
  boost::asio::io_service::work io_work_;

  rpc::CoreWorkerGrpcService grpc_service_;
  std::unique_ptr&lt;rpc::GrpcServer&gt; core_worker_server_;
  
  // std::unique_ptr&lt;ObjectRecoveryManager&gt; object_recovery_manager_;

  std::shared_ptr&lt;TaskManager&gt; task_manager_;
  std::unique_ptr&lt;ActorManager&gt; actor_manager_;

  /// Implements gRPC server handler.
  void HandleXxxx(const rpc::PushTaskRequest &amp;request,
                      rpc::PushTaskReply *reply,
                      rpc::SendReplyCallback send_reply_callback) override;
}


// src/ray/core_worker/core_worker.cc

CoreWorker::CoreWorker(...) {
  io_work_(io_service_),
  grpc_service_(io_service_, *this),

  core_worker_server_-&gt;RegisterService(grpc_service_);
  core_worker_server_-&gt;Run();
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="pytorch"><a class="header" href="#pytorch">pytorch</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="tensor"><a class="header" href="#tensor">tensor</a></h1>
<p>The dependancy of tensor related API</p>
<p><img src="pytorch/tensor-api.png" alt="Tensor View API Dependance" /></p>
<p>Tensor view explication</p>
<p><img src="pytorch/tensor-view.png" alt="Tensor View API Dependance" /></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="pytorch-profiler"><a class="header" href="#pytorch-profiler">PyTorch Profiler</a></h1>
<h2 id="demo"><a class="header" href="#demo">Demo</a></h2>
<p>体现基本流程的示例</p>
<pre><code class="language-python">import torch
import torchvision.models as models
from torch.profiler import profile, record_function, ProfilerActivity

# 创建模型，需要 profile 的对象
model = models.resnet18()
inputs = torch.randn(5, 3, 224, 224)

# 配置
prof = profile(activities=[ProfilerActivity.CPU], record_shapes=True)

prof.start()

model(inputs)

prof.stop()

# 结果分析和输出
print(prof.key_averages().table(sort_by=&quot;cpu_time_total&quot;, row_limit=10))

prof.export_chrome_trace(&quot;trace.json&quot;)
</code></pre>
<p>其中 <code>torch.profiler.profile</code> 可以使用 <code>with</code> 语法</p>
<pre><code class="language-python">with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
    with record_function(&quot;model_inference&quot;):
        model(inputs)
</code></pre>
<p>输出</p>
<pre><code class="language-shell">---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                 model_inferencex         0.10%      15.353ms       100.00%       14.951s       14.951s             1
                 aten::batch_norm         0.00%     292.000us        43.14%        6.449s     322.464ms            20
     aten::_batch_norm_impl_index         0.00%     567.000us        43.13%        6.449s     322.450ms            20
          aten::native_batch_norm        32.92%        4.921s        43.13%        6.448s     322.419ms            20
                     aten::conv2d         0.00%     310.000us        42.00%        6.279s     313.938ms            20
                aten::convolution         0.00%     350.000us        41.99%        6.278s     313.923ms            20
               aten::_convolution         0.00%     601.000us        41.99%        6.278s     313.905ms            20
         aten::mkldnn_convolution        41.98%        6.276s        41.99%        6.278s     313.875ms            20
                       aten::mean         0.01%       1.209ms        10.54%        1.576s      75.043ms            21
                        aten::sum        10.45%        1.562s        10.45%        1.562s      74.386ms            21
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 14.951s
</code></pre>
<h3 id="api-1"><a class="header" href="#api-1">API</a></h3>
<p><code>torch.profiler.profile</code></p>
<p>参数说明</p>
<ul>
<li><code>activities</code> list 类型，profile 的内容，支持 <code>torch.profiler.ProfilerActivity.CPU</code> 和 <code>torch.profiler.ProfilerActivity.CUDA</code>，这里的设置需要和模型使用的 device 一致</li>
<li><code>schedule</code> 默认会持续记录所有事件，使用 scheduler() 作为帮助函数生成 schedule 函数，自定义记录逻辑</li>
<li><code>on_trace_ready</code> 配合 <code>schedule</code> 使用，在它返回 <code>ProfilerAction.RECORD_AND_SAVE</code> 后被调用</li>
<li><code>record_shapes</code> 是否记录 input shapes</li>
<li><code>profile_memory</code> 是否记录 内存/显存, 和 <code>activities</code> 对应</li>
<li><code>with_stack</code> 是否开启调用文件信息源的记录，包括代码文件和行号</li>
<li><code>with_flops</code> 预估FLOPs，主要针对 matrix multiplication and 2D convolution</li>
<li><code>with_modules</code> 层级记录，暂时只针对 TorchScript</li>
</ul>
<p><code>ProfilerAction</code> 用于状态的记录和转换</p>
<pre><code class="language-python">class ProfilerAction(Enum):
    NONE = 0
    WARMUP = 1
    RECORD = 2
    RECORD_AND_SAVE = 3
</code></pre>
<p><code>profile</code> 对象</p>
<pre><code class="language-python"># torch/profiler/profiler.py

# Profiler context manager
class profile(_KinetoProfile):

    def __init__(...):
        # 记录函数
        self.step_rec_fn: Optional[prof.record_function] = None

        # 状态转换时会触发一系列操作，action_map 记录里任意两个状态转换时执行的动作
        self.action_map: Dict[Tuple[ProfilerAction, Optional[ProfilerAction]], List[Any]] = {
            (ProfilerAction.NONE, ProfilerAction.WARMUP): [self.prepare_trace],
            (ProfilerAction.NONE, ProfilerAction.RECORD): [self.prepare_trace, self.start_trace],
            ...
        }

    def start(self):
        self._transit_action(ProfilerAction.NONE, self.current_action)
        if self.record_steps:
            self.step_rec_fn = prof.record_function(&quot;ProfilerStep#&quot; + str(self.step_num))
            self.step_rec_fn.__enter__()

    def stop(self):
        if self.record_steps and self.step_rec_fn:
            self.step_rec_fn.__exit__(None, None, None)
        self._transit_action(self.current_action, None)

    def step(self):
        self.step_num += 1
        # schedule 接受 step 数，返回当前 action
        self.current_action = self.schedule(self.step_num)

        # 转换状态，触发 map 中定义的动作
        self._transit_action(prev_action, self.current_action)

        if self.record_steps:
            self.step_rec_fn = prof.record_function(&quot;ProfilerStep#&quot; + str(cur_step))
            self.step_rec_fn.__enter__()
</code></pre>
<p><code>schedule</code></p>
<pre><code class="language-python">def schedule(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -&gt; Callable:
    # skip_fist + ( wait + warmup + active ) * repeat
    # NONE                 WARMUP   RECORD RECORD_AND_SAVE
    def schedule_fn(step: int) -&gt; ProfilerAction:
        # 根据 step 返回 当前的状态
    return schedule_fn
</code></pre>
<p><code>record_function</code></p>
<pre><code class="language-python"># torch/autograd/profiler.py

class record_function(ContextDecorator):
    def __init__(self, name: str, args: Optional[str] = None):
        self.record = torch.jit.annotate(Optional[&quot;torch.classes.profiler._RecordFunction&quot;], None)

    def __enter__(self):
        self.record = torch.ops.profiler._record_function_enter_new(self.name, self.args)

    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):
        torch.ops.profiler._record_function_exit(self.record)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paddle"><a class="header" href="#paddle">paddle</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paddle-ps-代码分析"><a class="header" href="#paddle-ps-代码分析">paddle ps 代码分析</a></h1>
<h2 id="python-前端"><a class="header" href="#python-前端">python 前端</a></h2>
<h3 id="api-2"><a class="header" href="#api-2">API</a></h3>
<pre><code class="language-python">import paddle.distributed.fleet as fleet

fleet.init()

if fleet.is_server():
    fleet.init_server()
    fleet.run_server()
elif fleet.is_worker():
    run_worker()
    fleet.stop_worker()

def run_worker():
    # paddle.static.Executor(place)
    exe.run(paddle.static.default_startup_program())
    fleet.init_worker()
    exe.train_from_dataset(...)

# Fin, fleet is optimizer
</code></pre>
<h3 id="fleet-initoptimizer"><a class="header" href="#fleet-initoptimizer">fleet init/optimizer</a></h3>
<pre><code class="language-python"># python/paddle/distributed/fleet/base/fleet_base.py

def init(self, role_maker=None, is_collective=False, strategy=None):
    # 配置之集大成者，就是各种配置，细到训练参数，粗到训练模式，开关
    strategy = DistributedStrategy()
    # role maker 包含分布式信息，基本上对接 launch 信息
    # 也负责初始化如 gloo 之类的工具
    self._role_maker._generate_role()

def minimize(...)
    def _minimize_impl(...)
        # runtime handle 做映射 init_server/_init_server, run_server/_run_server
        self._runtime_handle = RuntimeFactory()._create_runtime(context)
</code></pre>
<h3 id="runtime"><a class="header" href="#runtime">runtime</a></h3>
<pre><code class="language-python"># 使用实例
# python/paddle/distributed/ps/the_one_ps.py
class TheOnePSRuntime(RuntimeBase):
    def __init__(self):
        super(TheOnePSRuntime, self).__init__()
        self._communicator = None
        self._server = None
        self._worker = fluid.core.DistFleetWrapper()
        self._server_sub_program = []
        self._heter_client = None
        self._send_ctx = None
</code></pre>
<h3 id="pybind"><a class="header" href="#pybind">pybind</a></h3>
<pre><code class="language-cpp">void BindDistFleetWrapper(py::module* m) {
  py::class_&lt;FleetWrapper, std::shared_ptr&lt;FleetWrapper&gt;&gt;(*m,
                                                          &quot;DistFleetWrapper&quot;)
      .def(py::init([]() { return FleetWrapper::GetInstance(); }))
      .def(&quot;load_sparse&quot;, &amp;FleetWrapper::LoadSparseOnServer)
      .def(&quot;load_model&quot;, &amp;FleetWrapper::LoadModel)
      .def(&quot;load_one_table&quot;, &amp;FleetWrapper::LoadModelOneTable)
      .def(&quot;init_server&quot;, &amp;FleetWrapper::InitServer)
      .def(&quot;run_server&quot;,
           (uint64_t (FleetWrapper::*)(void)) &amp; FleetWrapper::RunServer)
      .def(&quot;run_server&quot;, (uint64_t (FleetWrapper::*)(          // NOLINT
                             const std::string&amp;, uint32_t)) &amp;  // NOLINT
                             FleetWrapper::RunServer)
      .def(&quot;init_worker&quot;, &amp;FleetWrapper::InitWorker)
      .def(&quot;push_dense_params&quot;, &amp;FleetWrapper::PushDenseParamSync)
      .def(&quot;pull_dense_params&quot;, &amp;FleetWrapper::PullDenseVarsSync)
      .def(&quot;save_all_model&quot;, &amp;FleetWrapper::SaveModel)
      .def(&quot;save_one_model&quot;, &amp;FleetWrapper::SaveModelOneTable)
      .def(&quot;recv_and_save_model&quot;, &amp;FleetWrapper::RecvAndSaveTable)
      .def(&quot;sparse_table_stat&quot;, &amp;FleetWrapper::PrintTableStat)
      .def(&quot;stop_server&quot;, &amp;FleetWrapper::StopServer)
      .def(&quot;stop_worker&quot;, &amp;FleetWrapper::FinalizeWorker)
      .def(&quot;barrier&quot;, &amp;FleetWrapper::BarrierWithTable)
      .def(&quot;shrink_sparse_table&quot;, &amp;FleetWrapper::ShrinkSparseTable)
      .def(&quot;set_clients&quot;, &amp;FleetWrapper::SetClients)
      .def(&quot;get_client_info&quot;, &amp;FleetWrapper::GetClientsInfo)
      .def(&quot;create_client2client_connection&quot;,
           &amp;FleetWrapper::CreateClient2ClientConnection);
}
</code></pre>
<h2 id="fleet-run_server"><a class="header" href="#fleet-run_server">fleet run_server</a></h2>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_server(self, dirname=None, var_names=None, **kwargs):
        # cpp instance
        self._server = fluid.core.DistFleetWrapper()
        self._server.init_server(server_desc, self.string_hosts, role_id,
                                 trainers, self._server_sub_program)
        # load_sparse 
        for var_name in load_varnames:
            table_id = sparse_table_maps[var_name]
            self._server.load_sparse(dirname, &quot;0&quot;, table_id)

    def _run_server(self):
        self._server.run_server(host, int(port))
</code></pre>
<h3 id="fleetwrapper"><a class="header" href="#fleetwrapper">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
// FleetWrapper 层
void FleetWrapper::InitServer(...){
    pserver_ptr_ = std::shared_ptr&lt;paddle::distributed::PSCore&gt;(
        new paddle::distributed::PSCore());
    pserver_ptr_-&gt;init_server(...)
}

uint64_t FleetWrapper::RunServer(...){
    auto ret = pserver_ptr_-&gt;run_server(ip, port);
}

void FleetWrapper::LoadSparseOnServer(...){
    // _server_ptr is PSServer
    pserver_ptr_-&gt;_server_ptr-&gt;table(table_id)-&gt;load(path, meta);
}
</code></pre>
<h3 id="pscore"><a class="header" href="#pscore">PSCore</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_service/service.cc
// PSCore layer

int PSCore::init_server(...){
  _ps_env = paddle::distributed::PaddlePSEnvironment();
  _ps_env.set_ps_servers(host_sign_list, node_num);
  _ps_env.set_trainers(trainers); // 没啥用
  _server_ptr = std::shared_ptr&lt;paddle::distributed::PSServer&gt;(
      paddle::distributed::PSServerFactory::create(_ps_param));
  ret = _server_ptr-&gt;configure(_ps_param, _ps_env, index, server_sub_program);
}

uint64_t PSCore::run_server(const std::string&amp; ip, uint32_t port) {
  return _server_ptr-&gt;start(ip, port);
}
</code></pre>
<h3 id="psserver"><a class="header" href="#psserver">PSServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/server.cc
// PSServer layer

PSServer *PSServerFactory::create(const PSParameter &amp;ps_config) {
    PSServer *server =
      CREATE_PSCORE_CLASS(PSServer, service_param.server_class());
    TableManager::instance().initialize();
}

int32_t PSServer::configure(...){
    // for i in downpour_param.downpour_table_param_size()
    auto *table = CREATE_PSCORE_CLASS(
        Table, downpour_param.downpour_table_param(i).table_class());
    table-&gt;set_program_env(scope_.get(), place_, &amp;server_sub_program);
    table-&gt;set_shard(_rank, shard_num);
    table-&gt;initialize(downpour_param.downpour_table_param(i),
                      config.fs_client_param());
    _table_map[downpour_param.downpour_table_param(i).table_id()].reset(table);

    return initialize();
}
</code></pre>
<h3 id="brpcpsserver"><a class="header" href="#brpcpsserver">BrpcPsServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_server.h
class BrpcPsServer : public PSServer {
    brpc::Server _server;
}

// paddle/fluid/distributed/ps/service/brpc_ps_server.cc
int32_t BrpcPsServer::initialize() {
    auto *service =
      CREATE_PSCORE_CLASS(PsBaseService, service_config.service_class());
    _server.AddService(service, brpc::SERVER_DOESNT_OWN_SERVICE)
}
uint64_t BrpcPsServer::start(const std::string &amp;ip, uint32_t port) {
    auto trainers = _environment-&gt;get_trainers(); // 可以去掉
    _server.Start(ip_port.c_str(), &amp;options)
    _environment-&gt;registe_ps_server(ip, port, _rank);
}
</code></pre>
<h3 id="brpcpsservice"><a class="header" href="#brpcpsservice">BrpcPsService</a></h3>
<pre><code class="language-cpp">class BrpcPsService : public PsBaseService {
  int32_t initialize_shard_info(...)
  int32_t pull_dense(...)
  int32_t push_dense(...)
  int32_t push_dense_param(...)
  int32_t push_sparse_param(...)
  int32_t pull_sparse(...)
  int32_t pull_geo_param(...)
  int32_t barrier(...)
  int32_t push_sparse(...)
  int32_t load_one_table(...)
  int32_t load_all_table(...)
  int32_t save_one_table(...)
  int32_t save_all_table(...)
  int32_t shrink_table(...)
  int32_t clear_one_table(...)
  int32_t clear_all_table(...)
  int32_t stop_server(...)
  int32_t start_profiler(...)
  int32_t stop_profiler(...)
  int32_t print_table_stat(...)
  int32_t push_global_step(...)
}
</code></pre>
<h2 id="fleet-run_worker"><a class="header" href="#fleet-run_worker">fleet run_worker</a></h2>
<h3 id="runtime-1"><a class="header" href="#runtime-1">runtime</a></h3>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_worker(self, scopes=None):
        # in init
        # self._worker = fluid.core.DistFleetWrapper()
        self._worker.init_worker(proto_txt, self.string_hosts, role_id)
        # GEO mode
        self._communicator = Communicator(...)
        self._communicator.init_with_ctx(...)
        # 
        info = self._worker.get_client_info()
        self._worker.set_clients(all_info) # _all_gather info is all_info
        self._worker.create_client2client_connection()
        #
        self._pull_all_dense(scopes, send_ctx, dense_map)
        # GEO mode
        self._communicator.start()    

    def _pull_all_dense(self, scopes, send_ctx, recv_map):
        for name, ctx in send_ctx.items():
            self._worker.pull_dense_params(scope, table_id, var_names)
</code></pre>
<h3 id="init-worker"><a class="header" href="#init-worker">init worker</a></h3>
<h3 id="fleetwrapper-1"><a class="header" href="#fleetwrapper-1">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
void FleetWrapper::InitWorker(...){
    ps_env_.set_ps_servers(&amp;host_sign_list, servers);
    worker_ptr_ = std::shared_ptr&lt;paddle::distributed::PSClient&gt;(
          paddle::distributed::PSClientFactory::create(ps_param));
    worker_ptr_-&gt;configure(ps_param, dense_pull_regions, ps_env_, index);
}

void FleetWrapper::PullDenseVarsSync(...){
    auto status = worker_ptr_-&gt;pull_dense(regions.data(), regions.size(), tid);
    status.wait();
}

int FleetWrapper::SetClients(std::vector&lt;uint64_t&gt;&amp; host_sign_list) {
    return ps_env_.set_ps_clients(host_sign_list.data(), node);
}
void FleetWrapper::CreateClient2ClientConnection() {
    worker_ptr_-&gt;create_client2client_connection(...)
}
</code></pre>
<h3 id="psclient"><a class="header" href="#psclient">PSClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_client.cc

PSClient *PSClientFactory::create(const PSParameter &amp;ps_config) {
    PSClient *client = CREATE_PSCORE_CLASS(PSClient, service_param.client_class());
    TableManager::instance().initialize();
}

int32_t PSClient::configure(...){
    // for i in work_param.downpour_table_param_size()
    auto *accessor = CREATE_PSCORE_CLASS(
        ValueAccessor,
        work_param.downpour_table_param(i).accessor().accessor_class());
    accessor-&gt;configure(work_param.downpour_table_param(i).accessor());
    accessor-&gt;initialize();
    _table_accessors[work_param.downpour_table_param(i).table_id()].reset(accessor);
    return initialize();
}
</code></pre>
<h3 id="brpcpsclient"><a class="header" href="#brpcpsclient">BrpcPsClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc
class BrpcPsClient : public PSClient {
    brpc::Server _server;
    DownpourPsClientService _service;
}

int32_t BrpcPsClient::initialize() {
    // for i in server_list.size()
    _server_channels[i][j].reset(new brpc::Channel());
    _server_channels[i][j]-&gt;Init(server_ip_port.c_str(), &quot;&quot;, &amp;options)
    // 启动client探听接口, 并相互建立连接
    start_client_service();
    // 异步push 请求队列初始化
    _push_dense_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;DenseAsyncTask *&gt;();
    _push_sparse_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;SparseAsyncTask *&gt;();
    // 启动异步push线程
    _async_push_sparse_thread = std::thread(std::bind(&amp;BrpcPsClient::push_sparse_task_consume, this));
    // _async_push_sparse_thread.detach();
    _async_push_dense_thread = std::thread(std::bind(&amp;BrpcPsClient::push_dense_task_consume, this));
}

// 启动client端RpcService 用于数据互发等操作
int32_t BrpcPsClient::start_client_service() {
    _service.configure(this, _client_id)
    _server.AddService(&amp;_service, brpc::SERVER_DOESNT_OWN_SERVICE);
    _server.Start(butil::my_ip_cstr(), brpc::PortRange(start_port, max_port), &amp;options)
    _env-&gt;registe_ps_client(...)
}

// how 弹性？？？
int32_t BrpcPsClient::create_client2client_connection(...){
    // for i in client_list.size()
    _client_channels[i].reset(new brpc::Channel());
    _client_channels[i]-&gt;Init(server_ip_port.c_str(), &quot;&quot;, &amp;options)
}
</code></pre>
<h3 id="downpourpsclientservice"><a class="header" href="#downpourpsclientservice">DownpourPsClientService</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc

class DownpourPsClientService : public PsService {
    PSClient *_client;
    void service(...)
}
</code></pre>
<h2 id="communicator"><a class="header" href="#communicator">communicator</a></h2>
<pre><code class="language-python"># python/paddle/fluid/communicator.py

class Communicator(object):
    def init_with_ctx(self,...):
        self.communicator_ = core.DistCommunicator(self.mode,...)
    def start(self):
        # Start communicator. Should call before training process.
        self.communicator_.start()
</code></pre>
<h3 id="bind"><a class="header" href="#bind">bind</a></h3>
<pre><code class="language-cpp">// paddle/fluid/pybind/communicator_py.cc
void BindCommunicator(py::module* m) {
  // Communicator is already used by nccl, change to DistCommunicator
  py::class_&lt;Communicator, std::shared_ptr&lt;Communicator&gt;&gt;(*m, &quot;DistCommunicator&quot;)
  .def(py::init([](...){Communicator::InitInstance&lt;GeoCommunicator&gt;(...)}
  .def(&quot;start&quot;, &amp;Communicator::Start)
// paddle/fluid/distributed/ps/service/communicator/communicator.h
static Communicator *InitInstance(...){
    std::call_once(init_flag_, &amp;Communicator::InitWithRpcCtx&lt;T&gt;,...);
}
static void InitWithRpcCtx(...){
    communicator_.reset(new T(std::ref(envs)));
    communicator_-&gt;InitEnvs();
    communicator_-&gt;InitBrpcClient(dist_desc, host_sign_list);
    communicator_-&gt;InitImpl(send_ctx, recv_ctx, recv_scope);
}
</code></pre>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/communicator/communicator.cc
void Communicator::InitBrpcClient(...){
    auto fleet = paddle::distributed::FleetWrapper::GetInstance();
    _worker_ptr = fleet-&gt;worker_ptr_;
}
void AsyncCommunicator::InitImpl(...){
    // for varnames
    send_varname_to_queue_[var_name] = std::make_shared&lt;BlockingQueue&lt;std::shared_ptr&lt;Variable&gt;&gt;&gt;(send_queue_size_);
    send_threadpool_.reset(new ::ThreadPool(thread_pool_size_));
    }

void AsyncCommunicator::Start() {
    main_thread_.reset(new std::thread(std::bind(&amp;AsyncCommunicator::MainThread, this))); // MainThread/RecvThread
}

void AsyncCommunicator::MainThread() {
    while (running_) {
        SendByCommunicator();
        RpcProfilerControl();
    }
}
void AsyncCommunicator::RecvThread() {
    while (running_) {
        RecvByCommunicator();
    }
}
</code></pre>
<h2 id="train_from_dataset"><a class="header" href="#train_from_dataset">train_from_dataset</a></h2>
<pre><code class="language-python"># dataset
dataset = paddle.distributed.InMemoryDataset() # &quot;MultiSlotInMemoryDataFeed&quot;
dataset.load_into_memory()
dataset.init(...)
dataset.set_filelist(train_files_list)

# InMemoryDataset -- MultiSlotInMemoryDataFeed  -- InMemoryDataFeed -- DataFeed
# QueueDataset -- MultiSlotDataFeed -- PrivateQueueDataFeed -- DataFeed
# python/paddle/fluid/executor.py
</code></pre>
<pre><code class="language-python"># class Executor(object):
def train_from_dataset(self,...):
    return self._run_from_dataset(...)

def _run_from_dataset(self,...):
    # dataset
    dataset = paddle.fluid.DatasetFactory().create_dataset(...)
    dataset.set_xxx(...)
    dataset._prepare_to_run()
    # trainer
    scope, trainer = self._prepare_trainer(...)
    trainer._gen_trainer_desc()
    # self._default_executor = core.Executor(p)
    trainer_instance = self._default_executor.init_for_dataset(
                    program.desc, trainer._desc(), scope, dataset.dataset)
    # run
    self._default_executor.run_from_dataset(trainer_instance)

def _prepare_trainer(self,...):
    trainer = TrainerFactory()._create_trainer(program.program._fleet_opt)
    # trainer._set_thread(thread)
</code></pre>
<h3 id="excutor"><a class="header" href="#excutor">excutor</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/executor.cc

std::shared_ptr&lt;TrainerBase&gt; Executor::InitForDataset(...){
  // MultiTrainer
  std::shared_ptr&lt;TrainerBase&gt; trainer;
  trainer = TrainerFactory::CreateTrainer(trainer_desc.class_name());
  // initialize trainer
  trainer-&gt;Initialize(trainer_desc, dataset);
  trainer-&gt;SetScope(scope);
  // prepare training environment and helper environment
  trainer-&gt;InitTrainerEnv(main_program, place_);
  // Try to init other environment
  trainer-&gt;InitOtherEnv(main_program);
}

void Executor::RunFromDataset(std::shared_ptr&lt;TrainerBase&gt; trainer) {
    trainer-&gt;Run();
}
</code></pre>
<h3 id="multitrainer"><a class="header" href="#multitrainer">MultiTrainer</a></h3>
<pre><code class="language-cpp">//paddle/fluid/framework/trainer.h
class MultiTrainer : public TrainerBase {
    std::vector&lt;DataFeed*&gt; readers_;
    std::vector&lt;std::shared_ptr&lt;DeviceWorker&gt;&gt; workers_;
}

// paddle/fluid/framework/multi_trainer.cc
void MultiTrainer::Initialize(const TrainerDesc&amp; trainer_desc, Dataset* dataset) {
    // Dataset -&gt; DataFeed
    const std::vector&lt;paddle::framework::DataFeed*&gt; readers = dataset-&gt;GetReaders();
    thread_num_ = readers.size(); // !!! thread num
    workers_.resize(thread_num_); 
    // for i in thread_num_
    workers_[i] = DeviceWorkerFactory::CreateDeviceWorker(...)
    workers_[i]-&gt;Setxxx()
    workers_[i]-&gt;Initialize(trainer_desc);
    workers_[i]-&gt;SetDataFeed(readers[i]);
}

void MultiTrainer::Run() {
    // for i in thread_num_
    threads_.push_back(std::thread(&amp;DeviceWorker::TrainFiles, workers_[thidx].get()));
    // for th in threads_
    th.join();
}
</code></pre>
<h3 id="hogwildworker"><a class="header" href="#hogwildworker">HogwildWorker</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/device_worker.cc
void DeviceWorker::SetDataFeed(DataFeed* data_feed) {
  device_reader_ = data_feed;
}

// paddle/fluid/framework/hogwild_worker.cc
void HogwildWorker::Initialize(const TrainerDesc &amp;desc) {
}

void HogwildWorker::TrainFiles() {
    device_reader_-&gt;Start();
    while ((cur_batch = device_reader_-&gt;Next()) &gt; 0) {
        // for op in ops_
        op-&gt;Run(*thread_scope_, place_);
    }
}
</code></pre>
<h3 id="multislotinmemorydatafeed"><a class="header" href="#multislotinmemorydatafeed">MultiSlotInMemoryDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class InMemoryDataFeed : public DataFeed {
    // 下面的 channel 赋值在 DatasetImpl&lt;T&gt;::CreateReaders()
    // input 为全局，output 和 consume 独立
    paddle::framework::ChannelObject&lt;T&gt;* input_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* output_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* consume_channel_;
}

bool InMemoryDataFeed&lt;T&gt;::Start() {
    //  input
    channel
    global channel
    input_channel_-&gt;Read(data); 
    output_channel_-&gt;Write(std::move(data));
}

int InMemoryDataFeed&lt;T&gt;::Next() {
    while (index &lt; this-&gt;default_batch_size_) {
        output_channel_-&gt;Get(instance);
        ins_vec.push_back(instance);
        ++index;
        consume_channel_-&gt;Put(std::move(instance));
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotInMemoryDataFeed : public InMemoryDataFeed&lt;Record&gt; {
}
</code></pre>
<h3 id="multislotdatafeed"><a class="header" href="#multislotdatafeed">MultiSlotDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class PrivateQueueDataFeed : public DataFeed {
    std::shared_ptr&lt;paddle::framework::ChannelObject&lt;T&gt;&gt; queue_;
}

bool PrivateQueueDataFeed&lt;T&gt;::Start() {
    read_thread_ = std::thread(&amp;PrivateQueueDataFeed::ReadThread, this);
}
void PrivateQueueDataFeed&lt;T&gt;::ReadThread() {
    while (PickOneFile(&amp;filename)) {
        fp_ = fs_open_read(filename, &amp;err_no, pipe_command_);
        while (ParseOneInstanceFromPipe(&amp;instance)) {
            queue_-&gt;Put(instance);
        }
    }
}
int PrivateQueueDataFeed&lt;T&gt;::Next() {
    while (index &lt; default_batch_size_) {
        queue_-&gt;Get(instance)
        AddInstanceToInsVec(&amp;ins_vec, instance, index++);
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotDataFeed : public PrivateQueueDataFeed&lt;std::vector&lt;MultiSlotType&gt;&gt; {
}
</code></pre>
<h4 id="misc"><a class="header" href="#misc">Misc</a></h4>
<ol>
<li>InMemoryDataset 流程分析</li>
</ol>
<ul>
<li>
<p>LoadIntoMemory 把文件读取进 input_channel_，注意 input_channel_ 是全局共享，由 GetReaders() 返回时设定；</p>
</li>
<li>
<p>Start() 从 input_channel_ 读取一份数据进 output_channel_</p>
</li>
<li>
<p>Next() 从 output_channel_ 取数据进 consume_channel_</p>
</li>
</ul>
<ol start="2">
<li>input_channel_ 在哪里初始化？
data_set.cc 中 DatasetImpl<T>::CreateChannel()，它是全局的，最终调用在 dataset.py 中 self.dataset.create_channel()，所以 InMemoryDataset 有调用，QueueDataset 没有调用</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="horovod"><a class="header" href="#horovod">Horovod</a></h1>
<p>Horovod core principles are based on MPI concepts such as size, rank, local rank, allreduce, allgather, broadcast, and alltoall</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="run"><a class="header" href="#run">Run</a></h1>
<h3 id="启动"><a class="header" href="#启动">启动</a></h3>
<pre><code class="language-python">setup(name='horovod',
      entry_points={
          'console_scripts': [
              'horovodrun = horovod.runner.launch:run_commandline'
          ]
      })
</code></pre>
<pre><code class="language-python"># horovod/runner/launch.py

def run_commandline():
    args = parse_args()
    _run(args)

def _run(args):
    # set args.hosts
    if _is_elastic(args):
        return _run_elastic(args)
    else:
        return _run_static(args)
</code></pre>
<h3 id="非弹性启动"><a class="header" href="#非弹性启动">非弹性启动</a></h3>
<pre><code class="language-python">def _run_static(args):
    settings = hvd_settings.Settings(...)
    nics = driver_service.get_common_interfaces(settings, all_host_names,
                                                remote_host_names, fn_cache)
    if args.run_func:
        executable = args.executable or sys.executable
        command = [executable, '-m', 'horovod.runner.run_task', str(driver_ip), str(run_func_server_port)]
    else:
        command = args.command
    _launch_job(args, settings, nics, command)

def _launch_job(args, settings, nics, command):
    def gloo_run_fn():
        driver_ip = network.get_driver_ip(nics)
        gloo_run(settings, nics, env, driver_ip, command)

    def mpi_run_fn():
        mpi_run(settings, nics, env, command)

    def js_run_fn():
        js_run(settings, nics, env, command)

    run_controller(args.use_gloo, gloo_run_fn,
                   args.use_mpi, mpi_run_fn,
                   args.use_jsrun, js_run_fn,
                   args.verbose)

def run_controller(use_gloo, gloo_run, use_mpi, mpi_run, use_jsrun, js_run, verbosity):
    if use_gloo:
        gloo_run()
    elif use_mpi:
        mpi_run()
    elif use_jsrun:
        js_run()

from horovod.runner.gloo_run import gloo_run, gloo_run_elastic
from horovod.runner.mpi_run import mpi_run
from horovod.runner.js_run import js_run, is_jsrun_installed
</code></pre>
<pre><code class="language-python"># horovod/runner/gloo_run.py

def gloo_run(settings, nics, env, server_ip, command):
    # 启动命令通过 ssh 分发，如果出错所有进程将被 kill
    # 先封装执行函数
    exec_command = _exec_command_fn(settings)
    # 再调用执行
    launch_gloo(command, exec_command, settings, nics, env, server_ip)

def _exec_command_fn(settings):
    def _exec_command(command, slot_info, events):
        # 如果是需要分发到 remote 的节点
        # from horovod.runner.util.remote import get_remote_command
        # get_remote_command 提供 ssh 封装
        if host_address not in local_addresses:
            command = get_remote_command(local_command,...)
        exit_code = safe_shell_exec.execute(command,...)
    return _exec_command

def launch_gloo(command, exec_command, settings, nics, env, server_ip):
    # exec_command 为执行的命令
    # args_list 是执行的参数，由每个节点所需参数组成的列表
    # 通过如下方法的调用实现多节点运行
    res = threads.execute_function_multithreaded(exec_command, args_list, block_until_all_done=True)
</code></pre>
<pre><code class="language-python"># horovod/runner/util/threads.py

def execute_function_multithreaded(fn, args_list, block_until_all_done=True, max_concurrent_executions=1000):
    worker_queue = queue.Queue()
    result_queue = queue.Queue() # 结果池，用于放置结果，后续忽略

    # 把任务放进任务池
    for i, arg in enumerate(args_list):
        worker_queue.put(arg)

    # 只要任务池里还有任务就取出来执行之
    def fn_execute():
        while True:
            arg = worker_queue.get(block=False)
            exec_index = arg[-1]
            res = fn(*arg[:-1])

    # 启动多线程分发命令，感觉必要性不大
    for _ in range(number_of_threads):
        thread = in_thread(target=fn_execute, daemon=not block_until_all_done)

def in_thread(target, args=(), name=None, daemon=True, silent=False):
    bg = threading.Thread(target=fn, args=args, name=name)
    bg.daemon = daemon
    bg.start()
</code></pre>
<pre><code class="language-python"># horovod/runner/common/util/safe_shell_exec.py

# 使用 multiprocessing.Process 启动进程
# 然后再使用 subprocess 启动进程执行

def execute(command, env=None, stdout=None, stderr=None, index=None, events=None,
            prefix_output_with_timestamp=False):
    ctx = multiprocessing.get_context('spawn')

    exit_event = _create_event(ctx)

    # 当 parent process 被 hard kill 时，这个 Pipe 会被关闭，然后 middleman 就会向子进程发送 SIGTERM，避免出现 orphaned process
    (r, w) = ctx.Pipe(duplex=False)

    middleman = ctx.Process(target=_exec_middleman, args=(command, env, exit_event, ..., (r, w)))
    middleman.start()

    middleman.join()
    return middleman.exitcode

def _exec_middleman(command, env, exit_event, stdout, stderr, rw):
    os.setsid()

    executor_shell = subprocess.Popen(command, shell=True, env=env,
                                      stdout=stdout_w, stderr=stderr_w)

</code></pre>
<pre><code class="language-python"># horovod/runner/mpi_run.py

def mpi_run(settings, nics, env, command, stdout=None, stderr=None):
    mpirun_command = (
        'mpirun {basic_args} '
        '-np {num_proc}{ppn_arg}{hosts_arg} '
        '{binding_args} '
        '{mpi_args} '
        '{mpi_ssh_args} '
        '{tcp_intf_arg} '
        '{nccl_socket_intf_arg} '
        '{output_filename_arg} '
        '{env} {extra_mpi_args} {command}'
    )

    if settings.run_func_mode:
        exit_code = safe_shell_exec.execute(mpirun_command, env=env, stdout=stdout, stderr=stderr)
    else:
        os.execve('/bin/sh', ['/bin/sh', '-c', mpirun_command], env)
</code></pre>
<h3 id="弹性启动"><a class="header" href="#弹性启动">弹性启动</a></h3>
<pre><code class="language-python">def _run_elastic(args):
    settings = elastic_settings.ElasticSettings(discovery=discover_hosts,...)
    return gloo_run_elastic(settings, env, args.run_func if args.run_func else args.command, executable)

from horovod.runner.gloo_run import gloo_run, gloo_run_elastic
</code></pre>
<pre><code class="language-python"></code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="api-3"><a class="header" href="#api-3">API</a></h1>
<h2 id="horovodbasics"><a class="header" href="#horovodbasics">HorovodBasics</a></h2>
<h3 id="python-api"><a class="header" href="#python-api">Python API</a></h3>
<ul>
<li>horovod 的基础 API，会被具体实现 (torch/tf) 使用</li>
<li>提供 C 接口的 py 封装，通过 ctypes 实现调用</li>
</ul>
<pre><code class="language-python"># horovod/common/basics.py

class HorovodBasics(object):
    def __init__(self, pkg_path, *args):
        # 加载 mpi lib 实现包
        self.MPI_LIB_CTYPES = ctypes.CDLL(full_path, mode=ctypes.RTLD_GLOBAL)

    def init(self, comm, process_sets):
        initialization_ok = self.MPI_LIB_CTYPES.horovod_init(...)
        # initialization_ok = self.MPI_LIB_CTYPES.horovod_init_multi_comm(...)

        _init_process_sets(process_sets)

    def shutdown(self):
    def is_initialized(self):
    def start_timeline(self, file_path, mark_cycles=False):
    def stop_timeline(self):
    def size(self):
    def local_size(self):
    def cross_size(self):
    def rank(self):
    def local_rank(self):
    def cross_rank(self):
    def is_homogeneous(self):
    def mpi_threads_supported(self):
    def mpi_enabled(self):
    def mpi_built(self):
    def gloo_enabled(self):
    def gloo_built(self):
    def nccl_built(self):
    def ddl_built(self):
    def ccl_built(self):
    def cuda_built(self):
    def rocm_built(self):
    def _add_process_set_impl(self, ranks: Sequence[int]) -&gt; Optional[int]:
    def _remove_process_set_impl(self, process_set_id: int) -&gt; Optional[int]:
    def _process_set_rank(self, process_set_id: int) -&gt; int:
    def _process_set_size(self, process_set_id: int) -&gt; int:
    def _get_process_set_ids_and_ranks(self) -&gt; Dict[int, List[int]]:
    def _comm_process_set_id(self, comm: MPI.Comm) -&gt; int:
</code></pre>
<h3 id="c-api"><a class="header" href="#c-api">C API</a></h3>
<p>这里的接口有两个部分</p>
<ul>
<li>系统相关的 C 接口，通过 py 的 ctypes 引用</li>
<li>通信相关的接口，直接被调用</li>
</ul>
<pre><code class="language-c">// horovod/common/operations.h

namespace horovod {
namespace common {

extern &quot;C&quot; {

bool horovod_init(const int* ranks, int nranks, const int* process_set_ranks,
                  const int* process_set_sizes, int num_process_sets);

#if HAVE_MPI
// 使用 MPI communicators 初始化
bool horovod_init_multi_comm(MPI_Comm* comm, int ncomms,
                             const int* process_set_ranks_via_ranks,
                             const int* process_set_sizes_via_ranks,
                             int num_process_sets_via_ranks);
#endif

void horovod_shutdown();

int horovod_rank();
int horovod_local_rank();

int horovod_size();
int horovod_local_size();

// bool horovod_xxx_enabled();
// bool horovod_xxx_built();

int horovod_reduce_op_average();
int horovod_reduce_op_sum();
int horovod_reduce_op_adasum();

int horovod_add_process_set(const int *ranks, int nranks);
int horovod_remove_process_set(int process_set_id);
int horovod_process_set_rank(int process_set_id);
int horovod_process_set_size(int process_set_id);
int horovod_process_set_included(int process_set_id);
int horovod_number_of_process_sets();
void horovod_process_set_ids(int* ids_prealloc);
int horovod_process_set_ranks(int id, int* ranks_prealloc);

} // C API 结束

Status EnqueueTensorAllreduce(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              std::shared_ptr&lt;Tensor&gt; output,
                              ReadyEventList ready_event_list,
                              std::string name, int device,
                              StatusCallback callback,
                              ReduceOp reduce_op = ReduceOp::SUM,
                              double prescale_factor = 1.0,
                              double postscale_factor = 1.0,
                              int32_t process_set_id = 0);

Status EnqueueTensorAllreduces(std::vector&lt;std::shared_ptr&lt;OpContext&gt;&gt;&amp; contexts,
                               std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; tensors,
                               std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; outputs,
                               std::vector&lt;ReadyEventList&gt;&amp; ready_event_lists,
                               std::vector&lt;std::string&gt;&amp; names,
                               int device,
                               std::vector&lt;StatusCallback&gt;&amp; callbacks,
                               ReduceOp reduce_op = ReduceOp::SUM,
                               double prescale_factor = 1.0,
                               double postscale_factor = 1.0,
                               int32_t process_set_id = 0);

Status EnqueueTensorAllgather(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              ReadyEventList ready_event_list,
                              const std::string&amp; name, int device,
                              StatusCallback callback,
                              int32_t process_set_id = 0);

Status EnqueueTensorBroadcast(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              std::shared_ptr&lt;Tensor&gt; output, int root_rank,
                              ReadyEventList ready_event_list,
                              const std::string&amp; name, int device,
                              StatusCallback callback,
                              int32_t process_set_id = 0);

Status EnqueueTensorAlltoall(std::shared_ptr&lt;OpContext&gt; context,
                             std::shared_ptr&lt;Tensor&gt; tensor,
                             std::shared_ptr&lt;Tensor&gt; splits,
                             ReadyEventList ready_event_list,
                             const std::string&amp; name, int device,
                             StatusCallback callback,
                             int32_t process_set_id = 0);

Status EnqueueTensorReducescatter(std::shared_ptr&lt;OpContext&gt; context,
                                  std::shared_ptr&lt;Tensor&gt; tensor,
                                  ReadyEventList ready_event_list,
                                  const std::string&amp; name, int device,
                                  StatusCallback callback,
                                  ReduceOp reduce_op = ReduceOp::SUM,
                                  int32_t process_set_id = 0);

Status EnqueueJoin(std::shared_ptr&lt;OpContext&gt; context,
                   std::shared_ptr&lt;Tensor&gt; output_last_joined_rank,
                   ReadyEventList ready_event_list,
                   const std::string&amp; name, int device,
                   StatusCallback callback,
                   int32_t process_set_id = 0);

Status EnqueueBarrier(StatusCallback callback,
                   int32_t process_set_id = 0);

} // namespace common
} // namespace horovod

#endif // HOROVOD_OPERATIONS_H
</code></pre>
<h2 id="pytorch-api"><a class="header" href="#pytorch-api">PyTorch API</a></h2>
<p><code>horovod.torch</code> api</p>
<p>大部分 api 从 mpi_ops 中引入</p>
<pre><code class="language-python"># horovod/torch/__init__.py

from horovod.torch import elastic
from horovod.torch.compression import Compression
from horovod.torch.functions import allgather_object, broadcast_object, broadcast_optimizer_state, broadcast_parameters
from horovod.torch.mpi_ops import allreduce, allreduce_async, allreduce_, allreduce_async_
from horovod.torch.mpi_ops import grouped_allreduce, grouped_allreduce_async, grouped_allreduce_, grouped_allreduce_async_
from horovod.torch.mpi_ops import sparse_allreduce_async
from horovod.torch.mpi_ops import allgather, allgather_async
from horovod.torch.mpi_ops import broadcast, broadcast_async, broadcast_, broadcast_async_
from horovod.torch.mpi_ops import alltoall, alltoall_async
from horovod.torch.mpi_ops import reducescatter, reducescatter_async
from horovod.torch.mpi_ops import join
from horovod.torch.mpi_ops import barrier
from horovod.torch.mpi_ops import poll, synchronize
from horovod.torch.mpi_ops import init, shutdown
from horovod.torch.mpi_ops import is_initialized, start_timeline, stop_timeline
from horovod.torch.mpi_ops import size, local_size, cross_size, rank, local_rank, cross_rank
from horovod.torch.mpi_ops import mpi_threads_supported, mpi_enabled, mpi_built
from horovod.torch.mpi_ops import gloo_enabled, gloo_built
from horovod.torch.mpi_ops import nccl_built, ddl_built, ccl_built, cuda_built, rocm_built
from horovod.torch.mpi_ops import ProcessSet, global_process_set, add_process_set, remove_process_set
from horovod.torch.mpi_ops import Average, Sum, Adasum
from horovod.torch.optimizer import DistributedOptimizer
from horovod.torch.sync_batch_norm import SyncBatchNorm
</code></pre>
<p><code>mpi_ops</code></p>
<p>这里的 api 分为两部分</p>
<ul>
<li>一部分从 mpi_lib_v2 library 中通过 C api 暴露，然后通过 basic 引入</li>
<li>通信 api 通过 pybind 调用</li>
</ul>
<pre><code class="language-python"># horovod/torch/mpi_ops.py

# so library
from horovod.torch import mpi_lib_v2 as mpi_lib

_basics = _HorovodBasics(__file__, 'mpi_lib_v2')
# import basic methods
# mpi_ops 中会包含 basic 中的 api

# 重要
# handle 会被放在 map 中，避免被 gc
# 在 synchronize 之后被释放
_handle_map = {}

# inplace allreduce
# allreduce_ -&gt; allreduce_async_ + synchronize -&gt; _allreduce_async -&gt; mpi_lib.horovod_torch_allreduce_async_

# allreduce
# allreduce -&gt; HorovodAllreduce.apply -&gt; allreduce_async + synchronize -&gt; _allreduce_async -&gt; mpi_lib.horovod_torch_allreduce_async_

def _allreduce_function_factory(tensor):
    return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')


def _allreduce_async(tensor, output, name, op, prescale_factor, postscale_factor, process_set: ProcessSet):
    function = _check_function(_allreduce_function_factory, tensor)
    try:
        handle = getattr(mpi_lib, function)(tensor, output, divisor,
                                            name.encode() if name is not None else _NULL, op,
                                            prescale_factor, postscale_factor, process_set.process_set_id)
    except RuntimeError as e:
        raise HorovodInternalError(e)
    return handle


def allreduce_async(tensor, average=None, name=None, op=None,
                    prescale_factor=1.0, postscale_factor=1.0,
                    process_set=global_process_set):
    output = tensor.new(tensor.shape)
    return _allreduce_async(tensor, output, name, op, prescale_factor, postscale_factor, process_set)


class HorovodAllreduce(torch.autograd.Function):
    @staticmethod
    def forward(ctx, tensor, average, name, op, prescale_factor, postscale_factor, process_set):
        ctx.average = average
        ctx.op = op
        ctx.prescale_factor = prescale_factor
        ctx.postscale_factor = postscale_factor
        ctx.process_set = process_set
        handle = allreduce_async(tensor, average, name, op, prescale_factor, postscale_factor, process_set)
        return synchronize(handle)

    @staticmethod
    def backward(ctx, grad_output):
        return allreduce(grad_output, average=ctx.average, op=ctx.op,
                         prescale_factor=ctx.prescale_factor,
                         postscale_factor=ctx.postscale_factor,
                         process_set=ctx.process_set), None, None, None, None, None, None


def allreduce(tensor, average=None, name=None, compression=Compression.none, op=None,
              prescale_factor=1.0, postscale_factor=1.0, process_set=global_process_set):
    &quot;&quot;&quot;
    This acts as a thin wrapper around an autograd function.  If your input
    tensor requires gradients, then callings this function will allow gradients
    to be computed and backpropagated.
    &quot;&quot;&quot;
    tensor_compressed, ctx = compression.compress(tensor)
    summed_tensor_compressed = HorovodAllreduce.apply(tensor_compressed, average, name, op,
                                                      prescale_factor, postscale_factor,
                                                      process_set)
    return compression.decompress(summed_tensor_compressed, ctx)


def allreduce_async_(tensor, average=None, name=None, op=None,
                     prescale_factor=1.0, postscale_factor=1.0,
                     process_set=global_process_set):
    op = handle_average_backwards_compatibility(op, average)
    return _allreduce_async(tensor, tensor, name, op, prescale_factor, postscale_factor, process_set)


def allreduce_(tensor, average=None, name=None, op=None,
               prescale_factor=1.0, postscale_factor=1.0,
               process_set=global_process_set):
    handle = allreduce_async_(tensor, average, name, op, prescale_factor, postscale_factor, process_set)
    return synchronize(handle)

</code></pre>
<p>cpp api</p>
<pre><code class="language-cpp">// horovod/torch/mpi_ops_v2.cc

PYBIND11_MODULE(mpi_lib_v2, m) {
  m.def(&quot;horovod_torch_allreduce_async_torch_IntTensor&quot;, &amp;DoAllreduce);
  ...
}

int DoAllreduce(::torch::Tensor tensor, ::torch::Tensor output, int divisor,
                const std::string&amp; name, int reduce_op_int,
                double prescale_factor, double postscale_factor,
                int process_set_id) {
  auto handle = handle_manager.AllocateHandle();
  common::ReadyEventList ready_event_list;
  auto hvd_tensor = std::make_shared&lt;TorchTensor&gt;(tensor);
  auto hvd_context = std::make_shared&lt;TorchOpContext&gt;(device, output);
  auto hvd_output = std::make_shared&lt;TorchTensor&gt;(output);

  ReduceOp reduce_op = static_cast&lt;ReduceOp&gt;(reduce_op_int);
  auto enqueue_result = EnqueueTensorAllreduce(hvd_context, hvd_tensor, hvd_output, ready_event_list, ...);

  return handle;
}
</code></pre>
<ul>
<li><code>EnqueueTensorAllreduce</code> 来自 horovod/common/operations.h</li>
</ul>
<pre><code class="language-cpp">// horovod/torch/adapter_v2.h

class TorchPersistentBuffer : public PersistentBuffer {
  AccessData(std::shared_ptr&lt;OpContext&gt; context) const override;
  ::torch::Tensor tensor_;
};

class TorchTensor : public Tensor {
  ::torch::Tensor tensor_;
};

class TorchOpContext : public OpContext {
  std::vector&lt;::torch::Tensor&gt; outputs_;
};
</code></pre>
<h2 id="tensorflow-api"><a class="header" href="#tensorflow-api">Tensorflow api</a></h2>
<h3 id="demo-1"><a class="header" href="#demo-1">Demo</a></h3>
<pre><code class="language-python">    import tensorflow as tf
    import horovod.tensorflow as hvd


    # Initialize Horovod
    hvd.init()

    # Pin GPU to be used to process local rank (one GPU per process)
    config = tf.ConfigProto()
    config.gpu_options.visible_device_list = str(hvd.local_rank())

    # Build model...
    loss = ...
    opt = tf.train.AdagradOptimizer(0.01 * hvd.size())

    # Add Horovod Distributed Optimizer
    opt = hvd.DistributedOptimizer(opt)

    # Add hook to broadcast variables from rank 0 to all other processes during
    # initialization.
    hooks = [hvd.BroadcastGlobalVariablesHook(0)]

    # Make training operation
    train_op = opt.minimize(loss)

    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.
    checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None

    # The MonitoredTrainingSession takes care of session initialization,
    # restoring from a checkpoint, saving to a checkpoint, and closing when done
    # or an error occurs.
    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,
                                           config=config,
                                           hooks=hooks) as mon_sess:
      while not mon_sess.should_stop():
        # Perform synchronous training.
        mon_sess.run(train_op)

</code></pre>
<pre><code class="language-python"># horovod/tensorflow/mpi_ops.py 

def init(*args, **kwargs):
    _basics.init(*args, **kwargs)
    _setup_process_sets(_basics)

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="workflow"><a class="header" href="#workflow">Workflow</a></h1>
<h3 id="operation"><a class="header" href="#operation">Operation</a></h3>
<p>Horovod 的主要流程都在 <code>horovod/common/operations.cc</code> 中，主线包含两个方面</p>
<ul>
<li>init 接口调用启动后台进程，不断从 tensor_queue 中取出需要通信的 tensor 进行通信并返回结果</li>
<li>用户前端接口调用间接调用 EnqueueTensorAllreduces 以及类似的 API 不断将需要进行通信的 tensor 放入 tensor_queue </li>
</ul>
<h3 id="初始化和出-queue"><a class="header" href="#初始化和出-queue">初始化和出 Queue</a></h3>
<p>初始化接口的具体实现，启动一个后台进程，不断出发执行通信操作</p>
<pre><code class="language-c">// horovod/common/operations.cc

extern &quot;C&quot; {

bool horovod_init(const int* ranks, int nranks, const int* process_set_ranks,
                  const int* process_set_sizes, int num_process_sets) {
  return InitializeHorovodOnce(...);
}

bool horovod_init_multi_comm(MPI_Comm* comm, int ncomms,
                             const int* process_set_ranks_via_ranks,
                             const int* process_set_sizes_via_ranks,
                             int num_process_sets_via_ranks) {
  return InitializeHorovodOnce(std::vector&lt;int&gt;(), process_set_ranks_vecs);
}

// 启动 horovod 后台进程，只执行一次
bool InitializeHorovodOnce(
    const std::vector&lt;int&gt;&amp; ranks,
    const std::vector&lt;std::vector&lt;int&gt;&gt;&amp; process_set_ranks) {
  if (!horovod_global.initialize_flag.test_and_set()) {
    horovod_global.initialization_done = false;
    horovod_global.background_thread =
        std::thread(BackgroundThreadLoop, std::ref(horovod_global));
  }

  while (!horovod_global.initialization_done &amp;&amp;
         !horovod_global.initialization_failed) {
    std::this_thread::sleep_for(std::chrono::milliseconds(1));
  }
}

void BackgroundThreadLoop(HorovodGlobalState&amp; state) {
  auto mpi_ctx_manager = MPIContextManager();
  if (global_mpi_context.IsEnabled()) {
    global_mpi_context.Initialize(mpi_ctx_manager);
    if (state.control_operation == LibType::MPI) {
      // Initializes global controller
      state.process_set_table.Initialize(global_mpi_context);
    }
  }

  bool is_coordinator = state.global_controller-&gt;IsCoordinator();
  bool is_homogeneous = state.global_controller-&gt;IsHomogeneous();
  int size = state.global_controller-&gt;GetSize();
  int local_size = state.global_controller-&gt;GetLocalSize();
  int local_rank = state.global_controller-&gt;GetLocalRank();

  # 一堆配置
  state.parameter_manager.SetTensorFusionThresholdBytes(128 * 1024 * 1024);
  state.parameter_manager.SetTensorFusionThresholdBytes(threshold, true);
  state.parameter_manager.SetCycleTimeMs(1);
  state.parameter_manager.SetCacheEnabled(true);
  state.process_set_table.Get(0).response_cache.set_capacity(...)
  state.parameter_manager.SetHierarchicalAllgather(false);
  state.parameter_manager.SetHierarchicalAllreduce(false);

  while (RunLoopOnce(state));

  state.shut_down = true;

  horovod_global.process_set_table.Finalize(global_mpi_context,...)
}

bool RunLoopOnce(HorovodGlobalState&amp; state) {
  state.process_set_table.InitializeRegisteredAndRemoveMarkedIfReady(global_mpi_context);

  for (auto process_set_id : state.process_set_table.Ids()) {
    auto&amp; process_set = state.process_set_table.Get(process_set_id);
    auto response_list = process_set.IsCurrentProcessIncluded()
            ? process_set.controller-&gt;ComputeResponseList(this_process_requested_shutdown, state, process_set)
            : ResponseList();

    if (process_set.IsCurrentProcessIncluded()) {
      int global_rank = state.global_controller-&gt;GetRank();
      for (auto&amp; response : response_list.responses()) {
        PerformOperation(response, process_set);
      }
    }
  }
}
</code></pre>
<p>这里主要包含两个操作</p>
<ul>
<li>process_set.controller-&gt;ComputeResponseList 处理通信前的协同</li>
<li>PerformOperation 从 process_set 的 tensor_queue 中取出内容执行通信</li>
</ul>
<h3 id="performoperation"><a class="header" href="#performoperation">PerformOperation</a></h3>
<pre><code class="language-cpp">// 执行通信操作，获取 Response 
void PerformOperation(Response response, ProcessSet&amp; process_set) {
  std::vector&lt;TensorTableEntry&gt; entries;
  process_set.tensor_queue.GetTensorEntriesFromResponse(response, entries, process_set.joined);

  if (response.response_type() != Response::JOIN &amp;&amp;
      response.response_type() != Response::BARRIER) {
    if (entries.size() &gt; 1) {
      auto first_entry = entries[0];
      // 创建 buffer
      Status status = horovod_global.fusion_buffer.InitializeBuffer(
          process_set.controller-&gt;TensorFusionThresholdBytes(),
          first_entry.device, first_entry.context,
          horovod_global.current_nccl_stream,
          [&amp;]() { timeline.ActivityStartAll(entries, INIT_FUSION_BUFFER); },
          [&amp;]() { timeline.ActivityEndAll(entries); });
    }
  }

  // std::unique_ptr&lt;OperationManager&gt; op_manager;
  Status status = op_manager-&gt;ExecuteOperation(entries, response, process_set);
}
</code></pre>
<p><em>OperationManager-&gt;ExecuteOperation</em> 即调用对应 api 完成 op 的执行</p>
<h3 id="computeresponselist"><a class="header" href="#computeresponselist">ComputeResponseList</a></h3>
<p>这是 controller 里最重要的函数，它在 worker 间进行 allreduce/allgather 的协同，返回准备好通信的 tensor 列表，其中</p>
<ul>
<li>0 号 worker 作为 coordinator</li>
<li>每个 worker 都存有一份别的 worker 发送的准备好的 tensor 列表作为 cache</li>
</ul>
<p>具体流程如下</p>
<ul>
<li>worker 所有计划的通信操作都会先发送给 coordinator，Request 类型，包括 (tensor, reduce/gather, shape, type)</li>
<li>worker 发送 DONE 消息给 coordinator 当所有计划通信操作都已发送</li>
<li>coordinator 接受来自 worker 的计划通信请求，直到收集到所有节点的 DONE 消息</li>
<li>coordinator 为准备好的 tensor 构建并向 worker 发送 Response 消息，当发送完毕时发送 DONE 消息</li>
<li>worker 监听来自 coordinator 的消息，执行对应的 reduce/gather 操作，直到收到 DONE 消息</li>
</ul>
<pre><code class="language-cpp">// horovod/common/controller.cc

ResponseList Controller::ComputeResponseList(bool this_process_requested_shutdown,
                                             HorovodGlobalState&amp; state,
                                             ProcessSet&amp; process_set) {
  CacheCoordinator cache_coordinator(response_cache_.num_active_bits());

  // tensor_queue_ --&gt; message_queue_tmp
  std::deque&lt;Request&gt; message_queue_tmp;
  tensor_queue_.PopMessagesFromQueue(message_queue_tmp);

  // cache 机制
  // tensor_queue_.PushMessagesToQueue(messages_to_replace);

  ResponseList response_list;

  if (!need_communication) {
    std::deque&lt;Response&gt; responses;
    for (auto bit : cache_coordinator.cache_hits()) {
      responses.push_back(response_cache_.get_response(bit));
    }
    FuseResponses(responses, state, response_list);
  } else {
    std::vector&lt;std::string&gt; ready_to_reduce;

    if (is_coordinator_) { // 0 号 worker
      // message_queue_tmp --&gt; ready_to_reduce
      while (!message_queue_tmp.empty()) {
        Request message = message_queue_tmp.front();
        ready_to_reduce.push_back(message.tensor_name());
      }
      // Receive ready tensors from other ranks
      std::vector&lt;RequestList&gt; ready_list;
      RecvReadyTensors(ready_to_reduce, ready_list); // ready_to_reduce 未实际使用

      // ready_list +-&gt; ready_to_reduce 即把各 worker 收集到的和自己的合并
      for (int i = 1; i &lt; size_; ++i) {
        auto received_message_list = ready_list[i];
        for (auto&amp; received_message : received_message_list.requests()) {
          auto&amp; received_name = received_message.tensor_name();
          ready_to_reduce.push_back(received_name);
        }
      }

      // 到此准备通信的 tensor 准备完毕
      std::deque&lt;Response&gt; responses;

      for (auto&amp; tensor_name : ready_to_reduce) {
        Response response = ConstructResponse(tensor_name, process_set.joined_size);
        responses.push_back(std::move(response));
      }
      FuseResponses(responses, state, response_list);

      // Broadcast final results to other ranks.
      SendFinalTensors(response_list);

    } else { // 非 0 号 worker
      RequestList message_list;
      while (!message_queue_tmp.empty()) {
        message_list.add_request(message_queue_tmp.front());
      }

      // Send ready tensors to rank zero
      SendReadyTensors(message_list);

      // Receive final tensors to be processed from rank zero
      RecvFinalTensors(response_list);
    }
  }

  return response_list;
}
</code></pre>
<h3 id="调用和入-queue"><a class="header" href="#调用和入-queue">调用和入 Queue</a></h3>
<p>主要流程如下</p>
<ul>
<li>通过入参 process_set_id 从 global state 的 process_set_table 中取出 process_set 对象</li>
<li>使用入参 Tensor tensors 和 outputs 封装 Request 和 TensorTableEntry </li>
<li>把上述封装列表添加到 process_set 对象的 tensor_queue 中</li>
</ul>
<pre><code class="language-cpp">// horovod/common/operations.cc

Status
EnqueueTensorAllreduces(std::vector&lt;std::shared_ptr&lt;OpContext&gt;&gt;&amp; contexts,
                        std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; tensors,
                        std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; outputs,
                        std::vector&lt;ReadyEventList&gt;&amp; ready_event_lists,
                        std::vector&lt;std::string&gt;&amp; names, const int device,
                        std::vector&lt;StatusCallback&gt;&amp; callbacks,
                        ReduceOp reduce_op, double prescale_factor,
                        double postscale_factor, int32_t process_set_id) {

  auto&amp; process_set = horovod_global.process_set_table.Get(process_set_id);
  Status status;

  std::vector&lt;Request&gt; messages;
  std::vector&lt;TensorTableEntry&gt; entries;

  for (int n = 0; n &lt; (int)tensors.size(); ++n) {
    Request message;
    message.set_xxxx(...);
    messages.push_back(std::move(message));

    TensorTableEntry e;
    e.tensor = tensors[n];
    e.output = outputs[n];
    e.process_set_id = process_set_id;
    entries.push_back(std::move(e));
  }

  status = process_set.tensor_queue.AddToTensorQueueMulti(entries, messages);
  return status;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="object"><a class="header" href="#object">Object</a></h1>
<p>Object 列表</p>
<ul>
<li>OperationManager </li>
<li>HorovodGlobalState </li>
<li>ParameterManager </li>
<li>FusionBufferManager</li>
<li>ProcessSet &amp; ProcessSetTable</li>
<li>Controller</li>
<li>TensorQueue</li>
<li>GroupTable</li>
</ul>
<h3 id="horovodglobalstate"><a class="header" href="#horovodglobalstate">HorovodGlobalState</a></h3>
<p>全局状态类，用于保存全局信息，同时持有以下几个对象</p>
<pre><code class="language-cpp">// horovod/common/global_state.h

struct HorovodGlobalState {
  // Background thread running MPI communication.
  std::thread background_thread;

  bool elastic_enabled = false;

  ParameterManager parameter_manager;

  // handles resizing and auto-tuning of buffer size.
  FusionBufferManager fusion_buffer;

  ProcessSetTable process_set_table;

  std::shared_ptr&lt;Controller&gt; global_controller;
};
</code></pre>
<h3 id="parametermanager"><a class="header" href="#parametermanager">ParameterManager</a></h3>
<p><strong>WHAT</strong>
ParameterManager 会记录 tunable 的变量，包括时间和 fusion buffer size 等，然后以获得高吞吐为目标对其进程动态调整</p>
<p>Manager 类主要包含几个部分</p>
<ul>
<li>manager 自身的配置包括开关、warmup 信息等等</li>
<li>API 包括 update、tune 等</li>
<li>对可调整参数的封装，包括调整对象和调整方法</li>
<li>调整策略, 基于上述封装，实现怎样调整的策略</li>
</ul>
<pre><code class="language-cpp">// horovod/common/parameter_manager.h

class ParameterManager {
  // 根据 tensor 信息计算 score 和调用 Tune 调整参数
  // 返回值决定是否将更新后的参数广播出去
  bool Update(const std::vector&lt;std::string&gt;&amp; tensor_names, int64_t bytes);
  // 根据 score 调整参数
  bool Tune(double score);

  // Manager 的配置
  struct Params {
    bool hierarchical_allreduce;
    bool hierarchical_allgather;
    bool cache_enabled;
    double tensor_fusion_threshold;
    double cycle_time;
    bool active;
  };
  Params GetParams();

  // Interface used to represent a parameter (or group of parameters) being tuned.
  class ITunableParameter {
  public:
    virtual bool Tune(double score, double* best_score) = 0;
    virtual void UpdateBestValue(double score) = 0;
    virtual double BestScore() const = 0;
    virtual bool IsTunable() const = 0;
  };

  // Abstract base class used to implement hierarchical parameter tuning.
  template &lt;class T&gt;
  class TunableParameter : public ITunableParameter {
    TunableParameter(T initial_value);
    T initial_value_;
    T value_;
    T best_value_;
  };
  // 需要调整的参数对象
  std::vector&lt;ITunableParameter*&gt; parameter_chain_;

  // A parameter that optimizes over a finite set of discrete values to be tried sequentially.
  template &lt;class T&gt;
  class CategoricalParameter : public TunableParameter&lt;T&gt; {
    CategoricalParameter(std::vector&lt;T&gt; values);
    std::vector&lt;T&gt; values_;
  };

  enum BayesianVariable { fusion_buffer_threshold_mb, cycle_time_ms };

  struct BayesianVariableConfig {
    BayesianVariable variable;
    std::pair&lt;double, double&gt; bounds;
  };

  // A set of numerical parameters optimized jointly using Bayesian Optimization.
  class BayesianParameter : public TunableParameter&lt;Eigen::VectorXd&gt; {
    std::unique_ptr&lt;BayesianOptimization&gt; bayes_;
  };

  CategoricalParameter&lt;bool&gt; hierarchical_allreduce_;
  CategoricalParameter&lt;bool&gt; hierarchical_allgather_;
  CategoricalParameter&lt;bool&gt; cache_enabled_;
  BayesianParameter joint_params_;
};
</code></pre>
<h3 id="fusionbuffermanager"><a class="header" href="#fusionbuffermanager">FusionBufferManager</a></h3>
<p>这里的 Buffer 会伴随进程的整个生命周期</p>
<pre><code class="language-cpp">// horovod/common/fusion_buffer_manager.h

class FusionBufferManager {
public:
  Status InitializeBuffer(int64_t threshold,
                          int device, std::shared_ptr&lt;OpContext&gt; context,
                          int stream_id,
                          std::function&lt;void()&gt; on_start_init,
                          std::function&lt;void()&gt; on_end_init);
  // Status status = context-&gt;AllocatePersistent(threshold, &amp;buffer);

  std::shared_ptr&lt;PersistentBuffer&gt; GetBuffer(int device, Framework framework, int stream_id);

  // map key: device ID, framework, stream_id
  std::unordered_map&lt;std::tuple&lt;int, Framework, int&gt;,
      std::pair&lt;std::shared_ptr&lt;PersistentBuffer&gt;, int64_t&gt;&gt; tensor_fusion_buffers_;
};

</code></pre>
<h3 id="processset--processsettable"><a class="header" href="#processset--processsettable">ProcessSet &amp; ProcessSetTable</a></h3>
<p>ProcessSet 持有以下对象</p>
<ul>
<li>Controller controller</li>
<li>TensorQueue tensor_queue</li>
<li>GroupTable group_table</li>
</ul>
<p>基本调用流程</p>
<ul>
<li>HorovodGlobalState 中的 ProcessSetTable 对象</li>
<li>ProcessSetTable Ids() 方法获取 ProcessSet 的 id</li>
<li>ProcessSetTable Get() 方法从 id_to_process_set_ 变量中获取 ProcessSet 对象</li>
<li>ProcessSet 的 IsCurrentProcessIncluded() 方法判断可见性</li>
</ul>
<pre><code class="language-cpp">// horovod/common/process_set.h

struct ProcessSet {
  std::shared_ptr&lt;Controller&gt; controller;

  TensorQueue tensor_queue;

  // LRU cache of Responses
  ResponseCache response_cache;

  // Information on registered groups.
  GroupTable group_table;

  std::vector&lt;int&gt; registered_global_ranks;

  MPIContext mpi_context;

  bool Initialize(const MPIContext&amp; global_mpi_context);

  void Finalize(const Status&amp; status);

  bool IsCurrentProcessIncluded() const;

  explicit ProcessSet(std::vector&lt;int&gt; global_ranks = {});
};

class ProcessSetTable {
  ProcessSetTable();

  void Initialize(const MPIContext&amp; global_mpi_context);

  int32_t RegisterProcessSet(std::vector&lt;int&gt; global_ranks = {});

  std::vector&lt;int32_t&gt; Ids() const; // Returns copy to be threadsafe

  bool Contains(int32_t id) const;

  ProcessSet&amp; Get(int32_t id);

  // Returns -1 if no process set with these ranks has been registered.
  int32_t FindId(const std::vector&lt;int32_t&gt;&amp; ranks);

  void MarkProcessSetForRemoval(int32_t process_set_id);

  void DeregisterProcessSet(int32_t process_set_id);

  // ProcessSet 对象
  std::unordered_map&lt;int32_t, ProcessSet&gt; id_to_process_set_;
};
</code></pre>
<h3 id="controller"><a class="header" href="#controller">Controller</a></h3>
<p>Controller 持有以下对象需要在创建时传入，即外部依赖</p>
<ul>
<li>TensorQueue&amp; tensor_queue_</li>
<li>ResponseCache&amp; response_cache_</li>
<li>ParameterManager&amp; parameter_manager_</li>
<li>GroupTable&amp; group_table_</li>
</ul>
<pre><code class="language-cpp">class Controller : public std::enable_shared_from_this&lt;Controller&gt; {
  void Initialize();

  virtual int GetTypeSize(DataType dtype) = 0;

  virtual void CrossRankBitwiseAnd(std::vector&lt;long long&gt;&amp; bitvector, int count) = 0;

  virtual void CrossRankBitwiseOr(std::vector&lt;long long&gt;&amp; bitvector, int count) = 0;

  virtual void Bcast(void* buffer, size_t size, int root_rank, Communicator communicator) = 0;

  virtual void AlltoallGetRecvSplits(const std::vector&lt;int32_t&gt;&amp; splits, std::vector&lt;int32_t&gt;&amp; recvsplits) = 0;

  virtual void Barrier(Communicator communicator) = 0;

  virtual void Allgather2Ints(std::array&lt;int, 2&gt; values, std::vector&lt;int&gt;&amp; recv_values) = 0;

  void SynchronizeParameters();

  ResponseList ComputeResponseList(bool this_process_requested_shutdown, HorovodGlobalState&amp; state, ProcessSet&amp; process_set);

  virtual void DoInitialization() = 0;

  // For rank 0 to receive other ranks' ready tensors.
  virtual void RecvReadyTensors(std::vector&lt;std::string&gt;&amp; ready_to_reduce,
                                std::vector&lt;RequestList&gt;&amp; ready_list) = 0;

  // For other ranks to send their ready tensors to rank 0
  virtual void SendReadyTensors(RequestList&amp; message_list) = 0;

  // For rank 0 to send final tensors ready to be allreduced/allgathered to other ranks.
  virtual void SendFinalTensors(ResponseList&amp; response_list) = 0;

  // For other ranks to receive final ready tensors.
  virtual void RecvFinalTensors(ResponseList&amp; response_list) = 0;

  // Once a tensor is ready to be reduced, the coordinator sends a Response
  // instructing all ranks to start the reduction to all ranks. The Response
  // also contains error messages in case the submitted Requests were not
  // valid (for example, contained mismatched shapes or types).
  // Constructing the Response, thus, requires a whole lot of error checking.
  Response ConstructResponse(const std::string&amp; name, int joined_size = 0);

  // Routine to sync cache hit and invalid bit sets across workers.
  // Also determines global shutdown state and whether uncached requests
  // exist on any worker.
  void CoordinateCacheAndState(CacheCoordinator&amp; cache_coordinator);

  void FuseResponses(std::deque&lt;Response&gt;&amp; responses,
                     HorovodGlobalState&amp; state,
                     ResponseList&amp; response_list);

  // Return the total byte size of the final allgathered output tensor
  int64_t
  TotalByteSizeOfAllgatherOutput(const std::vector&lt;int64_t&gt;&amp; tensor_sizes,
                                 const TensorTableEntry&amp; entry);

  // Store the Request for a name, and return whether the total count of
  // Requests for that tensor is now equal to the HOROVOD size (and thus we are
  // ready to reduce the tensor).
  bool IncrementTensorCount(const Request&amp; msg, int joined_size = 0);

  bool is_initialized_ = false;

  int rank_ = 0;
  int local_rank_ = 0;
  int cross_rank_ = 0;
  int size_ = 1;
  int local_size_ = 1;
  int cross_size_ = 1;
  bool is_coordinator_ = false;
  bool is_homogeneous_ = false;

  // Global rank of each process in the set associated to this controller.
  std::vector&lt;int&gt; global_ranks_;

  // Map (global rank) -&gt; (process set controller rank) for each process in this
  // set.
  std::unordered_map&lt;int,int&gt; global_rank_to_controller_rank_;

  // Controller process set ranks of processes running on this node.
  std::vector&lt;int&gt; local_comm_ranks_;

  StallInspector stall_inspector_;

  // Only exists on the coordinator node (rank zero). Maintains a vector of
  // requests to allreduce every tensor (keyed by tensor name).
  MessageTable message_table_;

  // Outside dependencies
  TensorQueue&amp; tensor_queue_;

  ResponseCache&amp; response_cache_;

  ParameterManager&amp; parameter_manager_;

  GroupTable&amp; group_table_;
};
</code></pre>
<h3 id="tensorqueue"><a class="header" href="#tensorqueue">TensorQueue</a></h3>
<p>TensorQueue 被 Controller 持有，主要包含两个对象</p>
<ul>
<li>Request 的对象 message_queue_，在 controller 中被取出做准备工作</li>
<li>TensorTableEntry 对象 tensor_table_ 真正需要进行通信的 tensor</li>
</ul>
<pre><code class="language-cpp">// horovod/common/tensor_queue.h

class TensorQueue {
  TensorQueue() = default;
  Status AddToTensorQueue(TensorTableEntry&amp; e, Request&amp; message);
  Status AddToTensorQueueMulti(std::vector&lt;TensorTableEntry&gt;&amp; entries, std::vector&lt;Request&gt;&amp; messages);

  void FinalizeTensorQueue(const Status&amp; status);

  int64_t GetTensorDataForAutotuner(const ResponseList&amp; response_list,
                                    std::vector&lt;std::string&gt;&amp; tensor_names);

  void GetTensorEntriesFromResponse(const Response&amp; response,
                                    std::vector&lt;TensorTableEntry&gt;&amp; entries,
                                    bool joined = false);

  const TensorTableEntry&amp; GetTensorEntry(const std::string&amp; tensor_name) const;

  bool IsTensorPresentInTable (const std::string&amp; tensor_name) const;

  void PopMessagesFromQueue(std::deque&lt;Request&gt;&amp; message_queue_buffer);

  void PushMessageToQueue(Request&amp; message);

  void PushMessagesToQueue(std::deque&lt;Request&gt;&amp; messages);

  void RemoveJoinTensor();

  // Tensors waiting to be allreduced or allgathered.
  std::unordered_map&lt;std::string, TensorTableEntry&gt; tensor_table_;

  // Queue of MPI requests waiting to be sent to the coordinator node.
  std::queue&lt;Request&gt; message_queue_;
};
</code></pre>
<h3 id="grouptable"><a class="header" href="#grouptable">GroupTable</a></h3>
<p>在 ProcessSet 中持有，</p>
<ul>
<li>EnqueueTensorAllreduces 中调用 RegisterGroup</li>
<li>RunLoopOnce 中 PerformOperation 前会调用 DeregisterGroups</li>
</ul>
<pre><code class="language-cpp">// horovod/common/group_table.h

class GroupTable {
public:
  GroupTable() = default;

  int32_t GetGroupIDFromTensorName(const std::string&amp; tensor_name) const;
  const std::vector&lt;std::string&gt;&amp; GetGroupTensorNames(int32_t group_id) const;
  bool empty(void) const;

  int32_t RegisterGroup(std::vector&lt;std::string&gt;&amp;&amp; tensor_names);
  void DeregisterGroups(const std::vector&lt;std::string&gt;&amp; tensor_names);

  std::unordered_map&lt;std::string, int32_t&gt; tensor_name_to_id_;
  std::unordered_map&lt;int32_t, std::vector&lt;std::string&gt;&gt; id_to_tensor_names_;

  // Queue of ids that can be reused
  std::queue&lt;int32_t&gt; free_ids_;

  // Next available group id (increases each time a group is added)
  int32_t next_group_id_ = 0;
};
</code></pre>
<h3 id="operationmanager"><a class="header" href="#operationmanager">OperationManager</a></h3>
<p>对通信 op 的一个封装，基本上通过调用 op-&gt;Excute 实现，为 horovod/common/ops 目录中的各个 op 实现提供了接口。</p>
<pre><code class="language-cpp">// horovod/common/ops/operation_manager.h

class OperationManager {
  OperationManager(ParameterManager* param_manager,
                   std::vector&lt;std::shared_ptr&lt;AllreduceOp&gt;&gt; allreduce_ops,
                   std::vector&lt;std::shared_ptr&lt;AllgatherOp&gt;&gt; allgather_ops,
                   std::vector&lt;std::shared_ptr&lt;BroadcastOp&gt;&gt; broadcast_ops,
                   std::vector&lt;std::shared_ptr&lt;AlltoallOp&gt;&gt; alltoall_ops,
                   std::vector&lt;std::shared_ptr&lt;ReducescatterOp&gt;&gt; reducescatter_ops,
                   std::shared_ptr&lt;JoinOp&gt; join_op,
                   std::vector&lt;std::shared_ptr&lt;AllreduceOp&gt;&gt; adasum_ops,
                   std::shared_ptr&lt;BarrierOp&gt; barrier_op,
                   std::shared_ptr&lt;ErrorOp&gt; error_op);

  Status ExecuteAllreduce(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteAllgather(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteBroadcast(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteAlltoall(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteReducescatter(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteError(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteJoin(std::vector&lt;TensorTableEntry&gt;&amp; entries,
                     const Response&amp; response, ProcessSet&amp; process_set) const;

  Status ExecuteAdasum(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteBarrier(std::vector&lt;TensorTableEntry&gt;&amp; entries, const Response&amp; response) const;

  Status ExecuteOperation(std::vector&lt;TensorTableEntry&gt;&amp; entries,
                          const Response&amp; response,
                          ProcessSet&amp; process_set) const;

  ParameterManager* param_manager_;

  std::vector&lt;std::shared_ptr&lt;AllreduceOp&gt;&gt; allreduce_ops_;
  std::vector&lt;std::shared_ptr&lt;AllgatherOp&gt;&gt; allgather_ops_;
  std::vector&lt;std::shared_ptr&lt;BroadcastOp&gt;&gt; broadcast_ops_;
  std::vector&lt;std::shared_ptr&lt;AlltoallOp&gt;&gt; alltoall_ops_;
  std::vector&lt;std::shared_ptr&lt;ReducescatterOp&gt;&gt; reducescatter_ops_;
  std::shared_ptr&lt;JoinOp&gt; join_op_;
  std::vector&lt;std::shared_ptr&lt;AllreduceOp&gt;&gt; adasum_ops_;
  std::shared_ptr&lt;BarrierOp&gt; barrier_op_;
  std::shared_ptr&lt;ErrorOp&gt; error_op_;
};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="develop"><a class="header" href="#develop">Develop</a></h1>
<h2 id="horovod-对象"><a class="header" href="#horovod-对象">Horovod 对象</a></h2>
<pre><code class="language-cpp"># horovod/common/common.h

enum Framework { TENSORFLOW, PYTORCH, MXNET, XLA };
enum StatusType { OK, UNKNOWN_ERROR, PRECONDITION_ERROR, ABORTED, INVALID_ARGUMENT, IN_PROGRESS };
enum DeviceType { CPU, GPU };

// for gpu
struct Event {};

class Status {
public:
  Status();
  static Status OK();
  static Status UnknownError(const std::string&amp; message);
  static Status PreconditionError(const std::string&amp; message);
  static Status Aborted(const std::string&amp; message);
  static Status InvalidArgument(const std::string&amp; message);
  static Status InProgress();
  bool ok() const;
  bool in_progress() const;
  StatusType type() const;
  const std::string&amp; reason() const;
  Event event;

private:
  StatusType type_ = StatusType::OK;
  std::string reason_;
  Status(StatusType type, std::string reason);
};

class TensorShape {
public:
  TensorShape() : shape_() {}
  TensorShape(std::vector&lt;int64_t&gt; vec) : shape_(vec) {}
  void AddDim(int64_t dim);
  void AppendShape(TensorShape&amp; other);

  std::string DebugString() const;
  int dims() const;
  int64_t dim_size(int idx) const;
  int64_t num_elements() const;
  const std::vector&lt;int64_t&gt;&amp; to_vector() const;

  inline bool operator==(const TensorShape&amp; rhs) const {
    return shape_ == rhs.shape_;
  }

  inline bool operator!=(const TensorShape&amp; rhs) const {
    return shape_ != rhs.shape_;
  }

private:
  std::vector&lt;int64_t&gt; shape_;
};

class ReadyEvent {};
class ReadyEventList {};

class PersistentBuffer {
public:
  virtual const void* AccessData(std::shared_ptr&lt;OpContext&gt; context) const = 0;
  virtual ~PersistentBuffer() = default;
};

class Tensor {
public:
  virtual const DataType dtype() const = 0;
  virtual const TensorShape shape() const = 0;
  virtual const void* data() const = 0;
  virtual int64_t size() const = 0;
  virtual ~Tensor() = default;
};

class OpContext {
public:
  // These allocators are fully synchronous, unlike TensorFlow counterparts.
  virtual Status
  AllocatePersistent(int64_t size,
                     std::shared_ptr&lt;PersistentBuffer&gt;* tensor) = 0;
  virtual Status AllocateOutput(TensorShape shape,
                                std::shared_ptr&lt;Tensor&gt;* tensor,
                                std::shared_ptr&lt;ReadyEvent&gt;* event = nullptr) = 0;
  virtual Status AllocateOutput(int output_index, TensorShape shape,
                                std::shared_ptr&lt;Tensor&gt;* tensor,
                                std::shared_ptr&lt;ReadyEvent&gt;* event = nullptr) {
    if (output_index == 0) {
      return AllocateOutput(std::move(shape), tensor);
    } else {
      throw std::logic_error(&quot;output_index != 0 not supported&quot;);
    }
  }
  virtual Status AllocateZeros(int64_t num_elements, DataType dtype,
                                std::shared_ptr&lt;Tensor&gt;* tensor) = 0;
  virtual Framework framework() const = 0;
  virtual ~OpContext() = default;
};

// A callback to call after the communication completes. Since the
// allreduce and allgather ops are asynchronous, this callback is what resumes
// computation after the reduction is completed.
using StatusCallback = std::function&lt;void(const Status&amp;)&gt;;

// Table storing Tensors to be reduced, keyed by unique name.
// This table contains everything necessary to do the distributed operation.
struct TensorTableEntry {
  std::string tensor_name;
  std::shared_ptr&lt;OpContext&gt; context;
  std::shared_ptr&lt;Tensor&gt; tensor;
  std::shared_ptr&lt;Tensor&gt; output;
  // Identifier for the subset of Horovod processes partaking in this operation.
  int32_t process_set_id = 0;
  // Root rank for broadcast operation (relative to process set).
  int root_rank = 0;
  // List of events indicating that data is ready.
  ReadyEventList ready_event_list;
  // GPU to do reduction on, or CPU_DEVICE_ID in case of CPU.
  int device = CPU_DEVICE_ID;
  // A callback to call with the status.
  StatusCallback callback;
  // If we build with NVTX support: A range marking the start
  // and end of the distributed op for this tensor (may be
  // shared by multiple tensors).
  SharedNvtxOpRange nvtx_op_range;

  // Alltoall splits (if tensor is for an Alltoall operation)
  // Note: splits are stored in TensorTableEntry to avoid N^2
  // storage complexity of collecting all worker split arrays
  // on coordinator rank.
  std::vector&lt;int32_t&gt; splits;
  std::shared_ptr&lt;Tensor&gt; received_splits;

  void FinishWithCallback(const Status&amp; status);
};
</code></pre>
<h3 id="message"><a class="header" href="#message">Message</a></h3>
<h4 id="request"><a class="header" href="#request">Request</a></h4>
<pre><code class="language-cpp">// horovod/common/message.h

// Request 是非 0 worker 向 0 号 worker 即 coordinator 发送消息的消息体
class Request {
  enum RequestType {
    ALLREDUCE = 0,
    ALLGATHER = 1,
    BROADCAST = 2,
    JOIN = 3,
    ADASUM = 4,
    ALLTOALL = 5,
    BARRIER = 6,
    REDUCESCATTER = 7
  };

  static const std::string&amp; RequestType_Name(RequestType value);
  int32_t request_rank() const;
  void set_request_rank(int32_t value);
  RequestType request_type() const;
  void set_request_type(RequestType value);
  DataType tensor_type() const;
  void set_tensor_type(DataType value);
  const std::string&amp; tensor_name() const;
  void set_tensor_name(const std::string&amp; value);
  int32_t root_rank() const;
  void set_root_rank(int32_t value);
  int32_t device() const;
  void set_device(int32_t value);
  int32_t group_id() const;
  void set_group_id(int32_t value);
  const std::vector&lt;int64_t&gt;&amp; tensor_shape() const;
  void set_tensor_shape(const std::vector&lt;int64_t&gt;&amp; value);
  void add_tensor_shape(int64_t value);
  double prescale_factor() const;
  double postscale_factor() const;
  void set_prescale_factor(double prescale_factor);
  void set_postscale_factor(double postscale_factor);
  static void ParseFromBytes(Request&amp; request, const uint8_t* input);
  static void SerializeToString(const Request&amp; request, std::string&amp; output);

  int32_t request_rank_ = 0;
  RequestType request_type_ = RequestType::ALLREDUCE;
  DataType tensor_type_ = DataType::HOROVOD_UINT8;
  int32_t root_rank_ = 0;
  int32_t device_ = 0;
  int32_t group_id_ = NULL_GROUP_ID;
  std::string tensor_name_;
  std::vector&lt;int64_t&gt; tensor_shape_;
  double prescale_factor_ = 1.0;
  double postscale_factor_ = 1.0;
};

class RequestList {
  const std::vector&lt;Request&gt;&amp; requests() const;
  void set_requests(const std::vector&lt;Request&gt;&amp; value);
  void add_request(const Request&amp; value);
  void emplace_request(Request&amp;&amp; value);
  bool shutdown() const;
  void set_shutdown(bool value);
  static void ParseFromBytes(RequestList&amp; request_list, const uint8_t* input);
  static void SerializeToString(const RequestList&amp; request_list, std::string&amp; output);

  std::vector&lt;Request&gt; requests_;
  bool shutdown_ = false;
};
</code></pre>
<h4 id="response"><a class="header" href="#response">Response</a></h4>
<pre><code class="language-cpp">// Response 是 0 号 worker 即 coordinator 向非 0 worker 发送消息的消息体
class Response {
public:
  enum ResponseType {
    ALLREDUCE = 0,
    ALLGATHER = 1,
    BROADCAST = 2,
    JOIN = 3,
    ADASUM = 4,
    ALLTOALL = 5,
    BARRIER = 6,
    REDUCESCATTER = 7,
    ERROR = 8
  };

  static const std::string&amp; ResponseType_Name(ResponseType value);
  ResponseType response_type() const;
  void set_response_type(ResponseType value);
  const std::vector&lt;std::string&gt;&amp; tensor_names() const;
  DataType tensor_type() const;
  void set_tensor_type(DataType value);
  const std::string tensor_names_string() const;
  void set_tensor_names(const std::vector&lt;std::string&gt;&amp; value);
  void add_tensor_name(const std::string&amp; value);
  void add_tensor_name(std::string&amp;&amp; value);
  const std::string&amp; error_message() const;
  void set_error_message(const std::string&amp; value);
  const std::vector&lt;int32_t&gt;&amp; devices() const;
  void set_devices(const std::vector&lt;int32_t&gt;&amp; value);
  void add_device(int32_t value);
  const std::vector&lt;int64_t&gt;&amp; tensor_sizes() const;
  void set_tensor_sizes(const std::vector&lt;int64_t&gt;&amp; value);
  void add_tensor_size(int64_t value);
  void add_allgather_response(const Response&amp; response);
  double prescale_factor() const;
  double postscale_factor() const;
  void set_prescale_factor(double prescale_factor);
  void set_postscale_factor(double postscale_factor);
  int last_joined_rank() const;
  void set_last_joined_rank(int value);
  static void ParseFromBytes(Response&amp; response, const uint8_t* input);
  static void SerializeToString(const Response&amp; response, std::string&amp; output);

  ResponseType response_type_ = ResponseType::ALLREDUCE;
  std::vector&lt;std::string&gt; tensor_names_;
  DataType tensor_type_ = DataType::HOROVOD_UINT8;
  std::string error_message_;
  std::vector&lt;int32_t&gt; devices_;
  std::vector&lt;int64_t&gt; tensor_sizes_;
  double prescale_factor_ = 1.0;
  double postscale_factor_ = 1.0;
  int last_joined_rank_ = -1;
};

class ResponseList {
  const std::vector&lt;Response&gt;&amp; responses() const;
  void set_responses(const std::vector&lt;Response&gt;&amp; value);
  void add_response(const Response&amp; value);
  void add_response(Response&amp;&amp; value);
  void emplace_response(Response&amp;&amp; value);
  bool shutdown() const;
  void set_shutdown(bool value);
  static void ParseFromBytes(ResponseList&amp; response_list, const uint8_t* input);
  static void SerializeToString(const ResponseList&amp; response_list, std::string&amp; output);

  std::vector&lt;Response&gt; responses_;
  bool shutdown_ = false;
};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="python"><a class="header" href="#python">python</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="concurrent-execution"><a class="header" href="#concurrent-execution">Concurrent Execution</a></h1>
<p><a href="https://docs.python.org/3/library/concurrency.html">doc</a></p>
<h3 id="1-subprocess"><a class="header" href="#1-subprocess">1. subprocess</a></h3>
<ul>
<li>启动子进程，自定义 excutable</li>
</ul>
<pre><code class="language-python">import subprocess

proc = subprocess.Popen([&quot;/usr/bin/git&quot;, &quot;commit&quot;, &quot;-m&quot;, &quot;Fixes a bug.&quot;])
proc.poll()
proc.wait()
proc.communicate()
proc.terminate()
proc.kill()
</code></pre>
<p><a href="https://docs.python.org/3/library/subprocess.html">doc</a></p>
<h3 id="2-threading"><a class="header" href="#2-threading">2. threading</a></h3>
<ul>
<li>启动线程</li>
<li>存在 GIL 问题，无法完全利用 CPU</li>
</ul>
<pre><code class="language-python">import threading

def serve():
    pass

thread = threading.Thread(target=serve, daemon=None)
thread.start()
thread.join()
</code></pre>
<p><a href="https://docs.python.org/3/library/threading.html">doc</a></p>
<h4 id="pros"><a class="header" href="#pros">Pros</a></h4>
<ul>
<li>Lightweight - low memory footprint</li>
<li>Shared memory - makes access to state from another context easier</li>
<li>Allows you to easily make responsive UIs</li>
<li>cPython C extension modules that properly release the GIL will run in parallel</li>
<li>Great option for I/O-bound applications</li>
</ul>
<h4 id="cons"><a class="header" href="#cons">Cons</a></h4>
<ul>
<li>cPython - subject to the GIL</li>
<li>Not interruptible/killable</li>
<li>If not following a command queue/message pump model (using the Queue module), then manual use of synchronization primitives become a necessity (decisions are needed for the granularity of locking)</li>
<li>Code is usually harder to understand and to get right - the potential for race conditions increases dramatically</li>
</ul>
<h3 id="3-multiprocessing"><a class="header" href="#3-multiprocessing">3. multiprocessing</a></h3>
<ul>
<li>启动子进程，使用内置函数</li>
<li>使用进程，充分利用 CPU 资源</li>
</ul>
<pre><code class="language-python">from multiprocessing

proc = multiprocessing.Process(target=serve, daemon=None)
proc.start()
proc.join()
proc.close()

</code></pre>
<p><a href="https://docs.python.org/3/library/multiprocessing.html">doc</a></p>
<h4 id="pros-1"><a class="header" href="#pros-1">Pros</a></h4>
<ul>
<li>Separate memory space</li>
<li>Code is usually straightforward</li>
<li>Takes advantage of multiple CPUs &amp; cores</li>
<li>Avoids GIL limitations for cPython</li>
<li>Eliminates most needs for synchronization primitives unless if you use shared memory (instead, it's more of a communication model for IPC)</li>
<li>Child processes are interruptible/killable</li>
<li>Python multiprocessing module includes useful abstractions with an interface much like threading.Thread</li>
<li>A must with cPython for CPU-bound processing</li>
</ul>
<h4 id="cons-1"><a class="header" href="#cons-1">Cons</a></h4>
<ul>
<li>IPC a little more complicated with more overhead (communication model vs. shared memory/objects)</li>
<li>Larger memory footprint</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
