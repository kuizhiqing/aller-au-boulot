<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Aller au boulot</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">1.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">1.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">1.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">1.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">1.4.</strong> api</a></li></ol></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">2.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">2.1.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">2.2.</strong> profiler</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">3.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps/ps-code-overview.html"><strong aria-hidden="true">3.1.</strong> ps</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">4.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">4.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/api.html"><strong aria-hidden="true">4.2.</strong> api</a></li><li class="chapter-item "><a href="horovod/common.html"><strong aria-hidden="true">4.3.</strong> common</a></li><li class="chapter-item "><a href="horovod/torch_api.html"><strong aria-hidden="true">4.4.</strong> torch api</a></li></ol></li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">5.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">5.1.</strong> concurrent execution</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="ray"><a class="header" href="#ray">Ray</a></h1>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<ul>
<li><a href="https://github.com/ray-project/ray">Ray Github</a></li>
<li><a href="https://docs.ray.io/en/latest/">Ray Documentation</a></li>
</ul>
<p>源代码和官方文档永远是最好的学习资料，总结和学习笔记能辅助快速理解，抓住重点，提高效率。</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<h2 id="build"><a class="header" href="#build">BUILD</a></h2>
<p>Ray 的编译使用 bazel，其中大量的 proto 编译也依赖于此，
具体定义在 <code>src/ray/protobuf/BUILD</code> 文件中，参考 <a href="https://rules-proto-grpc.com/en/latest/example.html">文档</a>.</p>
<h2 id="启动部署"><a class="header" href="#启动部署">启动部署</a></h2>
<p>安装完成后，可以直接运行的相关命令如下</p>
<pre><code class="language-python"># python/setup.py

entry_points={
    &quot;console_scripts&quot;: [
        &quot;ray=ray.scripts.scripts:main&quot;,
        &quot;rllib=ray.rllib.scripts:cli [rllib]&quot;,
        &quot;tune=ray.tune.scripts:cli&quot;,
        &quot;ray-operator=ray.ray_operator.operator:main&quot;,
        &quot;serve=ray.serve.scripts:cli&quot;,
    ]
}
</code></pre>
<p>常驻模式启动 ray 集群</p>
<pre><code class="language-shell"># 启动 head 节点
ray start --head
# 启动 worker 节点
ray start --address=RAY_HEAD_IP:6379

ray start --head --redis-password=&quot;&quot; --port=6389
</code></pre>
<p>start 命令的解析如下</p>
<pre><code class="language-python"># python/ray/scripts/scripts.py

# args 解析用的是 click

@cli.command()
@click.option(&quot;--head&quot;,...)
def start(...,head,...):
    ray_params = ray._private.parameter.RayParams(...)
    if head:
        # 启动 head 节点
        ray_params.update_if_absent(...)
        node = ray.node.Node(ray_params, head=True,...)
    else:
        # 启动 worker 节点
        bootstrap_address = services.canonicalize_bootstrap_address(address)
        ray_params.gcs_address = bootstrap_address
        node = ray.node.Node(ray_params, head=False,...)

cli.add_command(start)
def main():
    return cli()
</code></pre>
<p>可以看出本质上都是初始化了节点 Node 对象，是否为 head 则通过参数指定。</p>
<h3 id="node"><a class="header" href="#node">Node</a></h3>
<p>Node 节点的初始化</p>
<pre><code class="language-python"># python/ray/node.py

class Node:
    def __init__(self, ray_params, head=False,...):
        self.head = head

        if not head:
            # GCS GRPC client, 确保 gcs 已启动
            self.get_gcs_client()

        # 初始化持久化存储
        storage._init_storage(ray_params.storage, is_head=head) 

        if head:
            self.validate_external_storage()

        if ...:
            # 启动 reaper 进程，负责在主进程意外退出后回收进程
            self.start_reaper_process()

        if head:
            self.start_head_processes()
            # 尝试写入 gcs
            self.get_gcs_client().internal_kv_put(...)

        if not connect_only:
            self.start_ray_processes()
            ray._private.services.wait_for_node(...)
</code></pre>
<p>Node 的初始化包括启动 start_head_processes 和 start_ray_processes 两部分。</p>
<pre><code class="language-python"># python/ray/node.py

class Node:
    def start_head_processes(self):
        # 如果使用外部 redis，需要配置
        # 这里的逻辑目前有点 confuse，external 和 local 不够明确
        if self._ray_params.external_addresses is not None:
            self.start_or_configure_redis()
            self.create_redis_client()

        # 启动 gcs，包含 redis 服务, 默认端口 6379
        self.start_gcs_server()

        self.start_ray_client_server()

    def start_or_configure_redis(self):
        # 如果 external 有配置，并不真正启动
        ray._private.services.start_redis(...)

    def start_gcs_server(self):
        process_info = ray._private.services.start_gcs_server(self.redis_address,...)
        # 等待启动
        self.get_gcs_client()

    def start_ray_client_server(self):
        process_info = ray._private.services.start_ray_client_server(self.address, self._node_ip_address, ...)

    def start_ray_processes(self):
        # 启动节点上所有的进程
        self.destroy_external_storage()
        # 启动 raylet
        self.start_raylet(plasma_directory, object_store_memory)

    def start_raylet(self, ...):
        process_info = ray._private.services.start_raylet(
            self.redis_address,
            self.gcs_address,
            self._node_ip_address,
            ...)

</code></pre>
<ul>
<li>Head 节点启动 gcs 服务和 ray_client 服务</li>
<li>Head 和 Worker 节点都启动 raylet 服务</li>
</ul>
<p>这些服务的具体启动都被封装在 services 里。</p>
<h3 id="services"><a class="header" href="#services">Services</a></h3>
<p>Services 提供多种服务启动的封装，包括 redis 服务启动。</p>
<blockquote>
<p>1.11 之前的版本 ray 通过启动 redis-server 二进制启动 redis，新版本中已经移除。</p>
</blockquote>
<pre><code class="language-python"># python/ray/_private/services.py

def start_gcs_server(redis_address, ...):
    # 调用 gcs 二进制启动服务，包含 redis 服务
    # GCS_SERVER_EXECUTABLE &quot;core/src/ray/gcs/gcs_server&quot;
    command = [GCS_SERVER_EXECUTABLE, &quot;--redis_xxxx=&quot;, ...]
    process_info = start_ray_process(command, ...)

def start_ray_client_server(address, ray_client_server_ip,...):
    command = [
        sys.executable,    # python
        setup_worker_path, # ray/workers/setup_worker.py
        &quot;-m&quot;,
        &quot;ray.util.client.server&quot;,
        ...]
    process_info = start_ray_process(command, ...)

def start_raylet(redis_address, gcs_address, ...):
    # 启动 raylet，包括 local scheduler 和 object manager
    # 支持 python、java 和 cpp
    # RAYLET_EXECUTABLE &quot;core/src/ray/raylet/raylet&quot;
    command = [
        RAYLET_EXECUTABLE,
        f&quot;--python_worker_command={subprocess.list2cmdline(start_worker_command)}&quot;,  # noqa
        f&quot;--java_worker_command={subprocess.list2cmdline(java_worker_command)}&quot;,  # noqa
        f&quot;--cpp_worker_command={subprocess.list2cmdline(cpp_worker_command)}&quot;,  # noqa
        ...]
    command.append(&quot;--agent_command={}&quot;.format(subprocess.list2cmdline(agent_command)))
    process_info = start_ray_process(command, ...)

def start_ray_process(command, ...):
    process = ConsolePopen(
        command,
        env=modified_env,
        cwd=cwd,
        stdout=stdout_file,
        stderr=stderr_file,
        stdin=subprocess.PIPE if pipe_stdin else None,
        preexec_fn=preexec_fn if sys.platform != &quot;win32&quot; else None,
        creationflags=CREATE_SUSPENDED if win32_fate_sharing else 0,
    )

class ConsolePopen(subprocess.Popen):
    pass

</code></pre>
<h3 id="setup-worker--runtime-env-context"><a class="header" href="#setup-worker--runtime-env-context">setup worker &amp; runtime env context</a></h3>
<p>setup worker 的作用是执行不同的程序，</p>
<pre><code class="language-python"># ray/workers/setup_worker.py

parser.add_argument(&quot;--serialized-runtime-env-context&quot;,...)
parser.add_argument(&quot;--language&quot;, ...)

if __name__ == &quot;__main__&quot;:
    args, remaining_args = parser.parse_known_args()
    runtime_env_context = RuntimeEnvContext.deserialize(
        args.serialized_runtime_env_context or &quot;{}&quot;
    )
    runtime_env_context.exec_worker(remaining_args, Language.Value(args.language))
</code></pre>
<pre><code class="language-python"># python/ray/_private/runtime_env/context.py

class RuntimeEnvContext:
    def exec_worker(self, passthrough_args: List[str], language: Language):
        os.environ.update(self.env_vars)

        # exec [python] passthrough_args
        command_str = &quot; &amp;&amp; &quot;.join(...)

        if sys.platform == &quot;win32&quot;:
            os.system(command_str)
        else:
            os.execvp(&quot;bash&quot;, args=[&quot;bash&quot;, &quot;-c&quot;, command_str])
</code></pre>
<h3 id="client-server"><a class="header" href="#client-server">client server</a></h3>
<pre><code class="language-bash">python -m ray.util.client.server --address=x.x.x.x:6379 --host=0.0.0.0 --port=10001 --mode=proxy --redis-password=
</code></pre>
<pre><code class="language-python"># python/ray/util/client/server/__main__.py
# -&gt; server.py main
# python/ray/util/client/server/server.py

def main():
    server = serve(hostport, ray_connect_handler)
    while True:
        ray.experimental.internal_kv._internal_kv_put(..., HEALTHCHECK)

def serve(connection_str, ray_connect_handler=None):
    server = grpc.server(
        futures.ThreadPoolExecutor(
            max_workers=CLIENT_SERVER_MAX_THREADS,
            thread_name_prefix=&quot;ray_client_server&quot;,
        ),
    )
    # mode proxy
    task_servicer = RayletServicerProxy(None, proxy_manager)
    data_servicer = DataServicerProxy(proxy_manager)
    logs_servicer = LogstreamServicerProxy(proxy_manager)
    # else
    task_servicer = RayletServicer(ray_connect_handler)
    data_servicer = DataServicer(task_servicer)
    logs_servicer = LogstreamServicer()
    ray_client_pb2_grpc.add_RayletDriverServicer_to_server(task_servicer, server)
    ray_client_pb2_grpc.add_RayletDataStreamerServicer_to_server(data_servicer, server)
    ray_client_pb2_grpc.add_RayletLogStreamerServicer_to_server(logs_servicer, server)
    server.start()
</code></pre>
<pre><code class="language-python"># python/ray/util/client/server/server.py

# class RayletServicerProxy(ray_client_pb2_grpc.RayletDriverServicer):
class RayletServicer(ray_client_pb2_grpc.RayletDriverServicer):
    def KVPut(self, request, context=None) -&gt; ray_client_pb2.KVPutResponse:
    def KVGet(self, request, context=None) -&gt; ray_client_pb2.KVGetResponse:
    def KVDel(self, request, context=None) -&gt; ray_client_pb2.KVDelResponse:
    def KVList(self, request, context=None) -&gt; ray_client_pb2.KVListResponse:
    def ListNamedActors(...):
    def ClusterInfo(self, request, context=None) -&gt; ray_client_pb2.ClusterInfoResponse:
    def release(self, client_id: str, id: bytes) -&gt; bool:
    def release_all(self, client_id):
    def Terminate(self, req, context=None):
    def GetObject(self, request: ray_client_pb2.GetRequest, context):
    def PutObject( self, request: ray_client_pb2.PutRequest, context=None) -&gt; ray_client_pb2.PutResponse:
    def WaitObject(self, request, context=None) -&gt; ray_client_pb2.WaitResponse:
    def Schedule( self, task: ray_client_pb2.ClientTask, context=None) -&gt; ray_client_pb2.ClientTaskTicket:
    def lookup_or_register_func( self, id: bytes, client_id: str, options: Optional[Dict]) -&gt; ray.remote_function.RemoteFunction:
    def lookup_or_register_actor( self, id: bytes, client_id: str, options: Optional[Dict]):
    def unify_and_track_outputs(self, output, client_id):
</code></pre>
<h3 id="总结"><a class="header" href="#总结">总结</a></h3>
<p><code>ray start [--head]</code>, 将启动以下进程</p>
<pre><code class="language-shell"># 以下进程只在 head 节点运行
# GCS_SERVER_EXECUTABLE 
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/gcs/gcs_server 
/usr/bin/python3.7 -m ray.util.client.server --host=0.0.0.0 --port=10001 --mode=proxy

# worker 进程
# RAYLET_EXECUTABLE
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/raylet/raylet
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/dashboard/agent.py

/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/autoscaler/_private/monitor.py
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/dashboard/dashboard.py
/usr/bin/python3.7 -u /usr/local/lib/python3.7/dist-packages/ray/_private/log_monitor.py
</code></pre>
<pre><code class="language-shell"># 新版本中的以下进程已被移除
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:6379
/usr/local/lib/python3.7/dist-packages/ray/core/src/ray/thirdparty/redis/src/redis-server *:64712
</code></pre>
<pre><code class="language-shell"># java worker command
python ray/workers/setup_worker.py java -Dx=x -cp xx RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker

#  cpp worker command
cpp/default_worker
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="gcs-server"><a class="header" href="#gcs-server">GCS Server</a></h1>
<p>服务由以下二进制启动</p>
<pre><code class="language-shell">&quot;core/src/ray/gcs/gcs_server&quot;
</code></pre>
<p>程序入口</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_server_main.cc

int main(int argc, char *argv[]) {
  // 初始化配置
  RayConfig::instance().initialize(config_list);
  // 启动 IO service
  // class instrumented_io_context : public boost::asio::io_context {...}
  instrumented_io_context main_service;
  boost::asio::io_service::work work(main_service);
  // 初始化状态模块
  ray::stats::Init(global_tags, metrics_agent_port);

  // 启动 grpc 服务
  ray::gcs::GcsServerConfig gcs_server_config;
  gcs_server_config.grpc_server_name = &quot;GcsServer&quot;;
  ray::gcs::GcsServer gcs_server(gcs_server_config, main_service);
  gcs_server.Start();

  main_service.run();
}
</code></pre>
<p>主服务</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_server.cc

// 服务初始化，根据配置使用外置 redis 存储或者内置存储
GcsServer::GcsServer(...) {
  if (storage_type_ == &quot;redis&quot;) {
    gcs_table_storage_ = std::make_shared&lt;gcs::RedisGcsTableStorage&gt;(GetOrConnectRedis());
  } else if (storage_type_ == &quot;memory&quot;) {
    gcs_table_storage_ = std::make_shared&lt;InMemoryGcsTableStorage&gt;(main_service_);
  }
}

void GcsServer::Start() {
  // 异步加载 gcs tables 数据
  auto gcs_init_data = std::make_shared&lt;GcsInitData&gt;(gcs_table_storage_);
  gcs_init_data-&gt;AsyncLoad([this, gcs_init_data] { DoStart(*gcs_init_data); });
}

void GcsServer::DoStart(const GcsInitData &amp;gcs_init_data) {
  // Init cluster resource scheduler.
  // Init gcs resource manager.
  // Init synchronization service
  // Init gcs node manager.
  // Init gcs heartbeat manager.
  // Init KV Manager
  // Init function manager
  // Init Pub/Sub handler
  // Init RuntimeENv manager
  // Init gcs job manager.
  // Init gcs placement group manager.
  // Init gcs actor manager.
  // Init gcs worker manager.
  // Init stats handler.
  // Install event listeners.

  // 启动 rpc 服务，依赖 tables 数据加载完成
  // rpc::GrpcServer rpc_server_;
  rpc_server_.Run();
  // 心跳服务
  gcs_heartbeat_manager_-&gt;Start();

  RecordMetrics();
}
</code></pre>
<p>Table storage</p>
<pre><code class="language-cpp">// src/ray/gcs/gcs_server/gcs_table_storage.h

class GcsTable {}

class GcsTableWithJobId : public GcsTable&lt;Key, Data&gt; {}

class GcsTableStorage {};

class RedisGcsTableStorage : public GcsTableStorage {};

class InMemoryGcsTableStorage : public GcsTableStorage {};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="raylet"><a class="header" href="#raylet">Raylet</a></h1>
<p>服务由以下二进制启动</p>
<pre><code class="language-shell">core/src/ray/raylet/raylet
</code></pre>
<p>程序入口</p>
<pre><code class="language-cpp">// src/ray/raylet/main.cc

int main(int argc, char *argv[]) {

  // 启动 IO service
  // class instrumented_io_context : public boost::asio::io_context {...}
  instrumented_io_context main_service;
  boost::asio::io_service::work main_work(main_service);

  std::unique_ptr&lt;ray::raylet::Raylet&gt; raylet;
  ray::raylet::NodeManagerConfig node_manager_config;
  ray::ObjectManagerConfig object_manager_config;
  raylet = std::make_unique&lt;ray::raylet::Raylet&gt;(main_service,
                                                 raylet_socket_name,
                                                 node_ip_address,
                                                 node_manager_config,
                                                 object_manager_config,
                                                 gcs_client,
                                                 metrics_export_port);
  raylet-&gt;Start();
  main_service.run();
}

</code></pre>
<p>主服务类</p>
<pre><code class="language-cpp">// src/ray/raylet/raylet.cc

class Raylet {
  // 用于和 gcs 连接的客户端
  std::shared_ptr&lt;gcs::GcsClient&gt; gcs_client_;
  NodeManager node_manager_;
}

void Raylet::Start() {
  RAY_CHECK_OK(RegisterGcs());

  // Start listening for clients.
  DoAccept();
}

ray::Status Raylet::RegisterGcs() {
  node_manager_.RegisterGcs();

  gcs_client_-&gt;Nodes().RegisterSelf(self_node_info_, register_callback);
}

void Raylet::DoAccept() {
  acceptor_.async_accept(
      socket_,
      boost::bind(&amp;Raylet::HandleAccept, this, boost::asio::placeholders::error));
}

void Raylet::HandleAccept(const boost::system::error_code &amp;error) {
  // 建立本地连接并分发到 node manager 处理
  auto new_connection = ClientConnection::Create(
      client_handler, // node_manager_.ProcessNewClient(client);
      message_handler, // node_manager_.ProcessClientMessage(client, message_type, message.data());
      std::move(socket_),
      &quot;worker&quot;,
      node_manager_message_enum,
      static_cast&lt;int64_t&gt;(protocol::MessageType::DisconnectClient),
      message_data);
  // 处理连接
  DoAccept();
}
</code></pre>
<p>Node Manager</p>
<p>NodeManager 本身是一个 ServiceHandler，所以在初始化 node_manager_service_ 时，使用 this 作为 handler 传递。</p>
<pre><code class="language-cpp">// src/ray/raylet/node_manager.h

class NodeManager : public rpc::NodeManagerServiceHandler {
  std::shared_ptr&lt;gcs::GcsClient&gt; gcs_client_;
  std::unique_ptr&lt;HeartbeatSender&gt; heartbeat_sender_;
  WorkerPool worker_pool_;
  ObjectManager object_manager_;
  rpc::GrpcServer node_manager_server_;
  rpc::NodeManagerGrpcService node_manager_service_;

  std::unique_ptr&lt;rpc::AgentManagerServiceHandler&gt; agent_manager_service_handler_;
  rpc::AgentManagerGrpcService agent_manager_service_;

  std::shared_ptr&lt;ClusterResourceScheduler&gt; cluster_resource_scheduler_;
  std::shared_ptr&lt;LocalTaskManager&gt; local_task_manager_;

  std::shared_ptr&lt;PlacementGroupResourceManager&gt; placement_group_resource_manager_;
}

// src/ray/raylet/node_manager.cc

// Push
// Pull

NodeManager::NodeManager(...) {
  // 非常多的初始化配置
  node_manager_service_(io_service, *this),
  // 然后注册服务并启动
  node_manager_server_.RegisterService(node_manager_service_);
  node_manager_server_.RegisterService(agent_manager_service_);
  node_manager_server_.Run();
}
</code></pre>
<p>NodeManager 的 rpc 接口</p>
<pre><code class="language-cpp">// src/ray/rpc/node_manager/node_manager_server.h

// `NodeManagerService` 的接口, 对应 `src/ray/protobuf/node_manager.proto`.
class NodeManagerServiceHandler {}
// 目前有以下接口
// UpdateResourceUsage
// RequestResourceReport
// RequestWorkerLease
// ReportWorkerBacklog
// ReturnWorker
// ReleaseUnusedWorkers
// CancelWorkerLease
// PinObjectIDs
// GetNodeStats
// GlobalGC
// FormatGlobalMemoryInfo
// PrepareBundleResources
// CommitBundleResources
// CancelResourceReserve
// RequestObjectSpillage
// ReleaseUnusedBundles
// GetSystemConfig
// GetGcsServerAddress
// ShutdownRaylet

class NodeManagerGrpcService : public GrpcService {
  NodeManagerGrpcService(instrumented_io_context &amp;io_service,
                         NodeManagerServiceHandler &amp;service_handler)
}
</code></pre>
<blockquote>
<p>关于怎么增加新的接口可以参考: <code>src/ray/core_worker/core_worker.h</code>.</p>
</blockquote>
<p>ObjectManager </p>
<pre><code class="language-cpp">// src/ray/object_manager/object_manager.h

class ObjectManager : public ObjectManagerInterface, public rpc::ObjectManagerServiceHandler {
  instrumented_io_context rpc_service_;
  boost::asio::io_service::work rpc_work_;

  rpc::GrpcServer object_manager_server_;
  rpc::ObjectManagerGrpcService object_manager_service_;
}

// 主要接口
// Push
// Pull

// src/ray/object_manager/object_manager.cc

ObjectManager::ObjectManager(...){
  rpc_work_(rpc_service_),
  object_manager_server_(&quot;ObjectManager&quot;,...)
  object_manager_service_(rpc_service_, *this),
  StartRpcService();
}

void ObjectManager::StartRpcService() {
  // for i in config_.rpc_service_threads_number
  rpc_threads_[i] = std::thread(&amp;ObjectManager::RunRpcService, this, i);
  object_manager_server_.RegisterService(object_manager_service_);
  object_manager_server_.Run();
}

void ObjectManager::RunRpcService(int index) {
  rpc_service_.run();
}
</code></pre>
<pre><code class="language-cpp">// src/ray/rpc/object_manager/object_manager_server.h


class ObjectManagerGrpcService : public GrpcService {
  ObjectManagerGrpcService(instrumented_io_context &amp;io_service,
                           ObjectManagerServiceHandler &amp;service_handler)
      : GrpcService(io_service), service_handler_(service_handler){};
</code></pre>
<p>Worker Pool</p>
<pre><code class="language-cpp">// src/ray/raylet/worker_pool.cc
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="api"><a class="header" href="#api">API</a></h1>
<pre><code class="language-python">import ray
ray.init()
# ray.init(address='ray://localhost:10001')

@ray.remote
def f(x):
    return x * x

futures = [f.remote(i) for i in range(4)]
print(ray.get(futures))
</code></pre>
<pre><code class="language-python"># python/ray/__init__.py

from ray.worker import (  # noqa: E402,F401
    get,
    init,
    put,
    remote,
    wait,
)
</code></pre>
<p>API <code>ray.init</code>, <code>ray.remote</code>, <code>ray.get</code> 都来自 <code>ray.worker</code>.</p>
<pre><code class="language-python"># python/ray/worker.py

def init(address: Optional[str] = None, ...):
    # if address
    builder = ray.client(address, _deprecation_warn_enabled=False)
    builder._init_args(**passed_kwargs)
    return builder.connect()

    # if bootstrap_address is None:
    _global_node = ray.node.Node(head=True, shutdown_at_exit=False, spawn_reaper=True, ray_params=ray_params)
    # else
    _global_node = ray.node.Node(ray_params, head=False, shutdown_at_exit=False, spawn_reaper=False, connect_only=True)

    connect(...)
    return RayContext(...)

def connect(node, worker=global_worker, ...):
    worker.node = node
    worker.core_worker = ray._raylet.CoreWorker(...)
</code></pre>
<p>CoreWorker</p>
<pre><code class="language-cpp">// src/ray/core_worker/core_worker.h

class CoreWorker : public rpc::CoreWorkerServiceHandler {
  instrumented_io_context io_service_;
  boost::asio::io_service::work io_work_;

  rpc::CoreWorkerGrpcService grpc_service_;
  std::unique_ptr&lt;rpc::GrpcServer&gt; core_worker_server_;
  
  // std::unique_ptr&lt;ObjectRecoveryManager&gt; object_recovery_manager_;

  std::shared_ptr&lt;TaskManager&gt; task_manager_;
  std::unique_ptr&lt;ActorManager&gt; actor_manager_;

  /// Implements gRPC server handler.
  void HandleXxxx(const rpc::PushTaskRequest &amp;request,
                      rpc::PushTaskReply *reply,
                      rpc::SendReplyCallback send_reply_callback) override;
}


// src/ray/core_worker/core_worker.cc

CoreWorker::CoreWorker(...) {
  io_work_(io_service_),
  grpc_service_(io_service_, *this),

  core_worker_server_-&gt;RegisterService(grpc_service_);
  core_worker_server_-&gt;Run();
}

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="pytorch"><a class="header" href="#pytorch">pytorch</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="tensor"><a class="header" href="#tensor">tensor</a></h1>
<p>The dependancy of tensor related API</p>
<p><img src="pytorch/tensor-api.png" alt="Tensor View API Dependance" /></p>
<p>Tensor view explication</p>
<p><img src="pytorch/tensor-view.png" alt="Tensor View API Dependance" /></p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="pytorch-profiler"><a class="header" href="#pytorch-profiler">PyTorch Profiler</a></h1>
<h2 id="demo"><a class="header" href="#demo">Demo</a></h2>
<p>体现基本流程的示例</p>
<pre><code class="language-python">import torch
import torchvision.models as models
from torch.profiler import profile, record_function, ProfilerActivity

# 创建模型，需要 profile 的对象
model = models.resnet18()
inputs = torch.randn(5, 3, 224, 224)

# 配置
prof = profile(activities=[ProfilerActivity.CPU], record_shapes=True)

prof.start()

model(inputs)

prof.stop()

# 结果分析和输出
print(prof.key_averages().table(sort_by=&quot;cpu_time_total&quot;, row_limit=10))

prof.export_chrome_trace(&quot;trace.json&quot;)
</code></pre>
<p>其中 <code>torch.profiler.profile</code> 可以使用 <code>with</code> 语法</p>
<pre><code class="language-python">with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
    with record_function(&quot;model_inference&quot;):
        model(inputs)
</code></pre>
<p>输出</p>
<pre><code class="language-shell">---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
                 model_inferencex         0.10%      15.353ms       100.00%       14.951s       14.951s             1
                 aten::batch_norm         0.00%     292.000us        43.14%        6.449s     322.464ms            20
     aten::_batch_norm_impl_index         0.00%     567.000us        43.13%        6.449s     322.450ms            20
          aten::native_batch_norm        32.92%        4.921s        43.13%        6.448s     322.419ms            20
                     aten::conv2d         0.00%     310.000us        42.00%        6.279s     313.938ms            20
                aten::convolution         0.00%     350.000us        41.99%        6.278s     313.923ms            20
               aten::_convolution         0.00%     601.000us        41.99%        6.278s     313.905ms            20
         aten::mkldnn_convolution        41.98%        6.276s        41.99%        6.278s     313.875ms            20
                       aten::mean         0.01%       1.209ms        10.54%        1.576s      75.043ms            21
                        aten::sum        10.45%        1.562s        10.45%        1.562s      74.386ms            21
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------
Self CPU time total: 14.951s
</code></pre>
<h3 id="api-1"><a class="header" href="#api-1">API</a></h3>
<p><code>torch.profiler.profile</code></p>
<p>参数说明</p>
<ul>
<li><code>activities</code> list 类型，profile 的内容，支持 <code>torch.profiler.ProfilerActivity.CPU</code> 和 <code>torch.profiler.ProfilerActivity.CUDA</code>，这里的设置需要和模型使用的 device 一致</li>
<li><code>schedule</code> 默认会持续记录所有事件，使用 scheduler() 作为帮助函数生成 schedule 函数，自定义记录逻辑</li>
<li><code>on_trace_ready</code> 配合 <code>schedule</code> 使用，在它返回 <code>ProfilerAction.RECORD_AND_SAVE</code> 后被调用</li>
<li><code>record_shapes</code> 是否记录 input shapes</li>
<li><code>profile_memory</code> 是否记录 内存/显存, 和 <code>activities</code> 对应</li>
<li><code>with_stack</code> 是否开启调用文件信息源的记录，包括代码文件和行号</li>
<li><code>with_flops</code> 预估FLOPs，主要针对 matrix multiplication and 2D convolution</li>
<li><code>with_modules</code> 层级记录，暂时只针对 TorchScript</li>
</ul>
<p><code>ProfilerAction</code> 用于状态的记录和转换</p>
<pre><code class="language-python">class ProfilerAction(Enum):
    NONE = 0
    WARMUP = 1
    RECORD = 2
    RECORD_AND_SAVE = 3
</code></pre>
<p><code>profile</code> 对象</p>
<pre><code class="language-python"># torch/profiler/profiler.py

# Profiler context manager
class profile(_KinetoProfile):

    def __init__(...):
        # 记录函数
        self.step_rec_fn: Optional[prof.record_function] = None

        # 状态转换时会触发一系列操作，action_map 记录里任意两个状态转换时执行的动作
        self.action_map: Dict[Tuple[ProfilerAction, Optional[ProfilerAction]], List[Any]] = {
            (ProfilerAction.NONE, ProfilerAction.WARMUP): [self.prepare_trace],
            (ProfilerAction.NONE, ProfilerAction.RECORD): [self.prepare_trace, self.start_trace],
            ...
        }

    def start(self):
        self._transit_action(ProfilerAction.NONE, self.current_action)
        if self.record_steps:
            self.step_rec_fn = prof.record_function(&quot;ProfilerStep#&quot; + str(self.step_num))
            self.step_rec_fn.__enter__()

    def stop(self):
        if self.record_steps and self.step_rec_fn:
            self.step_rec_fn.__exit__(None, None, None)
        self._transit_action(self.current_action, None)

    def step(self):
        self.step_num += 1
        # schedule 接受 step 数，返回当前 action
        self.current_action = self.schedule(self.step_num)

        # 转换状态，触发 map 中定义的动作
        self._transit_action(prev_action, self.current_action)

        if self.record_steps:
            self.step_rec_fn = prof.record_function(&quot;ProfilerStep#&quot; + str(cur_step))
            self.step_rec_fn.__enter__()
</code></pre>
<p><code>schedule</code></p>
<pre><code class="language-python">def schedule(*, wait: int, warmup: int, active: int, repeat: int = 0, skip_first: int = 0) -&gt; Callable:
    # skip_fist + ( wait + warmup + active ) * repeat
    # NONE                 WARMUP   RECORD RECORD_AND_SAVE
    def schedule_fn(step: int) -&gt; ProfilerAction:
        # 根据 step 返回 当前的状态
    return schedule_fn
</code></pre>
<p><code>record_function</code></p>
<pre><code class="language-python"># torch/autograd/profiler.py

class record_function(ContextDecorator):
    def __init__(self, name: str, args: Optional[str] = None):
        self.record = torch.jit.annotate(Optional[&quot;torch.classes.profiler._RecordFunction&quot;], None)

    def __enter__(self):
        self.record = torch.ops.profiler._record_function_enter_new(self.name, self.args)

    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):
        torch.ops.profiler._record_function_exit(self.record)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paddle"><a class="header" href="#paddle">paddle</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="paddle-ps-代码分析"><a class="header" href="#paddle-ps-代码分析">paddle ps 代码分析</a></h1>
<h2 id="python-前端"><a class="header" href="#python-前端">python 前端</a></h2>
<h3 id="api-2"><a class="header" href="#api-2">API</a></h3>
<pre><code class="language-python">import paddle.distributed.fleet as fleet

fleet.init()

if fleet.is_server():
    fleet.init_server()
    fleet.run_server()
elif fleet.is_worker():
    run_worker()
    fleet.stop_worker()

def run_worker():
    # paddle.static.Executor(place)
    exe.run(paddle.static.default_startup_program())
    fleet.init_worker()
    exe.train_from_dataset(...)

# Fin, fleet is optimizer
</code></pre>
<h3 id="fleet-initoptimizer"><a class="header" href="#fleet-initoptimizer">fleet init/optimizer</a></h3>
<pre><code class="language-python"># python/paddle/distributed/fleet/base/fleet_base.py

def init(self, role_maker=None, is_collective=False, strategy=None):
    # 配置之集大成者，就是各种配置，细到训练参数，粗到训练模式，开关
    strategy = DistributedStrategy()
    # role maker 包含分布式信息，基本上对接 launch 信息
    # 也负责初始化如 gloo 之类的工具
    self._role_maker._generate_role()

def minimize(...)
    def _minimize_impl(...)
        # runtime handle 做映射 init_server/_init_server, run_server/_run_server
        self._runtime_handle = RuntimeFactory()._create_runtime(context)
</code></pre>
<h3 id="runtime"><a class="header" href="#runtime">runtime</a></h3>
<pre><code class="language-python"># 使用实例
# python/paddle/distributed/ps/the_one_ps.py
class TheOnePSRuntime(RuntimeBase):
    def __init__(self):
        super(TheOnePSRuntime, self).__init__()
        self._communicator = None
        self._server = None
        self._worker = fluid.core.DistFleetWrapper()
        self._server_sub_program = []
        self._heter_client = None
        self._send_ctx = None
</code></pre>
<h3 id="pybind"><a class="header" href="#pybind">pybind</a></h3>
<pre><code class="language-cpp">void BindDistFleetWrapper(py::module* m) {
  py::class_&lt;FleetWrapper, std::shared_ptr&lt;FleetWrapper&gt;&gt;(*m,
                                                          &quot;DistFleetWrapper&quot;)
      .def(py::init([]() { return FleetWrapper::GetInstance(); }))
      .def(&quot;load_sparse&quot;, &amp;FleetWrapper::LoadSparseOnServer)
      .def(&quot;load_model&quot;, &amp;FleetWrapper::LoadModel)
      .def(&quot;load_one_table&quot;, &amp;FleetWrapper::LoadModelOneTable)
      .def(&quot;init_server&quot;, &amp;FleetWrapper::InitServer)
      .def(&quot;run_server&quot;,
           (uint64_t (FleetWrapper::*)(void)) &amp; FleetWrapper::RunServer)
      .def(&quot;run_server&quot;, (uint64_t (FleetWrapper::*)(          // NOLINT
                             const std::string&amp;, uint32_t)) &amp;  // NOLINT
                             FleetWrapper::RunServer)
      .def(&quot;init_worker&quot;, &amp;FleetWrapper::InitWorker)
      .def(&quot;push_dense_params&quot;, &amp;FleetWrapper::PushDenseParamSync)
      .def(&quot;pull_dense_params&quot;, &amp;FleetWrapper::PullDenseVarsSync)
      .def(&quot;save_all_model&quot;, &amp;FleetWrapper::SaveModel)
      .def(&quot;save_one_model&quot;, &amp;FleetWrapper::SaveModelOneTable)
      .def(&quot;recv_and_save_model&quot;, &amp;FleetWrapper::RecvAndSaveTable)
      .def(&quot;sparse_table_stat&quot;, &amp;FleetWrapper::PrintTableStat)
      .def(&quot;stop_server&quot;, &amp;FleetWrapper::StopServer)
      .def(&quot;stop_worker&quot;, &amp;FleetWrapper::FinalizeWorker)
      .def(&quot;barrier&quot;, &amp;FleetWrapper::BarrierWithTable)
      .def(&quot;shrink_sparse_table&quot;, &amp;FleetWrapper::ShrinkSparseTable)
      .def(&quot;set_clients&quot;, &amp;FleetWrapper::SetClients)
      .def(&quot;get_client_info&quot;, &amp;FleetWrapper::GetClientsInfo)
      .def(&quot;create_client2client_connection&quot;,
           &amp;FleetWrapper::CreateClient2ClientConnection);
}
</code></pre>
<h2 id="fleet-run_server"><a class="header" href="#fleet-run_server">fleet run_server</a></h2>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_server(self, dirname=None, var_names=None, **kwargs):
        # cpp instance
        self._server = fluid.core.DistFleetWrapper()
        self._server.init_server(server_desc, self.string_hosts, role_id,
                                 trainers, self._server_sub_program)
        # load_sparse 
        for var_name in load_varnames:
            table_id = sparse_table_maps[var_name]
            self._server.load_sparse(dirname, &quot;0&quot;, table_id)

    def _run_server(self):
        self._server.run_server(host, int(port))
</code></pre>
<h3 id="fleetwrapper"><a class="header" href="#fleetwrapper">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
// FleetWrapper 层
void FleetWrapper::InitServer(...){
    pserver_ptr_ = std::shared_ptr&lt;paddle::distributed::PSCore&gt;(
        new paddle::distributed::PSCore());
    pserver_ptr_-&gt;init_server(...)
}

uint64_t FleetWrapper::RunServer(...){
    auto ret = pserver_ptr_-&gt;run_server(ip, port);
}

void FleetWrapper::LoadSparseOnServer(...){
    // _server_ptr is PSServer
    pserver_ptr_-&gt;_server_ptr-&gt;table(table_id)-&gt;load(path, meta);
}
</code></pre>
<h3 id="pscore"><a class="header" href="#pscore">PSCore</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_service/service.cc
// PSCore layer

int PSCore::init_server(...){
  _ps_env = paddle::distributed::PaddlePSEnvironment();
  _ps_env.set_ps_servers(host_sign_list, node_num);
  _ps_env.set_trainers(trainers); // 没啥用
  _server_ptr = std::shared_ptr&lt;paddle::distributed::PSServer&gt;(
      paddle::distributed::PSServerFactory::create(_ps_param));
  ret = _server_ptr-&gt;configure(_ps_param, _ps_env, index, server_sub_program);
}

uint64_t PSCore::run_server(const std::string&amp; ip, uint32_t port) {
  return _server_ptr-&gt;start(ip, port);
}
</code></pre>
<h3 id="psserver"><a class="header" href="#psserver">PSServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/server.cc
// PSServer layer

PSServer *PSServerFactory::create(const PSParameter &amp;ps_config) {
    PSServer *server =
      CREATE_PSCORE_CLASS(PSServer, service_param.server_class());
    TableManager::instance().initialize();
}

int32_t PSServer::configure(...){
    // for i in downpour_param.downpour_table_param_size()
    auto *table = CREATE_PSCORE_CLASS(
        Table, downpour_param.downpour_table_param(i).table_class());
    table-&gt;set_program_env(scope_.get(), place_, &amp;server_sub_program);
    table-&gt;set_shard(_rank, shard_num);
    table-&gt;initialize(downpour_param.downpour_table_param(i),
                      config.fs_client_param());
    _table_map[downpour_param.downpour_table_param(i).table_id()].reset(table);

    return initialize();
}
</code></pre>
<h3 id="brpcpsserver"><a class="header" href="#brpcpsserver">BrpcPsServer</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_server.h
class BrpcPsServer : public PSServer {
    brpc::Server _server;
}

// paddle/fluid/distributed/ps/service/brpc_ps_server.cc
int32_t BrpcPsServer::initialize() {
    auto *service =
      CREATE_PSCORE_CLASS(PsBaseService, service_config.service_class());
    _server.AddService(service, brpc::SERVER_DOESNT_OWN_SERVICE)
}
uint64_t BrpcPsServer::start(const std::string &amp;ip, uint32_t port) {
    auto trainers = _environment-&gt;get_trainers(); // 可以去掉
    _server.Start(ip_port.c_str(), &amp;options)
    _environment-&gt;registe_ps_server(ip, port, _rank);
}
</code></pre>
<h3 id="brpcpsservice"><a class="header" href="#brpcpsservice">BrpcPsService</a></h3>
<pre><code class="language-cpp">class BrpcPsService : public PsBaseService {
  int32_t initialize_shard_info(...)
  int32_t pull_dense(...)
  int32_t push_dense(...)
  int32_t push_dense_param(...)
  int32_t push_sparse_param(...)
  int32_t pull_sparse(...)
  int32_t pull_geo_param(...)
  int32_t barrier(...)
  int32_t push_sparse(...)
  int32_t load_one_table(...)
  int32_t load_all_table(...)
  int32_t save_one_table(...)
  int32_t save_all_table(...)
  int32_t shrink_table(...)
  int32_t clear_one_table(...)
  int32_t clear_all_table(...)
  int32_t stop_server(...)
  int32_t start_profiler(...)
  int32_t stop_profiler(...)
  int32_t print_table_stat(...)
  int32_t push_global_step(...)
}
</code></pre>
<h2 id="fleet-run_worker"><a class="header" href="#fleet-run_worker">fleet run_worker</a></h2>
<h3 id="runtime-1"><a class="header" href="#runtime-1">runtime</a></h3>
<pre><code class="language-python"># runtime 层初始化
class TheOnePSRuntime(RuntimeBase):
    def _init_worker(self, scopes=None):
        # in init
        # self._worker = fluid.core.DistFleetWrapper()
        self._worker.init_worker(proto_txt, self.string_hosts, role_id)
        # GEO mode
        self._communicator = Communicator(...)
        self._communicator.init_with_ctx(...)
        # 
        info = self._worker.get_client_info()
        self._worker.set_clients(all_info) # _all_gather info is all_info
        self._worker.create_client2client_connection()
        #
        self._pull_all_dense(scopes, send_ctx, dense_map)
        # GEO mode
        self._communicator.start()    

    def _pull_all_dense(self, scopes, send_ctx, recv_map):
        for name, ctx in send_ctx.items():
            self._worker.pull_dense_params(scope, table_id, var_names)
</code></pre>
<h3 id="init-worker"><a class="header" href="#init-worker">init worker</a></h3>
<h3 id="fleetwrapper-1"><a class="header" href="#fleetwrapper-1">FleetWrapper</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/wrapper/fleet.cc
void FleetWrapper::InitWorker(...){
    ps_env_.set_ps_servers(&amp;host_sign_list, servers);
    worker_ptr_ = std::shared_ptr&lt;paddle::distributed::PSClient&gt;(
          paddle::distributed::PSClientFactory::create(ps_param));
    worker_ptr_-&gt;configure(ps_param, dense_pull_regions, ps_env_, index);
}

void FleetWrapper::PullDenseVarsSync(...){
    auto status = worker_ptr_-&gt;pull_dense(regions.data(), regions.size(), tid);
    status.wait();
}

int FleetWrapper::SetClients(std::vector&lt;uint64_t&gt;&amp; host_sign_list) {
    return ps_env_.set_ps_clients(host_sign_list.data(), node);
}
void FleetWrapper::CreateClient2ClientConnection() {
    worker_ptr_-&gt;create_client2client_connection(...)
}
</code></pre>
<h3 id="psclient"><a class="header" href="#psclient">PSClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/ps_client.cc

PSClient *PSClientFactory::create(const PSParameter &amp;ps_config) {
    PSClient *client = CREATE_PSCORE_CLASS(PSClient, service_param.client_class());
    TableManager::instance().initialize();
}

int32_t PSClient::configure(...){
    // for i in work_param.downpour_table_param_size()
    auto *accessor = CREATE_PSCORE_CLASS(
        ValueAccessor,
        work_param.downpour_table_param(i).accessor().accessor_class());
    accessor-&gt;configure(work_param.downpour_table_param(i).accessor());
    accessor-&gt;initialize();
    _table_accessors[work_param.downpour_table_param(i).table_id()].reset(accessor);
    return initialize();
}
</code></pre>
<h3 id="brpcpsclient"><a class="header" href="#brpcpsclient">BrpcPsClient</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc
class BrpcPsClient : public PSClient {
    brpc::Server _server;
    DownpourPsClientService _service;
}

int32_t BrpcPsClient::initialize() {
    // for i in server_list.size()
    _server_channels[i][j].reset(new brpc::Channel());
    _server_channels[i][j]-&gt;Init(server_ip_port.c_str(), &quot;&quot;, &amp;options)
    // 启动client探听接口, 并相互建立连接
    start_client_service();
    // 异步push 请求队列初始化
    _push_dense_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;DenseAsyncTask *&gt;();
    _push_sparse_task_queue_map[table_id] = paddle::framework::MakeChannel&lt;SparseAsyncTask *&gt;();
    // 启动异步push线程
    _async_push_sparse_thread = std::thread(std::bind(&amp;BrpcPsClient::push_sparse_task_consume, this));
    // _async_push_sparse_thread.detach();
    _async_push_dense_thread = std::thread(std::bind(&amp;BrpcPsClient::push_dense_task_consume, this));
}

// 启动client端RpcService 用于数据互发等操作
int32_t BrpcPsClient::start_client_service() {
    _service.configure(this, _client_id)
    _server.AddService(&amp;_service, brpc::SERVER_DOESNT_OWN_SERVICE);
    _server.Start(butil::my_ip_cstr(), brpc::PortRange(start_port, max_port), &amp;options)
    _env-&gt;registe_ps_client(...)
}

// how 弹性？？？
int32_t BrpcPsClient::create_client2client_connection(...){
    // for i in client_list.size()
    _client_channels[i].reset(new brpc::Channel());
    _client_channels[i]-&gt;Init(server_ip_port.c_str(), &quot;&quot;, &amp;options)
}
</code></pre>
<h3 id="downpourpsclientservice"><a class="header" href="#downpourpsclientservice">DownpourPsClientService</a></h3>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/brpc_ps_client.cc

class DownpourPsClientService : public PsService {
    PSClient *_client;
    void service(...)
}
</code></pre>
<h2 id="communicator"><a class="header" href="#communicator">communicator</a></h2>
<pre><code class="language-python"># python/paddle/fluid/communicator.py

class Communicator(object):
    def init_with_ctx(self,...):
        self.communicator_ = core.DistCommunicator(self.mode,...)
    def start(self):
        # Start communicator. Should call before training process.
        self.communicator_.start()
</code></pre>
<h3 id="bind"><a class="header" href="#bind">bind</a></h3>
<pre><code class="language-cpp">// paddle/fluid/pybind/communicator_py.cc
void BindCommunicator(py::module* m) {
  // Communicator is already used by nccl, change to DistCommunicator
  py::class_&lt;Communicator, std::shared_ptr&lt;Communicator&gt;&gt;(*m, &quot;DistCommunicator&quot;)
  .def(py::init([](...){Communicator::InitInstance&lt;GeoCommunicator&gt;(...)}
  .def(&quot;start&quot;, &amp;Communicator::Start)
// paddle/fluid/distributed/ps/service/communicator/communicator.h
static Communicator *InitInstance(...){
    std::call_once(init_flag_, &amp;Communicator::InitWithRpcCtx&lt;T&gt;,...);
}
static void InitWithRpcCtx(...){
    communicator_.reset(new T(std::ref(envs)));
    communicator_-&gt;InitEnvs();
    communicator_-&gt;InitBrpcClient(dist_desc, host_sign_list);
    communicator_-&gt;InitImpl(send_ctx, recv_ctx, recv_scope);
}
</code></pre>
<pre><code class="language-cpp">// paddle/fluid/distributed/ps/service/communicator/communicator.cc
void Communicator::InitBrpcClient(...){
    auto fleet = paddle::distributed::FleetWrapper::GetInstance();
    _worker_ptr = fleet-&gt;worker_ptr_;
}
void AsyncCommunicator::InitImpl(...){
    // for varnames
    send_varname_to_queue_[var_name] = std::make_shared&lt;BlockingQueue&lt;std::shared_ptr&lt;Variable&gt;&gt;&gt;(send_queue_size_);
    send_threadpool_.reset(new ::ThreadPool(thread_pool_size_));
    }

void AsyncCommunicator::Start() {
    main_thread_.reset(new std::thread(std::bind(&amp;AsyncCommunicator::MainThread, this))); // MainThread/RecvThread
}

void AsyncCommunicator::MainThread() {
    while (running_) {
        SendByCommunicator();
        RpcProfilerControl();
    }
}
void AsyncCommunicator::RecvThread() {
    while (running_) {
        RecvByCommunicator();
    }
}
</code></pre>
<h2 id="train_from_dataset"><a class="header" href="#train_from_dataset">train_from_dataset</a></h2>
<pre><code class="language-python"># dataset
dataset = paddle.distributed.InMemoryDataset() # &quot;MultiSlotInMemoryDataFeed&quot;
dataset.load_into_memory()
dataset.init(...)
dataset.set_filelist(train_files_list)

# InMemoryDataset -- MultiSlotInMemoryDataFeed  -- InMemoryDataFeed -- DataFeed
# QueueDataset -- MultiSlotDataFeed -- PrivateQueueDataFeed -- DataFeed
# python/paddle/fluid/executor.py
</code></pre>
<pre><code class="language-python"># class Executor(object):
def train_from_dataset(self,...):
    return self._run_from_dataset(...)

def _run_from_dataset(self,...):
    # dataset
    dataset = paddle.fluid.DatasetFactory().create_dataset(...)
    dataset.set_xxx(...)
    dataset._prepare_to_run()
    # trainer
    scope, trainer = self._prepare_trainer(...)
    trainer._gen_trainer_desc()
    # self._default_executor = core.Executor(p)
    trainer_instance = self._default_executor.init_for_dataset(
                    program.desc, trainer._desc(), scope, dataset.dataset)
    # run
    self._default_executor.run_from_dataset(trainer_instance)

def _prepare_trainer(self,...):
    trainer = TrainerFactory()._create_trainer(program.program._fleet_opt)
    # trainer._set_thread(thread)
</code></pre>
<h3 id="excutor"><a class="header" href="#excutor">excutor</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/executor.cc

std::shared_ptr&lt;TrainerBase&gt; Executor::InitForDataset(...){
  // MultiTrainer
  std::shared_ptr&lt;TrainerBase&gt; trainer;
  trainer = TrainerFactory::CreateTrainer(trainer_desc.class_name());
  // initialize trainer
  trainer-&gt;Initialize(trainer_desc, dataset);
  trainer-&gt;SetScope(scope);
  // prepare training environment and helper environment
  trainer-&gt;InitTrainerEnv(main_program, place_);
  // Try to init other environment
  trainer-&gt;InitOtherEnv(main_program);
}

void Executor::RunFromDataset(std::shared_ptr&lt;TrainerBase&gt; trainer) {
    trainer-&gt;Run();
}
</code></pre>
<h3 id="multitrainer"><a class="header" href="#multitrainer">MultiTrainer</a></h3>
<pre><code class="language-cpp">//paddle/fluid/framework/trainer.h
class MultiTrainer : public TrainerBase {
    std::vector&lt;DataFeed*&gt; readers_;
    std::vector&lt;std::shared_ptr&lt;DeviceWorker&gt;&gt; workers_;
}

// paddle/fluid/framework/multi_trainer.cc
void MultiTrainer::Initialize(const TrainerDesc&amp; trainer_desc, Dataset* dataset) {
    // Dataset -&gt; DataFeed
    const std::vector&lt;paddle::framework::DataFeed*&gt; readers = dataset-&gt;GetReaders();
    thread_num_ = readers.size(); // !!! thread num
    workers_.resize(thread_num_); 
    // for i in thread_num_
    workers_[i] = DeviceWorkerFactory::CreateDeviceWorker(...)
    workers_[i]-&gt;Setxxx()
    workers_[i]-&gt;Initialize(trainer_desc);
    workers_[i]-&gt;SetDataFeed(readers[i]);
}

void MultiTrainer::Run() {
    // for i in thread_num_
    threads_.push_back(std::thread(&amp;DeviceWorker::TrainFiles, workers_[thidx].get()));
    // for th in threads_
    th.join();
}
</code></pre>
<h3 id="hogwildworker"><a class="header" href="#hogwildworker">HogwildWorker</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/device_worker.cc
void DeviceWorker::SetDataFeed(DataFeed* data_feed) {
  device_reader_ = data_feed;
}

// paddle/fluid/framework/hogwild_worker.cc
void HogwildWorker::Initialize(const TrainerDesc &amp;desc) {
}

void HogwildWorker::TrainFiles() {
    device_reader_-&gt;Start();
    while ((cur_batch = device_reader_-&gt;Next()) &gt; 0) {
        // for op in ops_
        op-&gt;Run(*thread_scope_, place_);
    }
}
</code></pre>
<h3 id="multislotinmemorydatafeed"><a class="header" href="#multislotinmemorydatafeed">MultiSlotInMemoryDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class InMemoryDataFeed : public DataFeed {
    // 下面的 channel 赋值在 DatasetImpl&lt;T&gt;::CreateReaders()
    // input 为全局，output 和 consume 独立
    paddle::framework::ChannelObject&lt;T&gt;* input_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* output_channel_;
    paddle::framework::ChannelObject&lt;T&gt;* consume_channel_;
}

bool InMemoryDataFeed&lt;T&gt;::Start() {
    //  input
    channel
    global channel
    input_channel_-&gt;Read(data); 
    output_channel_-&gt;Write(std::move(data));
}

int InMemoryDataFeed&lt;T&gt;::Next() {
    while (index &lt; this-&gt;default_batch_size_) {
        output_channel_-&gt;Get(instance);
        ins_vec.push_back(instance);
        ++index;
        consume_channel_-&gt;Put(std::move(instance));
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotInMemoryDataFeed : public InMemoryDataFeed&lt;Record&gt; {
}
</code></pre>
<h3 id="multislotdatafeed"><a class="header" href="#multislotdatafeed">MultiSlotDataFeed</a></h3>
<pre><code class="language-cpp">// paddle/fluid/framework/data_feed.cc

class PrivateQueueDataFeed : public DataFeed {
    std::shared_ptr&lt;paddle::framework::ChannelObject&lt;T&gt;&gt; queue_;
}

bool PrivateQueueDataFeed&lt;T&gt;::Start() {
    read_thread_ = std::thread(&amp;PrivateQueueDataFeed::ReadThread, this);
}
void PrivateQueueDataFeed&lt;T&gt;::ReadThread() {
    while (PickOneFile(&amp;filename)) {
        fp_ = fs_open_read(filename, &amp;err_no, pipe_command_);
        while (ParseOneInstanceFromPipe(&amp;instance)) {
            queue_-&gt;Put(instance);
        }
    }
}
int PrivateQueueDataFeed&lt;T&gt;::Next() {
    while (index &lt; default_batch_size_) {
        queue_-&gt;Get(instance)
        AddInstanceToInsVec(&amp;ins_vec, instance, index++);
    }
    PutToFeedVec(ins_vec);
}

class MultiSlotDataFeed : public PrivateQueueDataFeed&lt;std::vector&lt;MultiSlotType&gt;&gt; {
}
</code></pre>
<h4 id="misc"><a class="header" href="#misc">Misc</a></h4>
<ol>
<li>InMemoryDataset 流程分析</li>
</ol>
<ul>
<li>
<p>LoadIntoMemory 把文件读取进 input_channel_，注意 input_channel_ 是全局共享，由 GetReaders() 返回时设定；</p>
</li>
<li>
<p>Start() 从 input_channel_ 读取一份数据进 output_channel_</p>
</li>
<li>
<p>Next() 从 output_channel_ 取数据进 consume_channel_</p>
</li>
</ul>
<ol start="2">
<li>input_channel_ 在哪里初始化？
data_set.cc 中 DatasetImpl<T>::CreateChannel()，它是全局的，最终调用在 dataset.py 中 self.dataset.create_channel()，所以 InMemoryDataset 有调用，QueueDataset 没有调用</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="horovod"><a class="header" href="#horovod">Horovod</a></h1>
<p>Horovod core principles are based on MPI concepts such as size, rank, local rank, allreduce, allgather, broadcast, and alltoall</p>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="run"><a class="header" href="#run">Run</a></h1>
<h3 id="启动"><a class="header" href="#启动">启动</a></h3>
<pre><code class="language-python">setup(name='horovod',
      entry_points={
          'console_scripts': [
              'horovodrun = horovod.runner.launch:run_commandline'
          ]
      })
</code></pre>
<pre><code class="language-python"># horovod/runner/launch.py

def run_commandline():
    args = parse_args()
    _run(args)

def _run(args):
    # set args.hosts
    if _is_elastic(args):
        return _run_elastic(args)
    else:
        return _run_static(args)
</code></pre>
<h3 id="非弹性启动"><a class="header" href="#非弹性启动">非弹性启动</a></h3>
<pre><code class="language-python">def _run_static(args):
    settings = hvd_settings.Settings(...)
    nics = driver_service.get_common_interfaces(settings, all_host_names,
                                                remote_host_names, fn_cache)
    if args.run_func:
        executable = args.executable or sys.executable
        command = [executable, '-m', 'horovod.runner.run_task', str(driver_ip), str(run_func_server_port)]
    else:
        command = args.command
    _launch_job(args, settings, nics, command)

def _launch_job(args, settings, nics, command):
    def gloo_run_fn():
        driver_ip = network.get_driver_ip(nics)
        gloo_run(settings, nics, env, driver_ip, command)

    def mpi_run_fn():
        mpi_run(settings, nics, env, command)

    def js_run_fn():
        js_run(settings, nics, env, command)

    run_controller(args.use_gloo, gloo_run_fn,
                   args.use_mpi, mpi_run_fn,
                   args.use_jsrun, js_run_fn,
                   args.verbose)

def run_controller(use_gloo, gloo_run, use_mpi, mpi_run, use_jsrun, js_run, verbosity):
    if use_gloo:
        gloo_run()
    elif use_mpi:
        mpi_run()
    elif use_jsrun:
        js_run()

from horovod.runner.gloo_run import gloo_run, gloo_run_elastic
from horovod.runner.mpi_run import mpi_run
from horovod.runner.js_run import js_run, is_jsrun_installed
</code></pre>
<pre><code class="language-python"># horovod/runner/gloo_run.py

def gloo_run(settings, nics, env, server_ip, command):
    # 启动命令通过 ssh 分发，如果出错所有进程将被 kill
    # 先封装执行函数
    exec_command = _exec_command_fn(settings)
    # 再调用执行
    launch_gloo(command, exec_command, settings, nics, env, server_ip)

def _exec_command_fn(settings):
    def _exec_command(command, slot_info, events):
        # 如果是需要分发到 remote 的节点
        # from horovod.runner.util.remote import get_remote_command
        # get_remote_command 提供 ssh 封装
        if host_address not in local_addresses:
            command = get_remote_command(local_command,...)
        exit_code = safe_shell_exec.execute(command,...)
    return _exec_command

def launch_gloo(command, exec_command, settings, nics, env, server_ip):
    # exec_command 为执行的命令
    # args_list 是执行的参数，由每个节点所需参数组成的列表
    # 通过如下方法的调用实现多节点运行
    res = threads.execute_function_multithreaded(exec_command, args_list, block_until_all_done=True)
</code></pre>
<pre><code class="language-python"># horovod/runner/util/threads.py

def execute_function_multithreaded(fn, args_list, block_until_all_done=True, max_concurrent_executions=1000):
    worker_queue = queue.Queue()
    result_queue = queue.Queue() # 结果池，用于放置结果，后续忽略

    # 把任务放进任务池
    for i, arg in enumerate(args_list):
        worker_queue.put(arg)

    # 只要任务池里还有任务就取出来执行之
    def fn_execute():
        while True:
            arg = worker_queue.get(block=False)
            exec_index = arg[-1]
            res = fn(*arg[:-1])

    # 启动多线程分发命令，感觉必要性不大
    for _ in range(number_of_threads):
        thread = in_thread(target=fn_execute, daemon=not block_until_all_done)

def in_thread(target, args=(), name=None, daemon=True, silent=False):
    bg = threading.Thread(target=fn, args=args, name=name)
    bg.daemon = daemon
    bg.start()
</code></pre>
<pre><code class="language-python"># horovod/runner/common/util/safe_shell_exec.py

# 使用 multiprocessing.Process 启动进程
# 然后再使用 subprocess 启动进程执行

def execute(command, env=None, stdout=None, stderr=None, index=None, events=None,
            prefix_output_with_timestamp=False):
    ctx = multiprocessing.get_context('spawn')

    exit_event = _create_event(ctx)

    # 当 parent process 被 hard kill 时，这个 Pipe 会被关闭，然后 middleman 就会向子进程发送 SIGTERM，避免出现 orphaned process
    (r, w) = ctx.Pipe(duplex=False)

    middleman = ctx.Process(target=_exec_middleman, args=(command, env, exit_event, ..., (r, w)))
    middleman.start()

    middleman.join()
    return middleman.exitcode

def _exec_middleman(command, env, exit_event, stdout, stderr, rw):
    os.setsid()

    executor_shell = subprocess.Popen(command, shell=True, env=env,
                                      stdout=stdout_w, stderr=stderr_w)

</code></pre>
<pre><code class="language-python"># horovod/runner/mpi_run.py

def mpi_run(settings, nics, env, command, stdout=None, stderr=None):
    mpirun_command = (
        'mpirun {basic_args} '
        '-np {num_proc}{ppn_arg}{hosts_arg} '
        '{binding_args} '
        '{mpi_args} '
        '{mpi_ssh_args} '
        '{tcp_intf_arg} '
        '{nccl_socket_intf_arg} '
        '{output_filename_arg} '
        '{env} {extra_mpi_args} {command}'
    )

    if settings.run_func_mode:
        exit_code = safe_shell_exec.execute(mpirun_command, env=env, stdout=stdout, stderr=stderr)
    else:
        os.execve('/bin/sh', ['/bin/sh', '-c', mpirun_command], env)
</code></pre>
<h3 id="弹性启动"><a class="header" href="#弹性启动">弹性启动</a></h3>
<pre><code class="language-python">def _run_elastic(args):
    settings = elastic_settings.ElasticSettings(discovery=discover_hosts,...)
    return gloo_run_elastic(settings, env, args.run_func if args.run_func else args.command, executable)

from horovod.runner.gloo_run import gloo_run, gloo_run_elastic
</code></pre>
<pre><code class="language-python"></code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="api-3"><a class="header" href="#api-3">API</a></h1>
<h3 id="demo-1"><a class="header" href="#demo-1">Demo</a></h3>
<pre><code class="language-python">    import tensorflow as tf
    import horovod.tensorflow as hvd


    # Initialize Horovod
    hvd.init()

    # Pin GPU to be used to process local rank (one GPU per process)
    config = tf.ConfigProto()
    config.gpu_options.visible_device_list = str(hvd.local_rank())

    # Build model...
    loss = ...
    opt = tf.train.AdagradOptimizer(0.01 * hvd.size())

    # Add Horovod Distributed Optimizer
    opt = hvd.DistributedOptimizer(opt)

    # Add hook to broadcast variables from rank 0 to all other processes during
    # initialization.
    hooks = [hvd.BroadcastGlobalVariablesHook(0)]

    # Make training operation
    train_op = opt.minimize(loss)

    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.
    checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None

    # The MonitoredTrainingSession takes care of session initialization,
    # restoring from a checkpoint, saving to a checkpoint, and closing when done
    # or an error occurs.
    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,
                                           config=config,
                                           hooks=hooks) as mon_sess:
      while not mon_sess.should_stop():
        # Perform synchronous training.
        mon_sess.run(train_op)

</code></pre>
<pre><code class="language-python"># horovod/tensorflow/mpi_ops.py 

def init(*args, **kwargs):
    _basics.init(*args, **kwargs)
    _setup_process_sets(_basics)

</code></pre>
<p>HorovodBasics 提供 mpi lib 的 c 接口封装</p>
<pre><code class="language-python"># horovod/common/basics.py

class HorovodBasics(object):
    def __init__(self, pkg_path, *args):
        # 加载 mpi lib 实现包
        self.MPI_LIB_CTYPES = ctypes.CDLL(full_path, mode=ctypes.RTLD_GLOBAL)

    def init(self, comm, process_sets):
        initialization_ok = self.MPI_LIB_CTYPES.horovod_init(...)
        # initialization_ok = self.MPI_LIB_CTYPES.horovod_init_multi_comm(...)

        _init_process_sets(process_sets)

    def shutdown(self):
    def is_initialized(self):
    def start_timeline(self, file_path, mark_cycles=False):
    def stop_timeline(self):
    def size(self):
    def local_size(self):
    def cross_size(self):
    def rank(self):
    def local_rank(self):
    def cross_rank(self):
    def is_homogeneous(self):
    def mpi_threads_supported(self):
    def mpi_enabled(self):
    def mpi_built(self):
    def gloo_enabled(self):
    def gloo_built(self):
    def nccl_built(self):
    def ddl_built(self):
    def ccl_built(self):
    def cuda_built(self):
    def rocm_built(self):
    def _add_process_set_impl(self, ranks: Sequence[int]) -&gt; Optional[int]:
    def _remove_process_set_impl(self, process_set_id: int) -&gt; Optional[int]:
    def _process_set_rank(self, process_set_id: int) -&gt; int:
    def _process_set_size(self, process_set_id: int) -&gt; int:
    def _get_process_set_ids_and_ranks(self) -&gt; Dict[int, List[int]]:
    def _comm_process_set_id(self, comm: MPI.Comm) -&gt; int:

</code></pre>
<p>HorovodBasics </p>
<pre><code class="language-c">// horovod/common/operations.h

namespace horovod {
namespace common {

extern &quot;C&quot; {

bool horovod_init(const int* ranks, int nranks, const int* process_set_ranks,
                  const int* process_set_sizes, int num_process_sets);

#if HAVE_MPI
// 使用 MPI communicators 初始化
bool horovod_init_multi_comm(MPI_Comm* comm, int ncomms,
                             const int* process_set_ranks_via_ranks,
                             const int* process_set_sizes_via_ranks,
                             int num_process_sets_via_ranks);
#endif

void horovod_shutdown();

int horovod_rank();
int horovod_local_rank();

int horovod_size();
int horovod_local_size();

// bool horovod_xxx_enabled();
// bool horovod_xxx_built();

int horovod_reduce_op_average();
int horovod_reduce_op_sum();
int horovod_reduce_op_adasum();

int horovod_add_process_set(const int *ranks, int nranks);
int horovod_remove_process_set(int process_set_id);
int horovod_process_set_rank(int process_set_id);
int horovod_process_set_size(int process_set_id);
int horovod_process_set_included(int process_set_id);
int horovod_number_of_process_sets();
void horovod_process_set_ids(int* ids_prealloc);
int horovod_process_set_ranks(int id, int* ranks_prealloc);

}

Status EnqueueTensorAllreduce(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              std::shared_ptr&lt;Tensor&gt; output,
                              ReadyEventList ready_event_list,
                              std::string name, int device,
                              StatusCallback callback,
                              ReduceOp reduce_op = ReduceOp::SUM,
                              double prescale_factor = 1.0,
                              double postscale_factor = 1.0,
                              int32_t process_set_id = 0);

Status EnqueueTensorAllreduces(std::vector&lt;std::shared_ptr&lt;OpContext&gt;&gt;&amp; contexts,
                               std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; tensors,
                               std::vector&lt;std::shared_ptr&lt;Tensor&gt;&gt;&amp; outputs,
                               std::vector&lt;ReadyEventList&gt;&amp; ready_event_lists,
                               std::vector&lt;std::string&gt;&amp; names,
                               int device,
                               std::vector&lt;StatusCallback&gt;&amp; callbacks,
                               ReduceOp reduce_op = ReduceOp::SUM,
                               double prescale_factor = 1.0,
                               double postscale_factor = 1.0,
                               int32_t process_set_id = 0);

Status EnqueueTensorAllgather(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              ReadyEventList ready_event_list,
                              const std::string&amp; name, int device,
                              StatusCallback callback,
                              int32_t process_set_id = 0);

Status EnqueueTensorBroadcast(std::shared_ptr&lt;OpContext&gt; context,
                              std::shared_ptr&lt;Tensor&gt; tensor,
                              std::shared_ptr&lt;Tensor&gt; output, int root_rank,
                              ReadyEventList ready_event_list,
                              const std::string&amp; name, int device,
                              StatusCallback callback,
                              int32_t process_set_id = 0);

Status EnqueueTensorAlltoall(std::shared_ptr&lt;OpContext&gt; context,
                             std::shared_ptr&lt;Tensor&gt; tensor,
                             std::shared_ptr&lt;Tensor&gt; splits,
                             ReadyEventList ready_event_list,
                             const std::string&amp; name, int device,
                             StatusCallback callback,
                             int32_t process_set_id = 0);

Status EnqueueTensorReducescatter(std::shared_ptr&lt;OpContext&gt; context,
                                  std::shared_ptr&lt;Tensor&gt; tensor,
                                  ReadyEventList ready_event_list,
                                  const std::string&amp; name, int device,
                                  StatusCallback callback,
                                  ReduceOp reduce_op = ReduceOp::SUM,
                                  int32_t process_set_id = 0);

Status EnqueueJoin(std::shared_ptr&lt;OpContext&gt; context,
                   std::shared_ptr&lt;Tensor&gt; output_last_joined_rank,
                   ReadyEventList ready_event_list,
                   const std::string&amp; name, int device,
                   StatusCallback callback,
                   int32_t process_set_id = 0);

Status EnqueueBarrier(StatusCallback callback,
                   int32_t process_set_id = 0);

} // namespace common
} // namespace horovod

#endif // HOROVOD_OPERATIONS_H
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="common"><a class="header" href="#common">common</a></h1>
<p>Horovod 对象</p>
<pre><code class="language-cpp"># horovod/common/common.h

enum Framework { TENSORFLOW, PYTORCH, MXNET, XLA };
enum StatusType { OK, UNKNOWN_ERROR, PRECONDITION_ERROR, ABORTED, INVALID_ARGUMENT, IN_PROGRESS };
enum DeviceType { CPU, GPU };

// for gpu
struct Event {};

class Status {
public:
  Status();
  static Status OK();
  static Status UnknownError(const std::string&amp; message);
  static Status PreconditionError(const std::string&amp; message);
  static Status Aborted(const std::string&amp; message);
  static Status InvalidArgument(const std::string&amp; message);
  static Status InProgress();
  bool ok() const;
  bool in_progress() const;
  StatusType type() const;
  const std::string&amp; reason() const;
  Event event;

private:
  StatusType type_ = StatusType::OK;
  std::string reason_;
  Status(StatusType type, std::string reason);
};

class TensorShape {
public:
  TensorShape() : shape_() {}
  TensorShape(std::vector&lt;int64_t&gt; vec) : shape_(vec) {}
  void AddDim(int64_t dim);
  void AppendShape(TensorShape&amp; other);

  std::string DebugString() const;
  int dims() const;
  int64_t dim_size(int idx) const;
  int64_t num_elements() const;
  const std::vector&lt;int64_t&gt;&amp; to_vector() const;

  inline bool operator==(const TensorShape&amp; rhs) const {
    return shape_ == rhs.shape_;
  }

  inline bool operator!=(const TensorShape&amp; rhs) const {
    return shape_ != rhs.shape_;
  }

private:
  std::vector&lt;int64_t&gt; shape_;
};

class ReadyEvent {};
class ReadyEventList {};

class PersistentBuffer {
public:
  virtual const void* AccessData(std::shared_ptr&lt;OpContext&gt; context) const = 0;
  virtual ~PersistentBuffer() = default;
};

class Tensor {
public:
  virtual const DataType dtype() const = 0;
  virtual const TensorShape shape() const = 0;
  virtual const void* data() const = 0;
  virtual int64_t size() const = 0;
  virtual ~Tensor() = default;
};

class OpContext {
public:
  // These allocators are fully synchronous, unlike TensorFlow counterparts.
  virtual Status
  AllocatePersistent(int64_t size,
                     std::shared_ptr&lt;PersistentBuffer&gt;* tensor) = 0;
  virtual Status AllocateOutput(TensorShape shape,
                                std::shared_ptr&lt;Tensor&gt;* tensor,
                                std::shared_ptr&lt;ReadyEvent&gt;* event = nullptr) = 0;
  virtual Status AllocateOutput(int output_index, TensorShape shape,
                                std::shared_ptr&lt;Tensor&gt;* tensor,
                                std::shared_ptr&lt;ReadyEvent&gt;* event = nullptr) {
    if (output_index == 0) {
      return AllocateOutput(std::move(shape), tensor);
    } else {
      throw std::logic_error(&quot;output_index != 0 not supported&quot;);
    }
  }
  virtual Status AllocateZeros(int64_t num_elements, DataType dtype,
                                std::shared_ptr&lt;Tensor&gt;* tensor) = 0;
  virtual Framework framework() const = 0;
  virtual ~OpContext() = default;
};

// A callback to call after the communication completes. Since the
// allreduce and allgather ops are asynchronous, this callback is what resumes
// computation after the reduction is completed.
using StatusCallback = std::function&lt;void(const Status&amp;)&gt;;

// Table storing Tensors to be reduced, keyed by unique name.
// This table contains everything necessary to do the distributed operation.
struct TensorTableEntry {
  std::string tensor_name;
  std::shared_ptr&lt;OpContext&gt; context;
  std::shared_ptr&lt;Tensor&gt; tensor;
  std::shared_ptr&lt;Tensor&gt; output;
  // Identifier for the subset of Horovod processes partaking in this operation.
  int32_t process_set_id = 0;
  // Root rank for broadcast operation (relative to process set).
  int root_rank = 0;
  // List of events indicating that data is ready.
  ReadyEventList ready_event_list;
  // GPU to do reduction on, or CPU_DEVICE_ID in case of CPU.
  int device = CPU_DEVICE_ID;
  // A callback to call with the status.
  StatusCallback callback;
  // If we build with NVTX support: A range marking the start
  // and end of the distributed op for this tensor (may be
  // shared by multiple tensors).
  SharedNvtxOpRange nvtx_op_range;

  // Alltoall splits (if tensor is for an Alltoall operation)
  // Note: splits are stored in TensorTableEntry to avoid N^2
  // storage complexity of collecting all worker split arrays
  // on coordinator rank.
  std::vector&lt;int32_t&gt; splits;
  std::shared_ptr&lt;Tensor&gt; received_splits;

  void FinishWithCallback(const Status&amp; status);
};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="torch-api"><a class="header" href="#torch-api">torch api</a></h1>
<pre><code class="language-python"># horovod/torch/mpi_ops.py

_basics = _HorovodBasics(__file__, 'mpi_lib_v2')

# 重要
# handle 会被放在 map 中，避免被 gc
# 在 synchronize 之后被释放
_handle_map = {}

# inplace allreduce
# allreduce_ -&gt; allreduce_async_ + synchronize -&gt; _allreduce_async -&gt; mpi_lib.horovod_torch_allreduce_async_

# allreduce
# allreduce -&gt; HorovodAllreduce.apply -&gt; allreduce_async + synchronize -&gt; _allreduce_async -&gt; mpi_lib.horovod_torch_allreduce_async_

def _allreduce_function_factory(tensor):
    return 'horovod_torch_allreduce_async_' + tensor.type().replace('.', '_')


def _allreduce_async(tensor, output, name, op, prescale_factor, postscale_factor, process_set: ProcessSet):
    function = _check_function(_allreduce_function_factory, tensor)
    try:
        handle = getattr(mpi_lib, function)(tensor, output, divisor,
                                            name.encode() if name is not None else _NULL, op,
                                            prescale_factor, postscale_factor, process_set.process_set_id)
    except RuntimeError as e:
        raise HorovodInternalError(e)
    return handle


def allreduce_async(tensor, average=None, name=None, op=None,
                    prescale_factor=1.0, postscale_factor=1.0,
                    process_set=global_process_set):
    output = tensor.new(tensor.shape)
    return _allreduce_async(tensor, output, name, op, prescale_factor, postscale_factor, process_set)


class HorovodAllreduce(torch.autograd.Function):
    @staticmethod
    def forward(ctx, tensor, average, name, op, prescale_factor, postscale_factor, process_set):
        ctx.average = average
        ctx.op = op
        ctx.prescale_factor = prescale_factor
        ctx.postscale_factor = postscale_factor
        ctx.process_set = process_set
        handle = allreduce_async(tensor, average, name, op, prescale_factor, postscale_factor, process_set)
        return synchronize(handle)

    @staticmethod
    def backward(ctx, grad_output):
        return allreduce(grad_output, average=ctx.average, op=ctx.op,
                         prescale_factor=ctx.prescale_factor,
                         postscale_factor=ctx.postscale_factor,
                         process_set=ctx.process_set), None, None, None, None, None, None


def allreduce(tensor, average=None, name=None, compression=Compression.none, op=None,
              prescale_factor=1.0, postscale_factor=1.0, process_set=global_process_set):
    &quot;&quot;&quot;
    This acts as a thin wrapper around an autograd function.  If your input
    tensor requires gradients, then callings this function will allow gradients
    to be computed and backpropagated.
    &quot;&quot;&quot;
    tensor_compressed, ctx = compression.compress(tensor)
    summed_tensor_compressed = HorovodAllreduce.apply(tensor_compressed, average, name, op,
                                                      prescale_factor, postscale_factor,
                                                      process_set)
    return compression.decompress(summed_tensor_compressed, ctx)


def allreduce_async_(tensor, average=None, name=None, op=None,
                     prescale_factor=1.0, postscale_factor=1.0,
                     process_set=global_process_set):
    op = handle_average_backwards_compatibility(op, average)
    return _allreduce_async(tensor, tensor, name, op, prescale_factor, postscale_factor, process_set)


def allreduce_(tensor, average=None, name=None, op=None,
               prescale_factor=1.0, postscale_factor=1.0,
               process_set=global_process_set):
    handle = allreduce_async_(tensor, average, name, op, prescale_factor, postscale_factor, process_set)
    return synchronize(handle)

</code></pre>
<pre><code class="language-cpp">// horovod/torch/mpi_ops_v2.cc

PYBIND11_MODULE(mpi_lib_v2, m) {
  m.def(&quot;horovod_torch_allreduce_async_torch_IntTensor&quot;, &amp;DoAllreduce);
  ...
}

int DoAllreduce(::torch::Tensor tensor, ::torch::Tensor output, int divisor,
                const std::string&amp; name, int reduce_op_int,
                double prescale_factor, double postscale_factor,
                int process_set_id) {
  auto handle = handle_manager.AllocateHandle();
  common::ReadyEventList ready_event_list;
  auto hvd_tensor = std::make_shared&lt;TorchTensor&gt;(tensor);
  auto hvd_context = std::make_shared&lt;TorchOpContext&gt;(device, output);
  auto hvd_output = std::make_shared&lt;TorchTensor&gt;(output);

  ReduceOp reduce_op = static_cast&lt;ReduceOp&gt;(reduce_op_int);
  auto enqueue_result = EnqueueTensorAllreduce(hvd_context, hvd_tensor, hvd_output, ready_event_list, ...);

  return handle;
}
</code></pre>
<pre><code class="language-cpp">// horovod/torch/adapter_v2.h

class TorchPersistentBuffer : public PersistentBuffer {
  AccessData(std::shared_ptr&lt;OpContext&gt; context) const override;
  ::torch::Tensor tensor_;
};

class TorchTensor : public Tensor {
  ::torch::Tensor tensor_;
};

class TorchOpContext : public OpContext {
  std::vector&lt;::torch::Tensor&gt; outputs_;
};
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="python"><a class="header" href="#python">python</a></h1>
<div style="break-before: page; page-break-before: always;"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="concurrent-execution"><a class="header" href="#concurrent-execution">Concurrent Execution</a></h1>
<p><a href="https://docs.python.org/3/library/concurrency.html">doc</a></p>
<h3 id="1-subprocess"><a class="header" href="#1-subprocess">1. subprocess</a></h3>
<ul>
<li>启动子进程，自定义 excutable</li>
</ul>
<pre><code class="language-python">import subprocess

proc = subprocess.Popen([&quot;/usr/bin/git&quot;, &quot;commit&quot;, &quot;-m&quot;, &quot;Fixes a bug.&quot;])
proc.poll()
proc.wait()
proc.communicate()
proc.terminate()
proc.kill()
</code></pre>
<p><a href="https://docs.python.org/3/library/subprocess.html">doc</a></p>
<h3 id="2-threading"><a class="header" href="#2-threading">2. threading</a></h3>
<ul>
<li>启动线程</li>
<li>存在 GIL 问题，无法完全利用 CPU</li>
</ul>
<pre><code class="language-python">import threading

def serve():
    pass

thread = threading.Thread(target=serve, daemon=None)
thread.start()
thread.join()
</code></pre>
<p><a href="https://docs.python.org/3/library/threading.html">doc</a></p>
<h4 id="pros"><a class="header" href="#pros">Pros</a></h4>
<ul>
<li>Lightweight - low memory footprint</li>
<li>Shared memory - makes access to state from another context easier</li>
<li>Allows you to easily make responsive UIs</li>
<li>cPython C extension modules that properly release the GIL will run in parallel</li>
<li>Great option for I/O-bound applications</li>
</ul>
<h4 id="cons"><a class="header" href="#cons">Cons</a></h4>
<ul>
<li>cPython - subject to the GIL</li>
<li>Not interruptible/killable</li>
<li>If not following a command queue/message pump model (using the Queue module), then manual use of synchronization primitives become a necessity (decisions are needed for the granularity of locking)</li>
<li>Code is usually harder to understand and to get right - the potential for race conditions increases dramatically</li>
</ul>
<h3 id="3-multiprocessing"><a class="header" href="#3-multiprocessing">3. multiprocessing</a></h3>
<ul>
<li>启动子进程，使用内置函数</li>
<li>使用进程，充分利用 CPU 资源</li>
</ul>
<pre><code class="language-python">from multiprocessing

proc = multiprocessing.Process(target=serve, daemon=None)
proc.start()
proc.join()
proc.close()

</code></pre>
<p><a href="https://docs.python.org/3/library/multiprocessing.html">doc</a></p>
<h4 id="pros-1"><a class="header" href="#pros-1">Pros</a></h4>
<ul>
<li>Separate memory space</li>
<li>Code is usually straightforward</li>
<li>Takes advantage of multiple CPUs &amp; cores</li>
<li>Avoids GIL limitations for cPython</li>
<li>Eliminates most needs for synchronization primitives unless if you use shared memory (instead, it's more of a communication model for IPC)</li>
<li>Child processes are interruptible/killable</li>
<li>Python multiprocessing module includes useful abstractions with an interface much like threading.Thread</li>
<li>A must with cPython for CPU-bound processing</li>
</ul>
<h4 id="cons-1"><a class="header" href="#cons-1">Cons</a></h4>
<ul>
<li>IPC a little more complicated with more overhead (communication model vs. shared memory/objects)</li>
<li>Larger memory footprint</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
