<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>autograd - Aller au boulot</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="../index.html"><strong aria-hidden="true">1.</strong> welcome</a></li><li class="chapter-item "><a href="../survey/survey.html"><strong aria-hidden="true">2.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../survey/pollux.html"><strong aria-hidden="true">2.1.</strong> pollux</a></li><li class="chapter-item "><a href="../survey/adasum.html"><strong aria-hidden="true">2.2.</strong> adasum</a></li><li class="chapter-item "><a href="../survey/adaptation_learning.html"><strong aria-hidden="true">2.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="../survey/gradient_descent.html"><strong aria-hidden="true">2.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="../survey/auto_parallel.html"><strong aria-hidden="true">2.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="../survey/scheduling.html"><strong aria-hidden="true">2.6.</strong> scheduling</a></li><li class="chapter-item "><a href="../survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">2.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../survey/gradient_compression/dgc.html"><strong aria-hidden="true">2.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="../survey/gradient_compression/csc.html"><strong aria-hidden="true">2.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="../survey/adaptive_training.html"><strong aria-hidden="true">2.8.</strong> adaptive training</a></li></ol></li><li class="chapter-item expanded "><a href="../pytorch/overview.html"><strong aria-hidden="true">3.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../pytorch/tensor.html"><strong aria-hidden="true">3.1.</strong> tensor</a></li><li class="chapter-item expanded "><a href="../pytorch/autograd.html" class="active"><strong aria-hidden="true">3.2.</strong> autograd</a></li><li class="chapter-item "><a href="../pytorch/profiler.html"><strong aria-hidden="true">3.3.</strong> profiler</a></li><li class="chapter-item "><a href="../pytorch/hook.html"><strong aria-hidden="true">3.4.</strong> hook</a></li><li class="chapter-item "><a href="../pytorch/elastic.html"><strong aria-hidden="true">3.5.</strong> elastic</a></li><li class="chapter-item "><a href="../pytorch/patch.html"><strong aria-hidden="true">3.6.</strong> patch</a></li></ol></li><li class="chapter-item "><a href="../paddle/paddle.html"><strong aria-hidden="true">4.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../paddle/ps/ps-code-overview.html"><strong aria-hidden="true">4.1.</strong> ps</a></li><li class="chapter-item "><a href="../paddle/framework.html"><strong aria-hidden="true">4.2.</strong> framework</a></li><li class="chapter-item "><a href="../paddle/cinn.html"><strong aria-hidden="true">4.3.</strong> cinn</a></li></ol></li><li class="chapter-item "><a href="../horovod/horovod.html"><strong aria-hidden="true">5.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../horovod/run.html"><strong aria-hidden="true">5.1.</strong> run</a></li><li class="chapter-item "><a href="../horovod/workflow.html"><strong aria-hidden="true">5.2.</strong> workflow</a></li><li class="chapter-item "><a href="../horovod/object.html"><strong aria-hidden="true">5.3.</strong> object</a></li><li class="chapter-item "><a href="../horovod/develop.html"><strong aria-hidden="true">5.4.</strong> develop</a></li><li class="chapter-item "><a href="../horovod/pytorch.html"><strong aria-hidden="true">5.5.</strong> pytorch</a></li><li class="chapter-item "><a href="../horovod/tensorflow.html"><strong aria-hidden="true">5.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="../horovod/elastic.html"><strong aria-hidden="true">5.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="../ray/ray.html"><strong aria-hidden="true">6.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../ray/overview.html"><strong aria-hidden="true">6.1.</strong> overview</a></li><li class="chapter-item "><a href="../ray/gcs.html"><strong aria-hidden="true">6.2.</strong> gcs</a></li><li class="chapter-item "><a href="../ray/raylet.html"><strong aria-hidden="true">6.3.</strong> raylet</a></li><li class="chapter-item "><a href="../ray/api.html"><strong aria-hidden="true">6.4.</strong> api</a></li></ol></li><li class="chapter-item "><a href="../python/python.html"><strong aria-hidden="true">7.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../python/concurrent.html"><strong aria-hidden="true">7.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="../python/multiprocessing.html"><strong aria-hidden="true">7.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="../python/decorator.html"><strong aria-hidden="true">7.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="../kubernetes/kubernetes.html"><strong aria-hidden="true">8.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../kubernetes/scheduler.html"><strong aria-hidden="true">8.1.</strong> scheduler</a></li><li class="chapter-item "><a href="../kubernetes/operator.html"><strong aria-hidden="true">8.2.</strong> operator</a></li><li class="chapter-item "><a href="../kubernetes/device_plugin.html"><strong aria-hidden="true">8.3.</strong> device plugin</a></li><li class="chapter-item "><a href="../kubernetes/docker.html"><strong aria-hidden="true">8.4.</strong> docker</a></li></ol></li><li class="chapter-item "><a href="../tvm/tvm.html"><strong aria-hidden="true">9.</strong> tvm</a></li><li class="chapter-item "><a href="../gloo/gloo.html"><strong aria-hidden="true">10.</strong> gloo</a></li><li class="chapter-item "><a href="../nccl/nccl.html"><strong aria-hidden="true">11.</strong> nccl</a></li><li class="chapter-item "><a href="../mpi/mpi.html"><strong aria-hidden="true">12.</strong> mpi</a></li><li class="chapter-item "><a href="../nlp/nlp.html"><strong aria-hidden="true">13.</strong> nlp</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="../nlp/transformer.html"><strong aria-hidden="true">13.1.</strong> transformer</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="autograd"><a class="header" href="#autograd">Autograd</a></h1>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.h

struct TORCH_API AutogradMeta : public c10::AutogradMetaInterface {
  std::string name_;

  Variable grad_;
  std::shared_ptr&lt;Node&gt; grad_fn_;
  std::weak_ptr&lt;Node&gt; grad_accumulator_;

  std::shared_ptr&lt;ForwardGrad&gt; fw_grad_;

  std::vector&lt;std::shared_ptr&lt;FunctionPreHook&gt;&gt; hooks_;
  std::shared_ptr&lt;hooks_list&gt; cpp_hooks_list_;
}
</code></pre>
<p>AutogradMeta 中包含 autograd 所需的元素</p>
<ul>
<li>grad_: Tensor 对应的 grad</li>
<li>grad_fn_: 反向 op</li>
<li>grad_accumulator_: 反向梯度累加器，Node 类型</li>
<li>cpp_hooks_list_, hooks_: 反向调用时的 hook</li>
</ul>
<h2 id="node"><a class="header" href="#node">Node</a></h2>
<p>Node</p>
<ul>
<li>The most important method on <code>Node</code> is the call operator, which takes in a list of variables and produces a list of variables. </li>
<li>The precise size of these lists can be determined with <code>num_inputs()</code> and <code>num_outputs()</code>.</li>
<li><code>Node</code>s are stitched together via their <code>next_edge</code> interface, which let you manipulate the set of outgoing edges of a <code>Node</code>. </li>
<li>You can add an edge with <code>add_next_edge()</code>, retrieve an edge with <code>next_edge(index)</code> and iterate over them via the <code>next_edges()</code> method. </li>
</ul>
<pre><code class="language-cpp">// torch/csrc/autograd/function.h

using edge_list = std::vector&lt;Edge&gt;;

struct TORCH_API Node : std::enable_shared_from_this&lt;Node&gt; {
 public:
  explicit Node(
      uint64_t sequence_nr,
      edge_list&amp;&amp; next_edges = edge_list())
      : sequence_nr_(sequence_nr),
      next_edges_(std::move(next_edges)) {

    for (const Edge&amp; edge: next_edges_) {
      update_topological_nr(edge);
    }
  }

  variable_list operator()(variable_list&amp;&amp; inputs) {
    ...
    return apply(std::move(inputs));
  }


  void update_topological_nr(const Edge&amp; edge) {
    Node* node = edge.function.get();
    if (node) {
      auto topo_nr = node-&gt;topological_nr();
      if (topological_nr_ &lt;= topo_nr) {
        topological_nr_ = topo_nr + 1;
      }
    }
  }

  void set_next_edge(size_t index, Edge edge) {
    update_topological_nr(edge);
    next_edges_[index] = std::move(edge);
  }

  void add_next_edge(Edge edge) {
    update_topological_nr(edge);
    next_edges_.push_back(std::move(edge));
  }

  void set_next_edges(edge_list&amp;&amp; next_edges) {
    next_edges_ = std::move(next_edges);
    for (const auto&amp; next_edge : next_edges_) {
      update_topological_nr(next_edge);
    }
  }

  const Edge&amp; next_edge(size_t index) const noexcept {
    return next_edges_[index];
  }

  const edge_list&amp; next_edges() const noexcept {
    return next_edges_;
  }

  edge_list&amp; next_edges() noexcept {
    return next_edges_;
  }

 protected:
  virtual variable_list apply(variable_list&amp;&amp; inputs) = 0;

  const uint64_t sequence_nr_;
  uint64_t topological_nr_ = 0;
  uint64_t thread_id_ = 0;
  edge_list next_edges_;
  std::vector&lt;std::unique_ptr&lt;FunctionPreHook&gt;&gt; pre_hooks_;
  std::vector&lt;std::unique_ptr&lt;FunctionPostHook&gt;&gt; post_hooks_;
  at::SmallVector&lt;InputMetadata, 2&gt; input_metadata_;
};
</code></pre>
<p>可以看到</p>
<ul>
<li>Node 的创建由 Edge 来完成，Node 中保存了连接情况和需要执行的方法。</li>
<li>Node 本身是 callable object, 通过虚函数 apply 被子类重载实现。</li>
<li>set_next_edge 方法可以添加 Edge</li>
</ul>
<p>Edge</p>
<pre><code class="language-cpp">// torch/csrc/autograd/edge.h

struct Edge {
  Edge() noexcept : function(nullptr), input_nr(0) {}

  Edge(std::shared_ptr&lt;Node&gt; function_, uint32_t input_nr_) noexcept
      : function(std::move(function_)), input_nr(input_nr_) {}

  // Required for use in associative containers.
  bool operator==(const Edge&amp; other) const noexcept {
    return this-&gt;function == other.function &amp;&amp; this-&gt;input_nr == other.input_nr;
  }

  bool operator!=(const Edge&amp; other) const noexcept {
    return !(*this == other);
  }

  /// The function this `Edge` points to.
  std::shared_ptr&lt;Node&gt; function;

  /// The identifier of a particular input to the function.
  uint32_t input_nr;
};
</code></pre>
<h2 id="workflow"><a class="header" href="#workflow">Workflow</a></h2>
<pre><code class="language-python">import torch
a = torch.tensor(1.0, requires_grad=True)
b = torch.tensor(2.0, requires_grad=True)
c = torch.add(a, b)
d = torch.mul(a, c)
d.backward()
print(f&quot;a grad:{a.grad} grad_fn:{a.grad_fn}&quot;)
print(f&quot;b grad:{b.grad} grad_fn:{b.grad_fn}&quot;)
print(f&quot;c grad:{c.grad} grad_fn:{c.grad_fn}&quot;)
print(f&quot;d grad:{d.grad} grad_fn:{d.grad_fn}&quot;)
'''
a grad:4.0 grad_fn:None
b grad:1.0 grad_fn:None
c grad:None grad_fn:&lt;AddBackward0 object at 0x7f6862dc76d0&gt;
d grad:None grad_fn:&lt;MulBackward0 object at 0x7f6862dc76d0&gt;
'''
</code></pre>
<p>以上代码的网络构建如图所示</p>
<p><img src="4.png" alt="1" /></p>
<p>下面解析详细过程.</p>
<p>在之前的版本中，<code>torch.tensor()</code> 的 bind 来自于自动代码生成，相关生成逻辑参考以下两个部分，</p>
<pre><code class="language-cpp">// tools/autograd/templates/python_torch_functions.cpp

static PyMethodDef torch_functions_shard[] = {
  {py_method_defs}
};
</code></pre>
<pre><code class="language-python"># tools/autograd/gen_python_functions.py

def create_python_bindings(...
def create_python_bindings_sharded(...
</code></pre>
<p>最新的 pytorch 使用 python object 暴露 python tensor. 对应类型 THPVariable_tensor.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/python_torch_functions_manual.cpp

// implemented on python object to allow torch.tensor to be constructed with
// arbitrarily nested python objects - list, tuple, np array, scalar, etc.
static PyObject* THPVariable_tensor( PyObject* self, PyObject* args, PyObject* kwargs) {
  jit::tracer::warn(&quot;torch.tensor&quot;, jit::tracer::WARN_CONSTRUCTOR);
  return THPVariable_Wrap(torch::utils::tensor_ctor(
      torch::tensors::get_default_dispatch_key(),
      torch::tensors::get_default_scalar_type(),
      r));
}
</code></pre>
<ul>
<li>torch::utils::tensor_ctor() 返回 cpp tensor</li>
<li>torch::tensors::get_default_dispatch_key() 获取默认 dispatch key</li>
<li>torch::tensors::get_default_scalar_type() 获取默认数据类型</li>
<li>THPVariable_Wrap 把 tensor 封装成 python 可使用的 THPVariable</li>
</ul>
<p><img src="3.png" alt="1" /></p>
<h3 id="tensor_new"><a class="header" href="#tensor_new">tensor_new</a></h3>
<pre><code class="language-cpp">// torch/csrc/utils/tensor_new.cpp

Tensor tensor_ctor(
    c10::DispatchKey dispatch_key,
    at::ScalarType scalar_type,
    PythonArgs&amp; r) {
  if (r.idx == 0) {
    PyObject* data = r.pyobject(0);
    bool type_inference = r.isNone(1);
    bool pin_memory = r.toBool(3);
    bool args_requires_grad = r.toBool(4);
    auto new_tensor = internal_new_from_data(
        typeIdWithDefault(r, 2, dispatch_key),
        r.scalartypeWithDefault(1, scalar_type),
        r.deviceOptional(2),
        data,
        /*copy_variables=*/true,
        /*copy_numpy=*/true,
        /*type_inference=*/type_inference,
        pin_memory);
    auto names = r.toDimnameListOptional(5);
    if (names) {
      at::namedinference::propagate_names(
          new_tensor, *names, /*validate_names=*/true);
    }
    new_tensor.detach_(); // ensure new_tensor a leaf node
    new_tensor.set_requires_grad(args_requires_grad);
    return new_tensor;
  }
}
</code></pre>
<ul>
<li>解析参数</li>
<li>调用 internal_new_from_data 创建 cpp tensor，初始化 storage_</li>
<li>new_tensor.detach_() 确保是叶子结点，初始化 autograd_meta_</li>
</ul>
<h3 id="internal_new_from_data"><a class="header" href="#internal_new_from_data">internal_new_from_data</a></h3>
<pre><code class="language-cpp">// torch/csrc/utils/tensor_new.cpp

Tensor internal_new_from_data(
    c10::TensorOptions options,
    at::ScalarType scalar_type,
    c10::optional&lt;Device&gt; device_opt,
    PyObject* data,
    bool copy_variables,
    bool copy_numpy,
    bool type_inference,
    bool pin_memory = false) {
  if (THPVariable_Check(data)) {
    auto var = THPVariable_Unpack(data);
    return var.to(...);
  }

  if (PyObject_HasAttrString(data, &quot;__cuda_array_interface__&quot;)) {
    auto tensor = tensor_from_cuda_array_interface(data);
    return tensor.to(...);
  }

  if (is_numpy_available() &amp;&amp; PyArray_Check(data)) {
     auto tensor = tensor_from_numpy(data, /*warn_if_not_writeable=*/!copy_numpy);
     return tensor.to(...);
  }

  auto device = device_opt.has_value() ? *device_opt : options.device();
  auto sizes = compute_sizes(data, scalar_type);
  ScalarType inferred_scalar_type = type_inference ? infer_scalar_type(data) : scalar_type;

  Tensor tensor;
  {
    {
      if (isStorage(data)) {
        Storage storage = createStorageGetType(data, storage_scalar_type, is_typed_storage);
        tensor = at::empty( sizes,
            at::initialTensorOptions().dtype( is_typed_storage ? storage_scalar_type : inferred_scalar_type)
                .pinned_memory(pin_memory)
                .device(storage.device()));
        tensor.set_(storage);

      } else {
        TensorOptions opts = at::initialTensorOptions().dtype(inferred_scalar_type);
        tensor = at::empty(sizes, opts.pinned_memory(pin_memory));
        recursive_store(
              (char*)tensor.data_ptr(),
              tensor.sizes(),
              tensor.strides(),
              0,
              inferred_scalar_type,
              tensor.dtype().itemsize(),
              data);
      }
    }
    maybe_initialize_cuda(device);
    tensor = tensor.to(device, inferred_scalar_type, /*non_blocking=*/false, /*copy=*/false);
  }

  return at::lift_fresh(tensor);
}
</code></pre>
<ul>
<li>at::empty() 创建 tensor</li>
<li>recursive_store() 初始化 tensor 数据</li>
</ul>
<p>其中 <code>detach_</code> 调用会调用 materialize_autograd_meta 初始化 autograd_meta_.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.cpp

AutogradMeta* materialize_autograd_meta(const at::TensorBase&amp; self) {
  auto p = self.unsafeGetTensorImpl();
  if (!p-&gt;autograd_meta()) {
    p-&gt;set_autograd_meta(std::make_unique&lt;AutogradMeta&gt;());
  }
  return get_autograd_meta(self);
}
</code></pre>
<h2 id="torchadd"><a class="header" href="#torchadd">torch.add</a></h2>
<blockquote>
<p>torch/csrc/autograd/generated/ 目录需要 build 生成</p>
</blockquote>
<pre><code class="language-cpp">// torch/csrc/autograd/generated/VariableType_2.cpp

at::Tensor add_Tensor(c10::DispatchKeySet ks, const at::Tensor &amp; self, const at::Tensor &amp; other, const at::Scalar &amp; alpha) {
  auto&amp; self_ = unpack(self, &quot;self&quot;, 0);
  auto&amp; other_ = unpack(other, &quot;other&quot;, 1);
  auto _any_requires_grad = compute_requires_grad( self, other );
  
  (void)_any_requires_grad;
  auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  (void)_any_has_forward_grad_result;
  std::shared_ptr&lt;AddBackward0&gt; grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr&lt;AddBackward0&gt;(new AddBackward0(), deleteNode);
    grad_fn-&gt;set_next_edges(collect_next_edges( self, other ));
    grad_fn-&gt;other_scalar_type = other.scalar_type();
    grad_fn-&gt;alpha = alpha;
    grad_fn-&gt;self_scalar_type = self.scalar_type();
  }

  auto _tmp = ([&amp;]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::add(ks &amp; c10::after_autograd_keyset, self_, other_, alpha);
  })();
  auto result = std::move(_tmp);

  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }
  return result;
}
</code></pre>
<ul>
<li>构建反向节点 AddBackward0</li>
<li>计算 at::redispatch::add，结果保存至 result</li>
<li>关联 AddBackward0 和 result</li>
</ul>
<p>AddBackward0</p>
<pre><code class="language-cpp">// torch/csrc/autograd/generated/Functions.h

struct TORCH_API AddBackward0 : public TraceableFunction {
  using TraceableFunction::TraceableFunction;
  variable_list apply(variable_list&amp;&amp; grads) override;
  std::string name() const override { return &quot;AddBackward0&quot;; }
  void release_variables() override { }

  at::ScalarType other_scalar_type;
  at::Scalar alpha;
  at::ScalarType self_scalar_type;
};
</code></pre>
<p>TraceableFunction </p>
<pre><code class="language-cpp">// torch/csrc/autograd/function.h

struct TraceableFunction : public Node {
  using Node::Node;
  bool is_traceable() final {
    return true;
  }
};

collect_next_edges 根据两个输入找到节点的 Edges

```cpp
// torch/csrc/autograd/function.h

/// Return the next edges of all the given variables, or tuples of variables.
template &lt;typename... Variables&gt;
edge_list collect_next_edges(Variables&amp;&amp;... variables) {
  detail::MakeNextFunctionList make;
  make.apply(std::forward&lt;Variables&gt;(variables)...);
  return std::move(make.next_edges);
}

struct MakeNextFunctionList : IterArgs&lt;MakeNextFunctionList&gt; {
  edge_list next_edges;
  using IterArgs&lt;MakeNextFunctionList&gt;::operator();
  void operator()(const Variable&amp; variable) {
    if (variable.defined()) {
      next_edges.push_back(impl::gradient_edge(variable));
    } else {
      next_edges.emplace_back();
    }
  }
  void operator()(const Variable* variable) { ... }
  void operator()(const c10::optional&lt;Variable&gt;&amp; variable) { ... }
};
</code></pre>
<p>gradient_edge 会返回一组 Edges</p>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.cpp

Edge gradient_edge(const Variable&amp; self) {
  if (const auto&amp; gradient = self.grad_fn()) {
    return Edge(gradient, self.output_nr());
  } else {
    return Edge(grad_accumulator(self), 0);
  }
}
</code></pre>
<p>如果 self 是内部创建的（非叶子结点），即通过运算生成的，则返回 self 的 grad_fn 数据成员，否则（即用户创建的叶子结点）返回 AccumulateGrad 实例。</p>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.cpp

std::shared_ptr&lt;Node&gt; grad_accumulator(const Variable&amp; self) {
  auto autograd_meta = get_autograd_meta(self);
  c10::raw::intrusive_ptr::incref(self.unsafeGetTensorImpl());
  auto intrusive_from_this =
      c10::intrusive_ptr&lt;at::TensorImpl&gt;::reclaim(self.unsafeGetTensorImpl());
  result = std::make_shared&lt;AccumulateGrad&gt;(
      Variable(std::move(intrusive_from_this)));
  autograd_meta-&gt;grad_accumulator_ = result;
  return result;
}
</code></pre>
<p>其中 AcculateGrad 中的 Variable 即 aten::Tensor 指向 self 的 TensorImpl 用于更新聚合梯度：</p>
<pre><code class="language-cpp">// torch/csrc/autograd/functions/accumulate_grad.h

struct TORCH_API AccumulateGrad : public Node {
  explicit AccumulateGrad(Variable variable_);
  variable_list apply(variable_list&amp;&amp; grads) override;
  Variable variable;
};
</code></pre>
<p>set_history</p>
<pre><code class="language-cpp">// torch/csrc/autograd/functions/utils.h

inline void set_history(
    at::Tensor&amp; variable,
    const std::shared_ptr&lt;Node&gt;&amp; grad_fn) {
  AT_ASSERT(grad_fn);
  if (variable.defined()) {
    auto output_nr = grad_fn-&gt;add_input_metadata(variable);
    impl::set_gradient_edge(variable, {grad_fn, output_nr});
  } else {
    grad_fn-&gt;add_input_metadata(Node::undefined_input());
  }
}

inline void set_history(
    std::vector&lt;Variable&gt;&amp;&amp; variables,
    const std::shared_ptr&lt;Node&gt;&amp; grad_fn) {
  for (auto&amp; variable : variables) {
    set_history(variable, grad_fn);
  }
}
</code></pre>
<p>set_gradient_edge 设置 Tensor 和 grad_fn_.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.cpp

void set_gradient_edge(const Variable&amp; self, Edge edge) {
  auto* meta = materialize_autograd_meta(self);
  meta-&gt;grad_fn_ = std::move(edge.function);
  meta-&gt;output_nr_ = edge.input_nr;
  auto diff_view_meta = get_view_autograd_meta(self);
  if (diff_view_meta &amp;&amp; diff_view_meta-&gt;has_bw_view()) {
    diff_view_meta-&gt;set_attr_version(self._version());
  }
}
</code></pre>
<h3 id="torchmul"><a class="header" href="#torchmul">torch.mul</a></h3>
<p>流程类似</p>
<pre><code class="language-cpp">// torch/csrc/autograd/generated/VariableType_0.cpp

at::Tensor mul_Tensor(c10::DispatchKeySet ks, const at::Tensor &amp; self, const at::Tensor &amp; other) {
  auto&amp; self_ = unpack(self, &quot;self&quot;, 0);
  auto&amp; other_ = unpack(other, &quot;other&quot;, 1);
  auto _any_requires_grad = compute_requires_grad( self, other );

  (void)_any_requires_grad;
  auto _any_has_forward_grad_result = (isFwGradDefined(self) || isFwGradDefined(other));
  (void)_any_has_forward_grad_result;
  std::shared_ptr&lt;MulBackward0&gt; grad_fn;
  if (_any_requires_grad) {
    grad_fn = std::shared_ptr&lt;MulBackward0&gt;(new MulBackward0(), deleteNode);
    grad_fn-&gt;set_next_edges(collect_next_edges( self, other ));
    if (grad_fn-&gt;should_compute_output(1)) {
      grad_fn-&gt;self_ = SavedVariable(self, false);
    }
    grad_fn-&gt;other_scalar_type = other.scalar_type();
    grad_fn-&gt;self_scalar_type = self.scalar_type();
    if (grad_fn-&gt;should_compute_output(0)) {
      grad_fn-&gt;other_ = SavedVariable(other, false);
    }
  }
  auto _tmp = ([&amp;]() {
    at::AutoDispatchBelowADInplaceOrView guard;
    return at::redispatch::mul(ks &amp; c10::after_autograd_keyset, self_, other_);
  })();
  auto result = std::move(_tmp);

  if (grad_fn) {
      set_history(flatten_tensor_args( result ), grad_fn);
  }

  if (result_new_fw_grad_opt.has_value() &amp;&amp; result_new_fw_grad_opt.value().defined() &amp;&amp; result.defined()) {
    // The hardcoded 0 here will need to be updated once we support multiple levels.
    result._set_fw_grad(result_new_fw_grad_opt.value(), /* level */ 0, /* is_inplace_op */ false);
  }
  return result;
}
</code></pre>
<p>不同是的是因为乘法的求导和输入有关，所以我们在构建 MulBackward0 的时候需要把输入保存下来，即代码中的 SavedVariable 用于保存实例.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/generated/Functions.h 

struct TORCH_API MulBackward0 : public TraceableFunction {
  using TraceableFunction::TraceableFunction;
  variable_list apply(variable_list&amp;&amp; grads) override;
  std::string name() const override { return &quot;MulBackward0&quot;; }
  void release_variables() override {
    std::lock_guard&lt;std::mutex&gt; lock(mutex_);
    self_.reset_data();
    other_.reset_data();
  }

  SavedVariable self_;
  at::ScalarType other_scalar_type;
  at::ScalarType self_scalar_type;
  SavedVariable other_;

};
</code></pre>
<p>使用 SavedVariable 来保存前向 Var 的数据区而不影响其管理反向 Op 的生命周期：</p>
<pre><code class="language-cpp">// torch/csrc/autograd/saved_variable.h

class TORCH_API SavedVariable {
 public:
  /// Reconstructs the saved variable. Pass `saved_for` as the gradient
  /// function if constructing the `SavedVariable` with it would have caused a
  /// circular reference.
  Variable unpack(std::shared_ptr&lt;Node&gt; saved_for = nullptr) const;

 private:
  at::Tensor data_;

  std::shared_ptr&lt;ForwardGrad&gt; fw_grad_;

  std::weak_ptr&lt;Node&gt; weak_grad_fn_;
  c10::VariableVersion version_counter_;

  uint32_t saved_version_ = 0;
  uint32_t output_nr_ = 0;
  bool was_default_constructed_ = true;
  bool is_inplace_on_view_ = false;
  bool saved_original_ = false;
  bool is_leaf_ = false;
  bool is_output_ = false;

  std::unique_ptr&lt;SavedVariableHooks&gt; hooks_;
  std::shared_ptr&lt;Node&gt; grad_fn_;
  std::weak_ptr&lt;Node&gt; grad_accumulator_;
  bool requires_grad_ = false;
};
</code></pre>
<h2 id="lifecycle"><a class="header" href="#lifecycle">Lifecycle</a></h2>
<p><img src="5.png" alt="1" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../pytorch/tensor.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                            <a rel="next" href="../pytorch/profiler.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../pytorch/tensor.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
                    <a rel="next" href="../pytorch/profiler.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="../clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="../highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="../book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
    </body>
</html>
