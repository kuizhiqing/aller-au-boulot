<!DOCTYPE HTML>
<html lang="en" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>tensor - Aller au boulot</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="index.html"><strong aria-hidden="true">1.</strong> welcome</a></li><li class="chapter-item "><a href="vllm/overview.html"><strong aria-hidden="true">2.</strong> vllm</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="vllm/architecture.html"><strong aria-hidden="true">2.1.</strong> architecture</a></li></ol></li><li class="chapter-item "><a href="somewhat/llama.html"><strong aria-hidden="true">3.</strong> llama</a></li><li class="chapter-item "><a href="chronicles/2024mar.html"><strong aria-hidden="true">4.</strong> chronicles</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chronicles/2024feb.html"><strong aria-hidden="true">4.1.</strong> feb 2024</a></li></ol></li><li class="chapter-item "><a href="projects/projects.html"><strong aria-hidden="true">5.</strong> projects</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="projects/copilot.html"><strong aria-hidden="true">5.1.</strong> copilot</a></li><li class="chapter-item "><a href="projects/library.html"><strong aria-hidden="true">5.2.</strong> library</a></li><li class="chapter-item "><a href="projects/rag.html"><strong aria-hidden="true">5.3.</strong> RAG</a></li></ol></li><li class="chapter-item "><a href="survey/papers.html"><strong aria-hidden="true">6.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/pollux.html"><strong aria-hidden="true">6.1.</strong> pollux</a></li><li class="chapter-item "><a href="survey/adasum.html"><strong aria-hidden="true">6.2.</strong> adasum</a></li><li class="chapter-item "><a href="survey/adaptation_learning.html"><strong aria-hidden="true">6.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="survey/gradient_descent.html"><strong aria-hidden="true">6.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="survey/auto_parallel.html"><strong aria-hidden="true">6.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="survey/scheduling.html"><strong aria-hidden="true">6.6.</strong> scheduling</a></li><li class="chapter-item "><a href="survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">6.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/gradient_compression/dgc.html"><strong aria-hidden="true">6.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="survey/gradient_compression/csc.html"><strong aria-hidden="true">6.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="survey/flash_attention.html"><strong aria-hidden="true">6.8.</strong> flash attention</a></li><li class="chapter-item "><a href="survey/lora.html"><strong aria-hidden="true">6.9.</strong> LoRA</a></li></ol></li><li class="chapter-item "><a href="mathematics/topics.html"><strong aria-hidden="true">7.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="mathematics/basic.html"><strong aria-hidden="true">7.1.</strong> basic</a></li><li class="chapter-item "><a href="mathematics/entropy.html"><strong aria-hidden="true">7.2.</strong> entropy</a></li><li class="chapter-item "><a href="mathematics/newton.html"><strong aria-hidden="true">7.3.</strong> newton</a></li><li class="chapter-item "><a href="mathematics/regression.html"><strong aria-hidden="true">7.4.</strong> regression</a></li><li class="chapter-item "><a href="mathematics/conjugate_descent.html"><strong aria-hidden="true">7.5.</strong> conjugate descent</a></li><li class="chapter-item "><a href="mathematics/gradient_descent.html"><strong aria-hidden="true">7.6.</strong> gradient descent</a></li><li class="chapter-item "><a href="mathematics/pca.html"><strong aria-hidden="true">7.7.</strong> pca</a></li><li class="chapter-item "><a href="mathematics/support_vector.html"><strong aria-hidden="true">7.8.</strong> support vector</a></li><li class="chapter-item "><a href="mathematics/differentiation.html"><strong aria-hidden="true">7.9.</strong> differentiation</a></li><li class="chapter-item "><a href="mathematics/fourier.html"><strong aria-hidden="true">7.10.</strong> fourier</a></li><li class="chapter-item "><a href="mathematics/kmeans_cos.html"><strong aria-hidden="true">7.11.</strong> kmeans</a></li></ol></li><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">8.</strong> wavelets</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">8.1.</strong> plan</a></li><li class="chapter-item "><a href="wavelets/preliminary.html"><strong aria-hidden="true">8.2.</strong> preliminary</a></li><li class="chapter-item "><a href="wavelets/haar.html"><strong aria-hidden="true">8.3.</strong> haar wavelet</a></li><li class="chapter-item "><a href="wavelets/fourier.html"><strong aria-hidden="true">8.4.</strong> fourier analysis</a></li><li class="chapter-item "><a href="wavelets/uncertainty_principle.html"><strong aria-hidden="true">8.5.</strong> uncertainty principle</a></li><li class="chapter-item "><a href="wavelets/multiresolution.html"><strong aria-hidden="true">8.6.</strong> multiresolution</a></li></ol></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">9.</strong> models</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="llm/llm.html"><strong aria-hidden="true">9.1.</strong> llm</a></li><li class="chapter-item "><a href="llm/falcon.html"><strong aria-hidden="true">9.2.</strong> falcon</a></li><li class="chapter-item "><a href="llm/llama.html"><strong aria-hidden="true">9.3.</strong> llama</a></li><li class="chapter-item "><a href="llm/peft.html"><strong aria-hidden="true">9.4.</strong> peft</a></li><li class="chapter-item "><a href="llm/transformer.html"><strong aria-hidden="true">9.5.</strong> transformer</a></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">9.6.</strong> models</a></li></ol></li><li class="chapter-item "><a href="megatron/megatron.html"><strong aria-hidden="true">10.</strong> megatron</a></li><li class="chapter-item "><a href="deepspeed/deepspeed.html"><strong aria-hidden="true">11.</strong> deepspeed</a></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">12.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">12.1.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/autograd.html"><strong aria-hidden="true">12.2.</strong> autograd</a></li><li class="chapter-item "><a href="pytorch/operator.html"><strong aria-hidden="true">12.3.</strong> operator</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">12.4.</strong> profiler</a></li><li class="chapter-item "><a href="pytorch/hook.html"><strong aria-hidden="true">12.5.</strong> hook</a></li><li class="chapter-item "><a href="pytorch/elastic.html"><strong aria-hidden="true">12.6.</strong> elastic</a></li><li class="chapter-item "><a href="pytorch/patch.html"><strong aria-hidden="true">12.7.</strong> patch</a></li><li class="chapter-item "><a href="pytorch/misc.html"><strong aria-hidden="true">12.8.</strong> misc</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">13.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps-code-overview.html"><strong aria-hidden="true">13.1.</strong> ps</a></li><li class="chapter-item "><a href="paddle/framework.html"><strong aria-hidden="true">13.2.</strong> framework</a></li><li class="chapter-item "><a href="paddle/cinn.html"><strong aria-hidden="true">13.3.</strong> cinn</a></li><li class="chapter-item "><a href="paddle/dataloader.html"><strong aria-hidden="true">13.4.</strong> dataloader</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">14.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">14.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/workflow.html"><strong aria-hidden="true">14.2.</strong> workflow</a></li><li class="chapter-item "><a href="horovod/object.html"><strong aria-hidden="true">14.3.</strong> object</a></li><li class="chapter-item "><a href="horovod/develop.html"><strong aria-hidden="true">14.4.</strong> develop</a></li><li class="chapter-item "><a href="horovod/pytorch.html"><strong aria-hidden="true">14.5.</strong> pytorch</a></li><li class="chapter-item "><a href="horovod/tensorflow.html"><strong aria-hidden="true">14.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="horovod/elastic.html"><strong aria-hidden="true">14.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">15.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">15.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">15.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">15.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">15.4.</strong> api</a></li><li class="chapter-item "><a href="ray/survey.html"><strong aria-hidden="true">15.5.</strong> survey</a></li></ol></li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">16.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">16.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="python/multiprocessing.html"><strong aria-hidden="true">16.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="python/decorator.html"><strong aria-hidden="true">16.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="tips/tips.html"><strong aria-hidden="true">17.</strong> tips</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/enable_shared_from_this.html"><strong aria-hidden="true">17.1.</strong> enable_shared_from_this</a></li><li class="chapter-item "><a href="tips/ip_local_port_range.html"><strong aria-hidden="true">17.2.</strong> ip_local_port_range</a></li><li class="chapter-item "><a href="tips/golang_error.html"><strong aria-hidden="true">17.3.</strong> golang error</a></li></ol></li><li class="chapter-item "><a href="infra/overview.html"><strong aria-hidden="true">18.</strong> infrastructure</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="infra/pki.html"><strong aria-hidden="true">18.1.</strong> pki</a></li><li class="chapter-item "><a href="infra/cache.html"><strong aria-hidden="true">18.2.</strong> linux cache</a></li></ol></li><li class="chapter-item "><a href="kubernetes/kubernetes.html"><strong aria-hidden="true">19.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kubernetes/concepts.html"><strong aria-hidden="true">19.1.</strong> concepts</a></li><li class="chapter-item "><a href="kubernetes/scheduler.html"><strong aria-hidden="true">19.2.</strong> scheduler</a></li><li class="chapter-item "><a href="kubernetes/operator.html"><strong aria-hidden="true">19.3.</strong> operator</a></li><li class="chapter-item "><a href="kubernetes/device_plugin.html"><strong aria-hidden="true">19.4.</strong> device plugin</a></li><li class="chapter-item "><a href="kubernetes/docker.html"><strong aria-hidden="true">19.5.</strong> docker</a></li><li class="chapter-item "><a href="kubernetes/install.html"><strong aria-hidden="true">19.6.</strong> install</a></li><li class="chapter-item "><a href="kubernetes/api_service.html"><strong aria-hidden="true">19.7.</strong> api-service</a></li><li class="chapter-item "><a href="kubernetes/controller.html"><strong aria-hidden="true">19.8.</strong> controller</a></li></ol></li><li class="chapter-item "><a href="nccl/nccl.html"><strong aria-hidden="true">20.</strong> nccl</a></li><li class="chapter-item "><a href="nvidia/nvidia.html"><strong aria-hidden="true">21.</strong> cuda</a></li><li class="chapter-item "><a href="somewhat/todo.html"><strong aria-hidden="true">22.</strong> todo</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="somewhat/gloo.html"><strong aria-hidden="true">22.1.</strong> gloo</a></li><li class="chapter-item "><a href="somewhat/mpi.html"><strong aria-hidden="true">22.2.</strong> mpi</a></li><li class="chapter-item "><a href="somewhat/jax.html"><strong aria-hidden="true">22.3.</strong> jax</a></li><li class="chapter-item "><a href="somewhat/tvm.html"><strong aria-hidden="true">22.4.</strong> tvm</a></li><li class="chapter-item "><a href="somewhat/github.html"><strong aria-hidden="true">22.5.</strong> llm</a></li></ol></li><li class="chapter-item "><a href="notes/index.html"><strong aria-hidden="true">23.</strong> notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="notes/influence_and_persuasion.html"><strong aria-hidden="true">23.1.</strong> influence and persuasion</a></li><li class="chapter-item "><a href="notes/feynman_technique.html"><strong aria-hidden="true">23.2.</strong> freynman technique</a></li><li class="chapter-item "><a href="notes/wavelet_tour_signal_processing_sparse.html"><strong aria-hidden="true">23.3.</strong> wavelet signal processing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="tensor"><a class="header" href="#tensor">Tensor</a></h1>
<p>How <code>torch.tensor</code> works ?</p>
<h2 id="init"><a class="header" href="#init">init</a></h2>
<p>What's behind <code>import torch</code> ?</p>
<pre><code class="language-python"># torch/__init__.py

from torch._C import *
</code></pre>
<p>cf <a href="https://docs.python.org/3/extending/building.html">doc</a></p>
<pre><code class="language-c">// torch/csrc/stub.c

PyMODINIT_FUNC PyInit__C(void)
{
  return initModule();
}
</code></pre>
<pre><code class="language-cpp">// torch/csrc/Module.cpp

extern "C"
TORCH_API PyObject* initModule();
PyObject* initModule() {
  THPUtils_addPyMethodDefs(methods, TorchMethods);
  THPUtils_addPyMethodDefs(methods, torch::autograd::python_functions());

  static struct PyModuleDef torchmodule = {PyModuleDef_HEAD_INIT, "torch._C", nullptr, -1, methods.data()};
  ASSERT_TRUE(module = PyModule_Create(&amp;torchmodule));
  ASSERT_TRUE(THPGenerator_init(module));
  ASSERT_TRUE(THPException_init(module));
  THPSize_init(module);
  THPDtype_init(module);
  THPDTypeInfo_init(module);
  THPLayout_init(module);
  THPMemoryFormat_init(module);
  THPQScheme_init(module);
  THPDevice_init(module);
  THPStream_init(module);
  ASSERT_TRUE(THPVariable_initModule(module));
  ASSERT_TRUE(THPFunction_initModule(module));
  ASSERT_TRUE(THPEngine_initModule(module));

  at::init();

  return module;
}
</code></pre>
<h2 id="function-torchtensor"><a class="header" href="#function-torchtensor">function torch.tensor</a></h2>
<pre><code class="language-python">torch.tensor(1.0)
</code></pre>
<p>pytorch 使用 <a href="https://docs.python.org/3/c-api/structures.html#c.PyMethodDef">PyMethodDef</a>
暴露 python tensor, 对应类型 THPVariable_tensor.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/python_torch_functions_manual.cpp

static PyMethodDef torch_functions_manual[] = {
    {"tensor",
     castPyCFunctionWithKeywords(THPVariable_tensor),
     METH_VARARGS | METH_KEYWORDS | METH_STATIC,
     nullptr},
}

// implemented on python object to allow torch.tensor to be constructed with
// arbitrarily nested python objects - list, tuple, np array, scalar, etc.
static PyObject* THPVariable_tensor( PyObject* self, PyObject* args, PyObject* kwargs) {
  jit::tracer::warn("torch.tensor", jit::tracer::WARN_CONSTRUCTOR);
  return THPVariable_Wrap(torch::utils::tensor_ctor(
      torch::tensors::get_default_dispatch_key(),
      torch::tensors::get_default_scalar_type(),
      r));
}
</code></pre>
<ul>
<li>torch::utils::tensor_ctor() 返回 cpp tensor</li>
<li>torch::tensors::get_default_dispatch_key() 获取默认 dispatch key</li>
<li>torch::tensors::get_default_scalar_type() 获取默认数据类型</li>
<li>THPVariable_Wrap 把 tensor 封装成 python 可使用的 THPVariable</li>
</ul>
<p><img src="3.png" alt="1" /></p>
<h3 id="tensor_new"><a class="header" href="#tensor_new">tensor_new</a></h3>
<pre><code class="language-cpp">// torch/csrc/utils/tensor_new.cpp

Tensor tensor_ctor(
    c10::DispatchKey dispatch_key,
    at::ScalarType scalar_type,
    PythonArgs&amp; r) {
  if (r.idx == 0) {
    PyObject* data = r.pyobject(0);
    bool type_inference = r.isNone(1);
    bool pin_memory = r.toBool(3);
    bool args_requires_grad = r.toBool(4);
    auto new_tensor = internal_new_from_data(
        typeIdWithDefault(r, 2, dispatch_key),
        r.scalartypeWithDefault(1, scalar_type),
        r.deviceOptional(2),
        data,
        /*copy_variables=*/true,
        /*copy_numpy=*/true,
        /*type_inference=*/type_inference,
        pin_memory);
    auto names = r.toDimnameListOptional(5);
    if (names) {
      at::namedinference::propagate_names(
          new_tensor, *names, /*validate_names=*/true);
    }
    new_tensor.detach_(); // ensure new_tensor a leaf node
    new_tensor.set_requires_grad(args_requires_grad);
    return new_tensor;
  }
}
</code></pre>
<ul>
<li>解析参数</li>
<li>调用 internal_new_from_data 创建 cpp tensor，初始化 storage_</li>
<li>new_tensor.detach_() 确保是叶子结点，初始化 autograd_meta_</li>
</ul>
<h3 id="internal_new_from_data"><a class="header" href="#internal_new_from_data">internal_new_from_data</a></h3>
<pre><code class="language-cpp">// torch/csrc/utils/tensor_new.cpp

Tensor internal_new_from_data(
    c10::TensorOptions options,
    at::ScalarType scalar_type,
    c10::optional&lt;Device&gt; device_opt,
    PyObject* data,
    bool copy_variables,
    bool copy_numpy,
    bool type_inference,
    bool pin_memory = false) {
  if (THPVariable_Check(data)) {
    auto var = THPVariable_Unpack(data);
    return var.to(...);
  }

  if (PyObject_HasAttrString(data, "__cuda_array_interface__")) {
    auto tensor = tensor_from_cuda_array_interface(data);
    return tensor.to(...);
  }

  if (is_numpy_available() &amp;&amp; PyArray_Check(data)) {
     auto tensor = tensor_from_numpy(data, /*warn_if_not_writeable=*/!copy_numpy);
     return tensor.to(...);
  }

  auto device = device_opt.has_value() ? *device_opt : options.device();
  auto sizes = compute_sizes(data, scalar_type);
  ScalarType inferred_scalar_type = type_inference ? infer_scalar_type(data) : scalar_type;

  Tensor tensor;
  {
    {
      if (isStorage(data)) {
        Storage storage = createStorageGetType(data, storage_scalar_type, is_typed_storage);
        tensor = at::empty( sizes,
            at::initialTensorOptions().dtype( is_typed_storage ? storage_scalar_type : inferred_scalar_type)
                .pinned_memory(pin_memory)
                .device(storage.device()));
        tensor.set_(storage);

      } else {
        TensorOptions opts = at::initialTensorOptions().dtype(inferred_scalar_type);
        tensor = at::empty(sizes, opts.pinned_memory(pin_memory));
        recursive_store(
              (char*)tensor.data_ptr(),
              tensor.sizes(),
              tensor.strides(),
              0,
              inferred_scalar_type,
              tensor.dtype().itemsize(),
              data);
      }
    }
    maybe_initialize_cuda(device);
    tensor = tensor.to(device, inferred_scalar_type, /*non_blocking=*/false, /*copy=*/false);
  }

  return at::lift_fresh(tensor);
}
</code></pre>
<ul>
<li>at::empty() 创建 tensor</li>
<li>recursive_store() 初始化 tensor 数据</li>
</ul>
<p>其中 <code>detach_</code> 调用会调用 materialize_autograd_meta 初始化 autograd_meta_.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/variable.cpp

AutogradMeta* materialize_autograd_meta(const at::TensorBase&amp; self) {
  auto p = self.unsafeGetTensorImpl();
  if (!p-&gt;autograd_meta()) {
    p-&gt;set_autograd_meta(std::make_unique&lt;AutogradMeta&gt;());
  }
  return get_autograd_meta(self);
}
</code></pre>
<h3 id="thpvariable_wrap"><a class="header" href="#thpvariable_wrap">THPVariable_Wrap</a></h3>
<pre><code class="language-cpp">// torch/csrc/autograd/python_variable.cpp

PyObject* THPVariable_Wrap(at::TensorBase var) {
  return THPVariable_NewWithVar( (PyTypeObject*)THPVariableClass, std::move(var), status);
}

static PyObject* THPVariable_NewWithVar(PyTypeObject* type, Variable _var, c10::impl::PyInterpreterStatus status) {
  PyObject* obj = type-&gt;tp_alloc(type, 0);
  if (obj) {
    auto v = (THPVariable*)obj;
  }
  return obj;
}
</code></pre>
<h3 id="pymethoddef"><a class="header" href="#pymethoddef">PyMethodDef</a></h3>
<p><a href="https://docs.python.org/3/extending/extending.html">PyMethodDef</a> including</p>
<ul>
<li>PyModuleDef</li>
<li>PyInit_* and PyModule_Create</li>
</ul>
<pre><code class="language-cpp">// torch/csrc/autograd/python_variable.cpp

bool THPVariable_initModule(PyObject* module) {
  PyModule_AddObject(module, "_TensorMeta", (PyObject*)&amp;THPVariableMetaType);

  static std::vector&lt;PyMethodDef&gt; methods;
  THPUtils_addPyMethodDefs(methods, torch::autograd::variable_methods);
  THPUtils_addPyMethodDefs(methods, extra_methods);
  THPVariableType.tp_methods = methods.data();
  PyModule_AddObject(module, "_TensorBase", (PyObject*)&amp;THPVariableType);
  torch::autograd::initTorchFunctions(module);
  torch::autograd::initTensorImplConversion(module);
  return true;
}
</code></pre>
<p>initTorchFunctions</p>
<pre><code class="language-cpp">// torch/csrc/autograd/python_torch_functions_manual.cpp

namespace torch {
namespace autograd {

static PyTypeObject THPVariableFunctions = {
    PyVarObject_HEAD_INIT(
        nullptr,
        0) "torch._C._VariableFunctionsClass", /* tp_name */
    0, /* tp_basicsize */
    ...
}

void initTorchFunctions(PyObject* module) {
  static std::vector&lt;PyMethodDef&gt; torch_functions;
  gatherTorchFunctions(torch_functions);
  THPVariableFunctions.tp_methods = torch_functions.data();

  if (PyModule_AddObject(
          module,
          "_VariableFunctionsClass",
          reinterpret_cast&lt;PyObject*&gt;(&amp;THPVariableFunctions)) &lt; 0) { }
  THPVariableFunctionsModule =
      PyType_GenericNew(&amp;THPVariableFunctions, Py_None, Py_None);
  if (PyModule_AddObject(
          module, "_VariableFunctions", THPVariableFunctionsModule) &lt; 0) { }
}
} // namespace autograd
} // namespace torch
</code></pre>
<p>checkout it out</p>
<pre><code class="language-python">torch.tensor
&lt;built-in method tensor of type object at 0x7f72955df1a0&gt;
torch._C._VariableFunctions.tensor
&lt;built-in method tensor of type object at 0x7f72955df1a0&gt;
torch._C._VariableFunctionsClass.__dict__['tensor']
&lt;staticmethod object at 0x7f7296e30a10&gt;
</code></pre>
<pre><code class="language-python"># torch/__init__.py

for name in dir(_C._VariableFunctions):
    if name.startswith('__') or name in PRIVATE_OPS:
        continue
    obj = getattr(_C._VariableFunctions, name)
    obj.__module__ = 'torch'
    globals()[name] = obj
    if not name.startswith("_"):
        __all__.append(name)

</code></pre>
<p>gatherTorchFunctions 填充 torch_functions, 包括 tensor.</p>
<pre><code class="language-cpp">// torch/csrc/autograd/python_torch_functions_manual.cpp

void gatherTorchFunctions(std::vector&lt;PyMethodDef&gt;&amp; torch_functions) {
  constexpr size_t num_functions =
      sizeof(torch_functions_manual) / sizeof(torch_functions_manual[0]);
  torch_functions.assign(
      torch_functions_manual, torch_functions_manual + num_functions);
  gatherTorchFunctions_0(torch_functions);
  gatherTorchFunctions_1(torch_functions);
  gatherTorchFunctions_2(torch_functions);

  static std::array&lt;std::pair&lt;const char*, const char*&gt;, 4&gt; aliases{
      {// Canonical function, alias name
       {"sspaddmm", "saddmm"},
       {"mm", "spmm"},
       {"mm", "dsmm"},
       {"hspmm", "hsmm"}}};

  for (const auto&amp; alias : aliases) {
    auto it = std::find_if(
        torch_functions.begin(),
        torch_functions.end(),
        [&amp;](const PyMethodDef&amp; def) {
          return strcmp(def.ml_name, alias.first) == 0;
        });
    TORCH_INTERNAL_ASSERT(
        it != torch_functions.end(),
        "Failed to create function alias from ",
        alias.first,
        " to ",
        alias.second);
    PyMethodDef alias_def = *it;
    alias_def.ml_name = alias.second;

    torch_functions.push_back(alias_def);
  }

  torch_functions.push_back({nullptr});
}
</code></pre>
<p><code>torch_functions.assign(torch_functions_manual, ...);</code></p>
<blockquote>
<p>很多 functions 由
<code>tools/autograd/gen_python_functions.py</code>
自动生成的文件
<code>torch/csrc/autograd/generated/python_torch_functions_0.cpp</code>
定义。文件中的这些 functions 由同文件中的函数
<code>gatherTorchFunctions_0</code> 添加到 <code>torch_functions</code> 从而添加进 <code>PyModuleDef</code>.</p>
</blockquote>
<pre><code class="language-python"># tools/autograd/gen_python_functions.py 

# These functions require manual Python bindings or are not exposed to Python
_SKIP_PYTHON_BINDINGS = [
    "tensor",
]

def gen( out: str, native_yaml_path: str, tags_yaml_path: str, deprecated_yaml_path: str, template_path: str, *, symint: bool = True,):
    create_python_bindings( None, "python_variable_methods.cpp",)
    create_python_bindings_sharded( "torch", "python_torch_functions.cpp",)
    create_python_bindings( "torch.nn", "python_nn_functions.cpp",)
    create_python_bindings( "torch.fft", "python_fft_functions.cpp",)
    create_python_bindings( "torch.linalg", "python_linalg_functions.cpp",)
    create_python_bindings( "torch.nested", "python_nested_functions.cpp",)
    create_python_bindings( "torch.sparse", "python_sparse_functions.cpp",)
    create_python_bindings( "torch.special", "python_special_functions.cpp",)
    create_python_return_type_bindings( fm, functions, lambda fn: True, "python_return_types.cpp")
</code></pre>
<h2 id="class-torchtensor"><a class="header" href="#class-torchtensor">class torch.Tensor</a></h2>
<p>What's behind <code>torch.Tensor</code> ?</p>
<h3 id="from-py-to-cpp"><a class="header" href="#from-py-to-cpp">From py to cpp</a></h3>
<pre><code class="language-python"># torch/__init__.py

from ._tensor import Tensor
</code></pre>
<p>python <code>Tensor</code> 都继承自 <code>torch._C._TensorBase</code></p>
<pre><code class="language-python"># torch/_tensor.py

class Tensor(torch._C._TensorBase):
    def __deepcopy__(self, memo):
        ...
    def storage(self):
        return self._typed_storage()
    def backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None):
        torch.autograd.backward(
            self, gradient, retain_graph, create_graph, inputs=inputs
        )
    def register_hook(self, hook):
        return handle
    ...
</code></pre>
<p>python 的 <code>torch._C._TensorBase</code> 绑定在 cpp <code>THPVariableType</code> 上</p>
<pre><code class="language-cpp">// torch/csrc/autograd/python_variable.cpp

bool THPVariable_initModule(PyObject *module)
{
  THPVariableMetaType.tp_base = &amp;PyType_Type;
  if (PyType_Ready(&amp;THPVariableMetaType) &lt; 0)
    return false;
  Py_INCREF(&amp;THPVariableMetaType);
  PyModule_AddObject(module, "_TensorMeta",   (PyObject *)&amp;THPVariableMetaType);

  static std::vector&lt;PyMethodDef&gt; methods;
  THPUtils_addPyMethodDefs(methods, torch::autograd::variable_methods);
  THPUtils_addPyMethodDefs(methods, extra_methods);
  THPVariableType.tp_methods = methods.data();
  if (PyType_Ready(&amp;THPVariableType) &lt; 0)
    return false;
  Py_INCREF(&amp;THPVariableType);
  PyModule_AddObject(module, "_TensorBase",   (PyObject *)&amp;THPVariableType);
  torch::autograd::initTorchFunctions(module);
  torch::autograd::initTensorImplConversion(module);
  return true;
}

PyTypeObject THPVariableType = {
    PyVarObject_HEAD_INIT(
        &amp;THPVariableMetaType,
        0) "torch._C._TensorBase", /* tp_name */
    sizeof(THPVariable), /* tp_basicsize */
    0, /* tp_itemsize */
    ...
    THPVariable_pynew, /* tp_new */
};

PyTypeObject THPVariableMetaType = {
  PyVarObject_HEAD_INIT(DEFERRED_ADDRESS(&amp;PyType_Type), 0)
  "torch._C._TensorMeta",                      /* tp_name */
  sizeof(THPVariableMeta),
  ...
  THPVariableMetaType_init,                    /* tp_init */
  nullptr,                                     /* tp_alloc */
  nullptr,                                     /* tp_new */
};
</code></pre>
<pre><code class="language-cpp">// torch/csrc/autograd/python_variable.h

// Python object that backs torch.autograd.Variable
struct THPVariable {
  PyObject_HEAD;
  // Payload
  c10::MaybeOwned&lt;at::Tensor&gt; cdata;
  // Hooks to be run on backwards pass (corresponds to Python attr
  // '_backwards_hooks', set by 'register_hook')
  PyObject* backward_hooks = nullptr;
};
</code></pre>
<pre><code class="language-cpp">// torch/csrc/autograd/function_hook.h

namespace torch { namespace autograd {

using Variable = at::Tensor;
using variable_list = std::vector&lt;Variable&gt;;

} }
</code></pre>
<h3 id="tensorbase-and-tensor"><a class="header" href="#tensorbase-and-tensor">TensorBase and Tensor</a></h3>
<p>TensorBase</p>
<p><code>Tensor</code> 继承自 <code>TensorBase</code>, <code>TensorBase</code> 不依赖 function 自动生成，使用 <code>TensorBase</code> 可以避免自动生成部分有改动时全量编译，其次是引用计数问题。</p>
<pre><code class="language-cpp">// aten/src/ATen/core/TensorBase.h

// Convert Tensor to TensorBase without any need to include Tensor.h
TORCH_API const TensorBase&amp; get_tensor_base(const Tensor&amp; t);

// NOTE: [Tensor vs. TensorBase]
//
// Tensor, being the central data structure in PyTorch, gets used and
// it's header included almost everywhere. Unfortunately this means
// every time an operator signature is updated or changed in
// native_functions.yaml, you (and every other PyTorch developer) need
// to recompile all of ATen and it's dependencies.
//
// TensorBase aims to break up these header dependencies, and improve
// incremental build times for all PyTorch developers. TensorBase
// represents a reference counted handle to TensorImpl, exactly the
// same as Tensor. However, TensorBase doesn't have code generated
// methods in it's API and thus no dependence on native_functions.yaml.
//
// Usage tips
// ----------
// - You can `#define TORCH_ASSERT_NO_OPERATORS` at the top of a .cpp
//   or .cu file to ensure it has no header dependencies on
//   native_functions.yaml (direct or indirect).
// - Tensor inherits from TensorBase, so functions taking
//   `const TensorBase &amp;` are callable with Tensor as well.
// - TensorBase can be converted to tensor with `Tensor(tensor_base)`,
//   but this requires a reference-count bump. OptionalTensorRef on
//   the other hand can materialize a `const Tensor &amp;` without
//   touching the reference-count.
class TORCH_API TensorBase {
 public:
  const c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt;&amp; getIntrusivePtr() const {
    return impl_;
  }

  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; unsafeReleaseIntrusivePtr() {
    return std::move(impl_);
  }

  DispatchKeySet key_set() const {
    return impl_-&gt;key_set();
  }
  ScalarType scalar_type() const {
    return typeMetaToScalarType(impl_-&gt;dtype());
  }
  const Storage&amp; storage() const {
    return impl_-&gt;storage();
  }

  Layout layout() const noexcept {
    return impl_-&gt;layout();
  }

  caffe2::TypeMeta dtype() const noexcept {
    return impl_-&gt;dtype();
  }

  inline Device device() const {
    return impl_-&gt;device();
  }

  int64_t get_device() const {
    // NB: this is not a native function to avoid dispatching overhead.
    return impl_-&gt;get_device();
  }

  TensorOptions options() const {
    return TensorOptions().dtype(dtype())
                          .device(device())
                          .layout(layout());
  }

  void* data_ptr() const {
    return this-&gt;unsafeGetTensorImpl()-&gt;data();
  }

  template &lt;typename T&gt;
  T * data_ptr() const;

  at::TensorBase tensor_data() const;

  at::TensorBase variable_data() const;

  const std::shared_ptr&lt;torch::autograd::Node&gt;&amp; grad_fn() const;

protected:
  c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; impl_;
};
</code></pre>
<p>Tensor API</p>
<pre><code class="language-cpp">// torch/include/ATen/core/TensorBody.h
// aten/src/ATen/templates/TensorBody.h

// Tensor is a "generic" object holding a pointer to the underlying TensorImpl object, which has an embedded reference count. 
// similar to boost::intrusive_ptr.
class TORCH_API Tensor: public TensorBase {
 protected:
  friend MaybeOwnedTraits&lt;Tensor&gt;;
  friend OptionalTensorRef;

 public:
  explicit Tensor(
      c10::intrusive_ptr&lt;TensorImpl, UndefinedTensorImpl&gt; tensor_impl)
      : TensorBase(std::move(tensor_impl)) {}
  Tensor(const Tensor &amp;tensor) = default;
  Tensor(Tensor &amp;&amp;tensor) = default;

  explicit Tensor(const TensorBase &amp;base): TensorBase(base) {}
  c10::MaybeOwned&lt;Tensor&gt; expect_contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) const &amp;;
  c10::MaybeOwned&lt;Tensor&gt; expect_contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) &amp;&amp; = delete;

  Tensor&amp; operator=(const TensorBase&amp; x) &amp; {
    impl_ = x.getIntrusivePtr();
    return *this;
  }
  Tensor&amp; operator=(TensorBase&amp;&amp; x) &amp; { }
  Tensor&amp; operator=(const Tensor &amp;x) &amp; { }
  Tensor&amp; operator=(Tensor &amp;&amp;x) &amp; { }
  Tensor&amp; operator=(Scalar v) &amp;&amp; { }
  Tensor&amp; operator=(const Tensor &amp;rhs) &amp;&amp; { }
  Tensor&amp; operator=(Tensor&amp;&amp; rhs) &amp;&amp; { }

  void backward(const Tensor &amp; gradient={}, c10::optional&lt;bool&gt; retain_graph=c10::nullopt, bool create_graph=false, c10::optional&lt;TensorList&gt; inputs=c10::nullopt) const { }
  Tensor data() const {
    return TensorBase::data();
  }
}
</code></pre>
<p>Tensor 的继承关系</p>
<p><img src="2.png" alt="1" /></p>
<p><code>at:Tensor</code> 本质是 Tensor 的 API，底层是 <code>TensorImpl</code></p>
<pre><code class="language-cpp">// c10/core/TensorImpl.h

/**
 * The low-level representation of a tensor, which contains a pointer
 * to a storage (which contains the actual data) and metadata (e.g., sizes and
 * strides) describing this particular view of the data as a tensor.
 *
 */

struct C10_API TensorImpl : public c10::intrusive_ptr_target {
  enum ImplType { VIEW };

 public:

  virtual IntArrayRef strides() const;

  TENSORIMPL_MAYBE_VIRTUAL const Storage&amp; storage() const {
    return storage_;
  }

  Device device() const {
    return *device_opt_;
  }

  Layout layout() const {
    // This keyset must also be kept in sync with the logic in
    // is_sparse() / is_sparse_csr() / is_mkldnn()
    constexpr auto sparse_and_sparsecsr_and_mkldnn_ks =
        c10::sparse_ks | c10::sparse_csr_ks | c10::mkldnn_ks;
    ...
  }

  Storage storage_;

  inline T* data() const {
      return data_ptr_impl&lt;T&gt;();
  }
  inline T* data_ptr_impl() const {
      return storage_.unsafe_data&lt;T&gt;() + storage_offset_;
  }

  inline void* data() const {
      return static_cast&lt;void*&gt;(
        static_cast&lt;char*&gt;(storage_.data()) +
        data_type_.itemsize() * storage_offset_);
  }

  const caffe2::TypeMeta dtype() const {
    return data_type_;
  }

  DeviceType device_type() const {
    return (*device_opt_).type();
  }

 private:
  // This pointer points to an AutogradMeta struct that stores autograd-specific
  // fields (such as grad_ / grad_fn_ / grad_accumulator_). This pointer always
  // has unique ownership (meaning only one TensorImpl can own it at a time).
  //
  std::unique_ptr&lt;c10::AutogradMetaInterface&gt; autograd_meta_ = nullptr;

 protected:
  std::unique_ptr&lt;c10::NamedTensorMetaInterface&gt; named_tensor_meta_ = nullptr;

  c10::VariableVersion version_counter_;

  PyObject* pyobj_;

  c10::impl::SizesAndStrides sizes_and_strides_;

  caffe2::TypeMeta data_type_;

  c10::optional&lt;c10::Device&gt; device_opt_;

  const at::Tensor&amp; grad() const;
}
</code></pre>
<p>Storage</p>
<pre><code class="language-cpp">// c10/core/Storage.h

struct C10_API Storage {

  void* data() const {
    return storage_impl_.get()-&gt;data();
  }

  at::DataPtr&amp; data_ptr() {
    return storage_impl_-&gt;data_ptr();
  }

  const at::DataPtr&amp; data_ptr() const {
    return storage_impl_-&gt;data_ptr();
  }

 protected:
  c10::intrusive_ptr&lt;StorageImpl&gt; storage_impl_;
};
</code></pre>
<p>StorageImpl</p>
<pre><code class="language-cpp">// c10/core/StorageImpl.h 

struct C10_API StorageImpl : public c10::intrusive_ptr_target {
 private:
  DataPtr data_ptr_;
  size_t size_bytes_;
  bool resizable_;
  bool received_cuda_;
  Allocator* allocator_;
}
</code></pre>
<pre><code class="language-cpp">// c10/core/Allocator.h

class C10_API DataPtr {
 private:
  c10::detail::UniqueVoidPtr ptr_;
  Device device_;
}
</code></pre>
<p>UniqueVoidPtr</p>
<pre><code class="language-cpp">// c10/util/UniqueVoidPtr.h

namespace c10 {

using DeleterFnPtr = void (*)(void*);

namespace detail {

class UniqueVoidPtr {
 private:
  void* data_;
  std::unique_ptr&lt;void, DeleterFnPtr&gt; ctx_;
}

} }
</code></pre>
<p><code>detail::UniqueVoidPtr</code> is an owning smart pointer like <code>unique_ptr</code></p>
<ul>
<li>specialized to void</li>
<li>specialized for a function pointer deleter <code>void(void* ctx)</code></li>
<li>deleter is guaranteed to be called when the unique pointer is destructed and the context is non-null</li>
</ul>
<h2 id="why-this-"><a class="header" href="#why-this-">Why this ?</a></h2>
<pre><code class="language-bash">&gt;&gt;&gt; torch.Tensor(1)
tensor([6.7871e-27])
&gt;&gt;&gt; torch.Tensor(1.0)
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
TypeError: new(): data must be a sequence (got float)
</code></pre>
<pre><code class="language-cpp">// torch/csrc/autograd/python_variable.cpp

PyObject* THPVariable_pynew(
    PyTypeObject* type,
    PyObject* args,
    PyObject* kwargs) {
  TORCH_CHECK(
      type != &amp;THPVariableType,
      "Cannot directly construct _TensorBase; subclass it and then construct that");
  auto tensor = torch::utils::base_tensor_ctor(args, kwargs);
  return THPVariable_NewWithVar(
      type,
      std::move(tensor),
      c10::impl::PyInterpreterStatus::MAYBE_UNINITIALIZED);
}
</code></pre>
<pre><code class="language-cpp">// torch/csrc/utils/tensor_new.cpp

Tensor base_tensor_ctor(PyObject* args, PyObject* kwargs) {
  return legacy_tensor_generic_ctor_new(
      torch::tensors::get_default_dispatch_key(),
      torch::tensors::get_default_scalar_type(),
      args,
      kwargs,
      CtorOrNew::BASE_CTOR);
}

Tensor legacy_tensor_generic_ctor_new(
    c10::DispatchKey dispatch_key,
    at::ScalarType scalar_type,
    PyObject* args,
    PyObject* kwargs,
    CtorOrNew ctor_or_new) {
  auto options = dispatchKeyToTensorOptions(dispatch_key);
  static PythonArgParser parser({
      "new(*, Device? device=None)",
      "new(Storage storage)",
      "new(*, int64_t cdata)|hidden",
      // This constructor is no longer legacy, it will also be usable for
      // subclass initialization
      "new(Tensor other)",
      "new(Tensor other, *, Device? device=None)|hidden", // prevent Tensor
                                                          // matching with
                                                          // IntArrayRef,
                                                          // PyObject*
      "new(IntArrayRef size, *, Device? device=None)",
      "new(PyObject* data, *, Device? device=None)",
  });

  if (isSparse(dispatchKeyToBackend(dispatch_key))) {
    return legacy_sparse_tensor_generic_ctor_new(
        dispatch_key, scalar_type, args, kwargs, ctor_or_new);
  }

  if (ctor_or_new == CtorOrNew::NEW)
    check_base_legacy_new(dispatch_key, c10::kStrided);

  ParsedArgs&lt;2&gt; parsed_args;
  auto r = parser.parse(args, kwargs, parsed_args);
  if (r.idx == 0) {
    auto deviceOptional = r.deviceOptional(0);
    check_legacy_ctor_device(dispatch_key, deviceOptional);
    at::OptionalDeviceGuard device_guard(deviceOptional);
    return at::empty({0}, build_options(options, scalar_type));
  } else if (r.idx == 1) {
    at::ScalarType storage_scalar_type;
    bool is_typed_storage = false;
    at::Storage storage = r.storage(0, storage_scalar_type, is_typed_storage);
    if (storage_scalar_type != at::ScalarType::Undefined &amp;&amp; is_typed_storage) {
      TORCH_CHECK(
          storage_scalar_type == scalar_type,
          "Expected a Storage of type ",
          scalar_type,
          " or an UntypedStorage, but got type ",
          storage_scalar_type,
          " for argument 1 'storage'");
    }
    return new_with_storage(options, scalar_type, storage);
  } else if (r.idx == 2) {
    auto cdata = reinterpret_cast&lt;void*&gt;(r.toInt64(0));
    return at::unsafeTensorFromTH(cdata, true);
  } else if (r.idx == 3) {
    const auto&amp; other = r.tensor(0);
    // BASE_CTOR (aka torch.Tensor) is now relaxed to accept any
    // dtype; previously it was "float" biased
    if (ctor_or_new != CtorOrNew::BASE_CTOR) {
      options = options.dtype(scalar_type);
      TORCH_CHECK_TYPE(
          other.options().type_equal(options),
          "expected ",
          options,
          " (got ",
          other.options(),
          ")");
    }
    return other.alias();
  } else if (r.idx == 4) {
    if (ctor_or_new == CtorOrNew::CTOR || ctor_or_new == CtorOrNew::BASE_CTOR) {
      TORCH_CHECK(
          false,
          "Legacy tensor constructor of the form torch.Tensor(tensor, device=device) "
          "is not supported.  Use torch.tensor(...) or torch.as_tensor(...) instead.");
    } else {
      TORCH_CHECK(
          false,
          "Legacy tensor new of the form tensor.new(tensor, device=device) "
          "is not supported.  Use torch.as_tensor(...) instead.");
    }
  } else if (r.idx == 5) {
    PyObject* arg = r.pyobject(0);
    auto deviceOptional = r.deviceOptional(1);
    check_legacy_ctor_device(dispatch_key, deviceOptional);
    if (!THPSize_Check(arg) &amp;&amp; PyTuple_GET_SIZE(args) &gt;= 1 &amp;&amp;
        arg == PyTuple_GET_ITEM(args, 0)) {
      // new(sequence) binds to this signature but should be treated differently
      // unless the sequences is a torch.Size
      return legacy_new_from_sequence(
          options, scalar_type, deviceOptional, r.pyobject(0));
    }
    return new_with_sizes(
        options, scalar_type, r.deviceOptional(1), r.intlist(0));
  } else if (r.idx == 6) {
    auto deviceOptional = r.deviceOptional(1);
    check_legacy_ctor_device(dispatch_key, deviceOptional);
    return legacy_new_from_sequence(
        options, scalar_type, deviceOptional, r.pyobject(0));
  }
  throw std::runtime_error("new(): invalid arguments");
}
</code></pre>
<p>PyTorch Tensor 相关主要数据结构和关系</p>
<p><img src="1.png" alt="1" /></p>
<h2 id="tensor-api-dependency"><a class="header" href="#tensor-api-dependency">Tensor API dependency</a></h2>
<p>The dependency of tensor related API</p>
<p><img src="tensor-api.png" alt="Tensor View API Dependence" /></p>
<h2 id="view"><a class="header" href="#view">View</a></h2>
<p>Tensor view explication</p>
<p><img src="tensor-view.png" alt="Tensor View API Dependence" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../pytorch/overview.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../pytorch/autograd.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../pytorch/overview.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../pytorch/autograd.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
