<!DOCTYPE HTML>
<html lang="en" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>transformer - Aller au boulot</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item affix "><a href="index.html">welcome</a></li><li class="chapter-item affix "><li class="part-title">Hello World</li><li class="chapter-item affix "><li class="part-title">Open Source</li><li class="chapter-item "><a href="vllm/overview.html"><strong aria-hidden="true">1.</strong> vllm</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="vllm/architecture.html"><strong aria-hidden="true">1.1.</strong> architecture</a></li></ol></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">2.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/torchrun.html"><strong aria-hidden="true">2.1.</strong> torchrun</a></li><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">2.2.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/autograd.html"><strong aria-hidden="true">2.3.</strong> autograd</a></li><li class="chapter-item "><a href="pytorch/operator.html"><strong aria-hidden="true">2.4.</strong> operator</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">2.5.</strong> profiler</a></li><li class="chapter-item "><a href="pytorch/hook.html"><strong aria-hidden="true">2.6.</strong> hook</a></li><li class="chapter-item "><a href="pytorch/elastic.html"><strong aria-hidden="true">2.7.</strong> elastic</a></li><li class="chapter-item "><a href="pytorch/patch.html"><strong aria-hidden="true">2.8.</strong> patch</a></li><li class="chapter-item "><a href="pytorch/misc.html"><strong aria-hidden="true">2.9.</strong> misc</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">3.</strong> paddlepaddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps-code-overview.html"><strong aria-hidden="true">3.1.</strong> ps</a></li><li class="chapter-item "><a href="paddle/framework.html"><strong aria-hidden="true">3.2.</strong> framework</a></li><li class="chapter-item "><a href="paddle/cinn.html"><strong aria-hidden="true">3.3.</strong> cinn</a></li><li class="chapter-item "><a href="paddle/dataloader.html"><strong aria-hidden="true">3.4.</strong> dataloader</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">4.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">4.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/workflow.html"><strong aria-hidden="true">4.2.</strong> workflow</a></li><li class="chapter-item "><a href="horovod/object.html"><strong aria-hidden="true">4.3.</strong> object</a></li><li class="chapter-item "><a href="horovod/develop.html"><strong aria-hidden="true">4.4.</strong> develop</a></li><li class="chapter-item "><a href="horovod/pytorch.html"><strong aria-hidden="true">4.5.</strong> pytorch</a></li><li class="chapter-item "><a href="horovod/tensorflow.html"><strong aria-hidden="true">4.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="horovod/elastic.html"><strong aria-hidden="true">4.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">5.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">5.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">5.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">5.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">5.4.</strong> api</a></li><li class="chapter-item "><a href="ray/survey.html"><strong aria-hidden="true">5.5.</strong> survey</a></li></ol></li><li class="chapter-item "><a href="somewhat/llama.html"><strong aria-hidden="true">6.</strong> llama</a></li><li class="chapter-item "><a href="nccl/nccl.html"><strong aria-hidden="true">7.</strong> nccl</a></li><li class="chapter-item "><a href="megatron/megatron.html"><strong aria-hidden="true">8.</strong> megatron</a></li><li class="chapter-item "><a href="deepspeed/deepspeed.html"><strong aria-hidden="true">9.</strong> deepspeed</a></li><li class="chapter-item affix "><li class="part-title">Survey</li><li class="chapter-item "><a href="survey/papers.html"><strong aria-hidden="true">10.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/pollux.html"><strong aria-hidden="true">10.1.</strong> pollux</a></li><li class="chapter-item "><a href="survey/adasum.html"><strong aria-hidden="true">10.2.</strong> adasum</a></li><li class="chapter-item "><a href="survey/adaptation_learning.html"><strong aria-hidden="true">10.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="survey/gradient_descent.html"><strong aria-hidden="true">10.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="survey/auto_parallel.html"><strong aria-hidden="true">10.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="survey/scheduling.html"><strong aria-hidden="true">10.6.</strong> scheduling</a></li><li class="chapter-item "><a href="survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">10.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/gradient_compression/dgc.html"><strong aria-hidden="true">10.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="survey/gradient_compression/csc.html"><strong aria-hidden="true">10.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="survey/flash_attention.html"><strong aria-hidden="true">10.8.</strong> flash attention</a></li><li class="chapter-item "><a href="survey/lora.html"><strong aria-hidden="true">10.9.</strong> LoRA</a></li></ol></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">11.</strong> models</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="llm/llm.html"><strong aria-hidden="true">11.1.</strong> llm</a></li><li class="chapter-item "><a href="llm/falcon.html"><strong aria-hidden="true">11.2.</strong> falcon</a></li><li class="chapter-item "><a href="llm/llama.html"><strong aria-hidden="true">11.3.</strong> llama</a></li><li class="chapter-item "><a href="llm/peft.html"><strong aria-hidden="true">11.4.</strong> peft</a></li><li class="chapter-item "><a href="llm/transformer.html"><strong aria-hidden="true">11.5.</strong> transformer</a></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">11.6.</strong> models</a></li></ol></li><li class="chapter-item "><li class="part-title">Programming</li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">12.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">12.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="python/multiprocessing.html"><strong aria-hidden="true">12.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="python/decorator.html"><strong aria-hidden="true">12.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="golang/index.html"><strong aria-hidden="true">13.</strong> golang</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/golang_error.html"><strong aria-hidden="true">13.1.</strong> golang error</a></li></ol></li><li class="chapter-item "><a href="cplusplus/index.html"><strong aria-hidden="true">14.</strong> cplusplus</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/enable_shared_from_this.html"><strong aria-hidden="true">14.1.</strong> enable_shared_from_this</a></li></ol></li><li class="chapter-item "><li class="part-title">Mathematics</li><li class="chapter-item "><a href="mathematics/topics.html"><strong aria-hidden="true">15.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="mathematics/basic.html"><strong aria-hidden="true">15.1.</strong> basic</a></li><li class="chapter-item "><a href="mathematics/entropy.html"><strong aria-hidden="true">15.2.</strong> entropy</a></li><li class="chapter-item "><a href="mathematics/newton.html"><strong aria-hidden="true">15.3.</strong> newton</a></li><li class="chapter-item "><a href="mathematics/regression.html"><strong aria-hidden="true">15.4.</strong> regression</a></li><li class="chapter-item "><a href="mathematics/conjugate_descent.html"><strong aria-hidden="true">15.5.</strong> conjugate descent</a></li><li class="chapter-item "><a href="mathematics/gradient_descent.html"><strong aria-hidden="true">15.6.</strong> gradient descent</a></li><li class="chapter-item "><a href="mathematics/pca.html"><strong aria-hidden="true">15.7.</strong> pca</a></li><li class="chapter-item "><a href="mathematics/support_vector.html"><strong aria-hidden="true">15.8.</strong> support vector</a></li><li class="chapter-item "><a href="mathematics/differentiation.html"><strong aria-hidden="true">15.9.</strong> differentiation</a></li><li class="chapter-item "><a href="mathematics/fourier.html"><strong aria-hidden="true">15.10.</strong> fourier</a></li><li class="chapter-item "><a href="mathematics/kmeans_cos.html"><strong aria-hidden="true">15.11.</strong> kmeans</a></li></ol></li><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">16.</strong> wavelets</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">16.1.</strong> plan</a></li><li class="chapter-item "><a href="wavelets/preliminary.html"><strong aria-hidden="true">16.2.</strong> preliminary</a></li><li class="chapter-item "><a href="wavelets/haar.html"><strong aria-hidden="true">16.3.</strong> haar wavelet</a></li><li class="chapter-item "><a href="wavelets/fourier.html"><strong aria-hidden="true">16.4.</strong> fourier analysis</a></li><li class="chapter-item "><a href="wavelets/uncertainty_principle.html"><strong aria-hidden="true">16.5.</strong> uncertainty principle</a></li><li class="chapter-item "><a href="wavelets/multiresolution.html"><strong aria-hidden="true">16.6.</strong> multiresolution</a></li></ol></li><li class="chapter-item "><li class="part-title">Learning Deep</li><li class="chapter-item "><a href="kubernetes/kubernetes.html"><strong aria-hidden="true">17.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kubernetes/concepts.html"><strong aria-hidden="true">17.1.</strong> concepts</a></li><li class="chapter-item "><a href="kubernetes/scheduler.html"><strong aria-hidden="true">17.2.</strong> scheduler</a></li><li class="chapter-item "><a href="kubernetes/operator.html"><strong aria-hidden="true">17.3.</strong> operator</a></li><li class="chapter-item "><a href="kubernetes/device_plugin.html"><strong aria-hidden="true">17.4.</strong> device plugin</a></li><li class="chapter-item "><a href="kubernetes/docker.html"><strong aria-hidden="true">17.5.</strong> docker</a></li><li class="chapter-item "><a href="kubernetes/install.html"><strong aria-hidden="true">17.6.</strong> install</a></li><li class="chapter-item "><a href="kubernetes/api_service.html"><strong aria-hidden="true">17.7.</strong> api-service</a></li><li class="chapter-item "><a href="kubernetes/controller.html"><strong aria-hidden="true">17.8.</strong> controller</a></li></ol></li><li class="chapter-item "><a href="nvidia/nvidia.html"><strong aria-hidden="true">18.</strong> cuda</a></li><li class="chapter-item "><a href="somewhat/todo.html"><strong aria-hidden="true">19.</strong> todo</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="somewhat/gloo.html"><strong aria-hidden="true">19.1.</strong> gloo</a></li><li class="chapter-item "><a href="somewhat/mpi.html"><strong aria-hidden="true">19.2.</strong> mpi</a></li><li class="chapter-item "><a href="somewhat/jax.html"><strong aria-hidden="true">19.3.</strong> jax</a></li><li class="chapter-item "><a href="somewhat/tvm.html"><strong aria-hidden="true">19.4.</strong> tvm</a></li><li class="chapter-item "><a href="somewhat/github.html"><strong aria-hidden="true">19.5.</strong> llm</a></li></ol></li><li class="chapter-item "><a href="notes/index.html"><strong aria-hidden="true">20.</strong> notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="notes/influence_and_persuasion.html"><strong aria-hidden="true">20.1.</strong> influence and persuasion</a></li><li class="chapter-item "><a href="notes/feynman_technique.html"><strong aria-hidden="true">20.2.</strong> freynman technique</a></li><li class="chapter-item "><a href="notes/wavelet_tour_signal_processing_sparse.html"><strong aria-hidden="true">20.3.</strong> wavelet signal processing</a></li></ol></li><li class="chapter-item "><a href="tips/tips.html"><strong aria-hidden="true">21.</strong> tips</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/ip_local_port_range.html"><strong aria-hidden="true">21.1.</strong> ip_local_port_range</a></li></ol></li><li class="chapter-item "><a href="infra/overview.html"><strong aria-hidden="true">22.</strong> infrastructure</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="infra/pki.html"><strong aria-hidden="true">22.1.</strong> pki</a></li><li class="chapter-item "><a href="infra/cache.html"><strong aria-hidden="true">22.2.</strong> linux cache</a></li></ol></li><li class="chapter-item "><a href="projects/projects.html"><strong aria-hidden="true">23.</strong> projects</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="projects/copilot.html"><strong aria-hidden="true">23.1.</strong> copilot</a></li><li class="chapter-item "><a href="projects/library.html"><strong aria-hidden="true">23.2.</strong> library</a></li><li class="chapter-item "><a href="projects/rag.html"><strong aria-hidden="true">23.3.</strong> RAG</a></li></ol></li><li class="chapter-item "><a href="chronicles/2024mar.html"><strong aria-hidden="true">24.</strong> chronicles</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chronicles/2024feb.html"><strong aria-hidden="true">24.1.</strong> feb 2024</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="transformer"><a class="header" href="#transformer">Transformer</a></h1>
<h2 id="bert"><a class="header" href="#bert">BERT</a></h2>
<h3 id="bertmodel"><a class="header" href="#bertmodel">BertModel</a></h3>
<pre><code class="language-python">class BertModel(BertPreTrainedModel):
    def __init__(self, config, add_pooling_layer=True):
        self.config = config

        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)

        self.pooler = BertPooler(config) if add_pooling_layer else None

    def forward(self, ...):
        embedding_output = self.embeddings(
            input_ids=input_ids,
            position_ids=position_ids,
            token_type_ids=token_type_ids,
            inputs_embeds=inputs_embeds,
            past_key_values_length=past_key_values_length,
        )
        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None

        return BaseModelOutputWithPoolingAndCrossAttentions(
            last_hidden_state=sequence_output,
            pooler_output=pooled_output,
            past_key_values=encoder_outputs.past_key_values,
            hidden_states=encoder_outputs.hidden_states,
            attentions=encoder_outputs.attentions,
            cross_attentions=encoder_outputs.cross_attentions,
        )
</code></pre>
<h3 id="bertembeddings"><a class="header" href="#bertembeddings">BertEmbeddings</a></h3>
<pre><code class="language-python"># src/transformers/models/bert/modeling_bert.py

class BertEmbeddings(nn.Module):

    def __init__(self, config):
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)

        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, ...):
        inputs_embeds = self.word_embeddings(input_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        position_embeddings = self.position_embeddings(position_ids) # absolute
        embeddings = inputs_embeds + token_type_embeddings + position_embeddings
        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
</code></pre>
<h3 id="bertencoder"><a class="header" href="#bertencoder">BertEncoder</a></h3>
<pre><code class="language-python">
class BertEncoder(nn.Module):
    def __init__(self, config):
        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(self, ...):
        for i, layer_module in enumerate(self.layer):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_head_mask = head_mask[i] if head_mask is not None else None
            past_key_value = past_key_values[i] if past_key_values is not None else None

            if self.gradient_checkpointing and self.training:

                if use_cache:
                    logger.warning(
                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                    )
                    use_cache = False

                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        return module(*inputs, past_key_value, output_attentions)

                    return custom_forward

                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(layer_module),
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            else:
                layer_outputs = layer_module(
                    hidden_states,
                    attention_mask,
                    layer_head_mask,
                    encoder_hidden_states,
                    encoder_attention_mask,
                    past_key_value,
                    output_attentions,
                )

            hidden_states = layer_outputs[0]
            if use_cache:
                next_decoder_cache += (layer_outputs[-1],)
            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)
                if self.config.add_cross_attention:
                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(
                v
                for v in [
                    hidden_states,
                    next_decoder_cache,
                    all_hidden_states,
                    all_self_attentions,
                    all_cross_attentions,
                ]
                if v is not None
            )
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=next_decoder_cache,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )


class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output



```python
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; model.config
BertConfig {
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.22.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; model.eval()
BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (1): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      ...
      (11): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
</code></pre>
<h2 id="t5"><a class="header" href="#t5">T5</a></h2>
<pre><code class="language-python">from transformers import T5Tokenizer, T5Model
model = T5Model.from_pretrained("t5-small")
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; model.config
T5Config {
  "_name_or_path": "t5-small",
  "architectures": [
    "T5WithLMHeadModel"
  ],
  "d_ff": 2048,
  "d_kv": 64,
  "d_model": 512,
  "decoder_start_token_id": 0,
  "dense_act_fn": "relu",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "relu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": false,
  "layer_norm_epsilon": 1e-06,
  "model_type": "t5",
  "n_positions": 512,
  "num_decoder_layers": 6,
  "num_heads": 8,
  "num_layers": 6,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "task_specific_params": {
    "summarization": {
      "early_stopping": true,
      "length_penalty": 2.0,
      "max_length": 200,
      "min_length": 30,
      "no_repeat_ngram_size": 3,
      "num_beams": 4,
      "prefix": "summarize: "
    },
    "translation_en_to_de": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to German: "
    },
    "translation_en_to_fr": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to French: "
    },
    "translation_en_to_ro": {
      "early_stopping": true,
      "max_length": 300,
      "num_beams": 4,
      "prefix": "translate English to Romanian: "
    }
  },
  "transformers_version": "4.22.2",
  "use_cache": true,
  "vocab_size": 32128
}
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; model.eval()
T5Model(
  (shared): Embedding(32128, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      ...
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
)
</code></pre>
<pre><code class="language-python">model = T5ForConditionalGeneration.from_pretrained("t5-small")
&gt;&gt;&gt; model.config
# same as T5Model
&gt;&gt;&gt; model.eval()
T5ForConditionalGeneration(
  ...
  (lm_head): Linear(in_features=512, out_features=32128, bias=False)
)
</code></pre>
<h2 id="gpt"><a class="header" href="#gpt">GPT</a></h2>
<pre><code class="language-python">model = OpenAIGPTModel.from_pretrained("openai-gpt")
&gt;&gt;&gt; model.config
OpenAIGPTConfig {
  "_name_or_path": "openai-gpt",
  "afn": "gelu",
  "architectures": [
    "OpenAIGPTLMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "openai-gpt",
  "n_ctx": 512,
  "n_embd": 768,
  "n_head": 12,
  "n_layer": 12,
  "n_positions": 512,
  "n_special": 0,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.22.2",
  "vocab_size": 40478
}
</code></pre>
<pre><code>OpenAIGPTModel(
  (tokens_embed): Embedding(40478, 768)
  (positions_embed): Embedding(512, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0): Block(
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    ...
    (11): Block(
      (attn): Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
)
</code></pre>
<h2 id="fastertransformer"><a class="header" href="#fastertransformer">FasterTransformer</a></h2>
<p>NVIDIA Transformer 推理前向优化方案。</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://huggingface.co/docs/transformers/model_doc/bert">huggingface bert doc</a></li>
<li><a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a></li>
<li><a href="https://github.com/microsoft/Swin-Transformer">Swin Transformer github</a></li>
<li><a href="https://arxiv.org/pdf/2103.14030.pdf">Swin Transformer arxiv</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../llm/peft.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../llm/models.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../llm/peft.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../llm/models.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
