<!DOCTYPE HTML>
<html lang="en" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>architecture - Aller au boulot</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="index.html"><strong aria-hidden="true">1.</strong> welcome</a></li><li class="chapter-item "><a href="vllm/overview.html"><strong aria-hidden="true">2.</strong> vllm</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="vllm/architecture.html"><strong aria-hidden="true">2.1.</strong> architecture</a></li></ol></li><li class="chapter-item "><a href="chronicles/2024mar.html"><strong aria-hidden="true">3.</strong> chronicles</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chronicles/2024feb.html"><strong aria-hidden="true">3.1.</strong> feb 2024</a></li></ol></li><li class="chapter-item "><a href="projects/projects.html"><strong aria-hidden="true">4.</strong> projects</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="projects/copilot.html"><strong aria-hidden="true">4.1.</strong> copilot</a></li><li class="chapter-item "><a href="projects/library.html"><strong aria-hidden="true">4.2.</strong> library</a></li><li class="chapter-item "><a href="projects/rag.html"><strong aria-hidden="true">4.3.</strong> RAG</a></li></ol></li><li class="chapter-item "><a href="survey/papers.html"><strong aria-hidden="true">5.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/pollux.html"><strong aria-hidden="true">5.1.</strong> pollux</a></li><li class="chapter-item "><a href="survey/adasum.html"><strong aria-hidden="true">5.2.</strong> adasum</a></li><li class="chapter-item "><a href="survey/adaptation_learning.html"><strong aria-hidden="true">5.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="survey/gradient_descent.html"><strong aria-hidden="true">5.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="survey/auto_parallel.html"><strong aria-hidden="true">5.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="survey/scheduling.html"><strong aria-hidden="true">5.6.</strong> scheduling</a></li><li class="chapter-item "><a href="survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">5.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/gradient_compression/dgc.html"><strong aria-hidden="true">5.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="survey/gradient_compression/csc.html"><strong aria-hidden="true">5.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="survey/flash_attention.html"><strong aria-hidden="true">5.8.</strong> flash attention</a></li><li class="chapter-item "><a href="survey/lora.html"><strong aria-hidden="true">5.9.</strong> LoRA</a></li></ol></li><li class="chapter-item "><a href="mathematics/topics.html"><strong aria-hidden="true">6.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="mathematics/basic.html"><strong aria-hidden="true">6.1.</strong> basic</a></li><li class="chapter-item "><a href="mathematics/entropy.html"><strong aria-hidden="true">6.2.</strong> entropy</a></li><li class="chapter-item "><a href="mathematics/newton.html"><strong aria-hidden="true">6.3.</strong> newton</a></li><li class="chapter-item "><a href="mathematics/regression.html"><strong aria-hidden="true">6.4.</strong> regression</a></li><li class="chapter-item "><a href="mathematics/conjugate_descent.html"><strong aria-hidden="true">6.5.</strong> conjugate descent</a></li><li class="chapter-item "><a href="mathematics/gradient_descent.html"><strong aria-hidden="true">6.6.</strong> gradient descent</a></li><li class="chapter-item "><a href="mathematics/pca.html"><strong aria-hidden="true">6.7.</strong> pca</a></li><li class="chapter-item "><a href="mathematics/support_vector.html"><strong aria-hidden="true">6.8.</strong> support vector</a></li><li class="chapter-item "><a href="mathematics/differentiation.html"><strong aria-hidden="true">6.9.</strong> differentiation</a></li><li class="chapter-item "><a href="mathematics/fourier.html"><strong aria-hidden="true">6.10.</strong> fourier</a></li><li class="chapter-item "><a href="mathematics/kmeans_cos.html"><strong aria-hidden="true">6.11.</strong> kmeans</a></li></ol></li><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">7.</strong> wavelets</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">7.1.</strong> plan</a></li><li class="chapter-item "><a href="wavelets/preliminary.html"><strong aria-hidden="true">7.2.</strong> preliminary</a></li><li class="chapter-item "><a href="wavelets/haar.html"><strong aria-hidden="true">7.3.</strong> haar wavelet</a></li><li class="chapter-item "><a href="wavelets/fourier.html"><strong aria-hidden="true">7.4.</strong> fourier analysis</a></li><li class="chapter-item "><a href="wavelets/uncertainty_principle.html"><strong aria-hidden="true">7.5.</strong> uncertainty principle</a></li><li class="chapter-item "><a href="wavelets/multiresolution.html"><strong aria-hidden="true">7.6.</strong> multiresolution</a></li></ol></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">8.</strong> models</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="llm/llm.html"><strong aria-hidden="true">8.1.</strong> llm</a></li><li class="chapter-item "><a href="llm/falcon.html"><strong aria-hidden="true">8.2.</strong> falcon</a></li><li class="chapter-item "><a href="llm/llama.html"><strong aria-hidden="true">8.3.</strong> llama</a></li><li class="chapter-item "><a href="llm/peft.html"><strong aria-hidden="true">8.4.</strong> peft</a></li><li class="chapter-item "><a href="llm/transformer.html"><strong aria-hidden="true">8.5.</strong> transformer</a></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">8.6.</strong> models</a></li></ol></li><li class="chapter-item "><a href="megatron/megatron.html"><strong aria-hidden="true">9.</strong> megatron</a></li><li class="chapter-item "><a href="deepspeed/deepspeed.html"><strong aria-hidden="true">10.</strong> deepspeed</a></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">11.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">11.1.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/autograd.html"><strong aria-hidden="true">11.2.</strong> autograd</a></li><li class="chapter-item "><a href="pytorch/operator.html"><strong aria-hidden="true">11.3.</strong> operator</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">11.4.</strong> profiler</a></li><li class="chapter-item "><a href="pytorch/hook.html"><strong aria-hidden="true">11.5.</strong> hook</a></li><li class="chapter-item "><a href="pytorch/elastic.html"><strong aria-hidden="true">11.6.</strong> elastic</a></li><li class="chapter-item "><a href="pytorch/patch.html"><strong aria-hidden="true">11.7.</strong> patch</a></li><li class="chapter-item "><a href="pytorch/misc.html"><strong aria-hidden="true">11.8.</strong> misc</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">12.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps-code-overview.html"><strong aria-hidden="true">12.1.</strong> ps</a></li><li class="chapter-item "><a href="paddle/framework.html"><strong aria-hidden="true">12.2.</strong> framework</a></li><li class="chapter-item "><a href="paddle/cinn.html"><strong aria-hidden="true">12.3.</strong> cinn</a></li><li class="chapter-item "><a href="paddle/dataloader.html"><strong aria-hidden="true">12.4.</strong> dataloader</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">13.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">13.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/workflow.html"><strong aria-hidden="true">13.2.</strong> workflow</a></li><li class="chapter-item "><a href="horovod/object.html"><strong aria-hidden="true">13.3.</strong> object</a></li><li class="chapter-item "><a href="horovod/develop.html"><strong aria-hidden="true">13.4.</strong> develop</a></li><li class="chapter-item "><a href="horovod/pytorch.html"><strong aria-hidden="true">13.5.</strong> pytorch</a></li><li class="chapter-item "><a href="horovod/tensorflow.html"><strong aria-hidden="true">13.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="horovod/elastic.html"><strong aria-hidden="true">13.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">14.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">14.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">14.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">14.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">14.4.</strong> api</a></li><li class="chapter-item "><a href="ray/survey.html"><strong aria-hidden="true">14.5.</strong> survey</a></li></ol></li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">15.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">15.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="python/multiprocessing.html"><strong aria-hidden="true">15.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="python/decorator.html"><strong aria-hidden="true">15.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="tips/tips.html"><strong aria-hidden="true">16.</strong> tips</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/enable_shared_from_this.html"><strong aria-hidden="true">16.1.</strong> enable_shared_from_this</a></li><li class="chapter-item "><a href="tips/ip_local_port_range.html"><strong aria-hidden="true">16.2.</strong> ip_local_port_range</a></li><li class="chapter-item "><a href="tips/golang_error.html"><strong aria-hidden="true">16.3.</strong> golang error</a></li></ol></li><li class="chapter-item "><a href="infra/overview.html"><strong aria-hidden="true">17.</strong> infrastructure</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="infra/pki.html"><strong aria-hidden="true">17.1.</strong> pki</a></li><li class="chapter-item "><a href="infra/cache.html"><strong aria-hidden="true">17.2.</strong> linux cache</a></li></ol></li><li class="chapter-item "><a href="kubernetes/kubernetes.html"><strong aria-hidden="true">18.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kubernetes/concepts.html"><strong aria-hidden="true">18.1.</strong> concepts</a></li><li class="chapter-item "><a href="kubernetes/scheduler.html"><strong aria-hidden="true">18.2.</strong> scheduler</a></li><li class="chapter-item "><a href="kubernetes/operator.html"><strong aria-hidden="true">18.3.</strong> operator</a></li><li class="chapter-item "><a href="kubernetes/device_plugin.html"><strong aria-hidden="true">18.4.</strong> device plugin</a></li><li class="chapter-item "><a href="kubernetes/docker.html"><strong aria-hidden="true">18.5.</strong> docker</a></li><li class="chapter-item "><a href="kubernetes/install.html"><strong aria-hidden="true">18.6.</strong> install</a></li><li class="chapter-item "><a href="kubernetes/api_service.html"><strong aria-hidden="true">18.7.</strong> api-service</a></li><li class="chapter-item "><a href="kubernetes/controller.html"><strong aria-hidden="true">18.8.</strong> controller</a></li></ol></li><li class="chapter-item "><a href="nccl/nccl.html"><strong aria-hidden="true">19.</strong> nccl</a></li><li class="chapter-item "><a href="nvidia/nvidia.html"><strong aria-hidden="true">20.</strong> cuda</a></li><li class="chapter-item "><a href="somewhat/todo.html"><strong aria-hidden="true">21.</strong> todo</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="somewhat/gloo.html"><strong aria-hidden="true">21.1.</strong> gloo</a></li><li class="chapter-item "><a href="somewhat/mpi.html"><strong aria-hidden="true">21.2.</strong> mpi</a></li><li class="chapter-item "><a href="somewhat/jax.html"><strong aria-hidden="true">21.3.</strong> jax</a></li><li class="chapter-item "><a href="somewhat/tvm.html"><strong aria-hidden="true">21.4.</strong> tvm</a></li><li class="chapter-item "><a href="somewhat/github.html"><strong aria-hidden="true">21.5.</strong> llm</a></li></ol></li><li class="chapter-item "><a href="notes/index.html"><strong aria-hidden="true">22.</strong> notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="notes/influence_and_persuasion.html"><strong aria-hidden="true">22.1.</strong> influence and persuasion</a></li><li class="chapter-item "><a href="notes/feynman_technique.html"><strong aria-hidden="true">22.2.</strong> freynman technique</a></li><li class="chapter-item "><a href="notes/wavelet_tour_signal_processing_sparse.html"><strong aria-hidden="true">22.3.</strong> wavelet signal processing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="architecture"><a class="header" href="#architecture">architecture</a></h1>
<h2 id="online-serving"><a class="header" href="#online-serving">online serving</a></h2>
<p>服务启动命令</p>
<pre><code class="language-bash">vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
</code></pre>
<p>命令在 <code>pyproject.toml</code> 中定义</p>
<pre><code class="language-toml">[project.scripts]
vllm = "vllm.entrypoints.cli.main:main"
</code></pre>
<p>入口
<code>vllm/entrypoints/cli/main.py</code>
dispatch 到真实启动的入口
<code>vllm.entrypoints.cli.serve</code>.</p>
<pre><code class="language-python"># vllm/entrypoints/cli/serve.py

class ServeSubcommand(CLISubcommand):
    def __init__(self):
        self.name = "serve"

    @staticmethod
    def cmd(args: argparse.Namespace) -&gt; None:
        args.model = args.model_tag
        uvloop.run(run_server(args))

    def subparser_init(self, subparsers):
        serve_parser = subparsers.add_parser("serve", usage="vllm serve &lt;model_tag&gt; [options]")
        serve_parser.add_argument("model_tag", ...)
        serve_parser.add_argument("--config", ...) # YAML config file
        return make_arg_parser(serve_parser)
</code></pre>
<p>可以看到，<code>serve</code> 命令的入口是 <code>vllm.entrypoints.cli.serve.ServeSubcommand.cmd</code>，它调用 <code>vllm.entrypoints.cli.serve.run_server</code>，而 <code>run_server</code> 会创建 <code>uvicorn</code> 服务。</p>
<pre><code class="language-python"># vllm/entrypoints/openai/api_server.py

@asynccontextmanager
async def build_async_engine_client(args) -&gt; AsyncIterator[EngineClient]:
    engine_args = AsyncEngineArgs.from_cli_args(args)
    engine_client = AsyncLLMEngine.from_engine_args(engine_args, ...)
    yield engine_client

def build_app(args: Namespace) -&gt; FastAPI:
    app = FastAPI(lifespan=lifespan)
    app.include_router(router)
    return app

async def run_server(args, **uvicorn_kwargs) -&gt; None:
    async with build_async_engine_client(args) as engine_client:
        app = build_app(args)
        model_config = await engine_client.get_model_config()
        await init_app_state(engine_client, model_config, app.state, args)
        await serve_http(app, ...  **uvicorn_kwargs)


if __name__ == "__main__":
    parser = FlexibleArgumentParser(...)
    args = parser.parse_args()
    uvloop.run(run_server(args))
</code></pre>
<p><code>run_server</code> 是个异步函数，它完成了 2 个主要任务：</p>
<ul>
<li>创建异步 engine : <code>AsyncLLMEngine</code></li>
<li>创建 FastAPI 应用，承接 http 请求</li>
</ul>
<p>这里通过 <code>init_app_state</code> 初始化了 app.state，即把 vllm LLMEngine 设置给了 FastAPI。</p>
<p>其中 <code>serve_http</code> 通过 uvicorn 实现了 http 服务的启动。</p>
<pre><code class="language-python"># vllm/entrypoints/launcher.py

async def serve_http(app: FastAPI, sock: Optional[socket.socket], **uvicorn_kwargs: Any):
    config = uvicorn.Config(app, **uvicorn_kwargs)
    server = uvicorn.Server(config)
    loop = asyncio.get_running_loop()
    server_task = loop.create_task(
        server.serve(sockets=[sock] if sock else None))
</code></pre>
<p>同时可以看到 <code>vllm serve</code> 的等价启动方式是</p>
<pre><code class="language-bash">python3 -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
</code></pre>
<h2 id="offline-inference"><a class="header" href="#offline-inference">offline inference</a></h2>
<pre><code class="language-python">from vllm import LLM

prompts = "Hello, my name is"

llm = LLM(model="facebook/opt-125m")

outputs = llm.generate(prompts)
</code></pre>
<pre><code class="language-python"># vllm/__init__.py

from vllm.entrypoints.llm import LLM
</code></pre>
<p>LLM</p>
<pre><code class="language-python"># vllm/entrypoints/llm.py

class LLM:
    def __init__(self, model, ...) -&gt; None:
        worker_cls = kwargs["worker_cls"]
        engine_args = EngineArgs(model, task, tokenizer)
        self.engine_class = self.get_engine_class()
        self.llm_engine = self.engine_class.from_engine_args(
            engine_args, usage_context=UsageContext.LLM_CLASS)

    @staticmethod
    def get_engine_class() -&gt; Type[LLMEngine]:
        if envs.VLLM_USE_V1:
            from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine
            return V1LLMEngine
        return LLMEngine

    def get_tokenizer(self) -&gt; AnyTokenizer:
        return self.llm_engine.get_tokenizer_group(TokenizerGroup).tokenizer

    @overload
    def generate(self, prompts, sampling_params):
        outputs = self._run_engine(use_tqdm=use_tqdm)
        return self.engine_class.validate_outputs(outputs, RequestOutput)

    def collective_rpc(self, ...):
        executor = self.llm_engine.model_executor
        return executor.collective_rpc(method, timeout, args, kwargs)

    def apply_model(self, func: Callable[[nn.Module], _R]) -&gt; list[_R]:
        executor = self.llm_engine.model_executor
        return executor.apply_model(func)
</code></pre>
<h2 id="engine"><a class="header" href="#engine">Engine</a></h2>
<h2 id="asyncllmengine"><a class="header" href="#asyncllmengine">AsyncLLMEngine</a></h2>
<pre><code class="language-python"># vllm/engine/async_llm_engine.py

if envs.VLLM_USE_V1:
    from vllm.v1.engine.async_llm import AsyncLLM
    AsyncLLMEngine = AsyncLLM
</code></pre>
<pre><code class="language-python"># vllm/v1/engine/async_llm.py

class AsyncLLM(EngineClient):

    def __init__(self, vllm_config, executor_class):
        # Tokenizer (+ ensure liveness if running in another process).
        self.tokenizer = init_tokenizer_from_configs(...)
        self.tokenizer.ping()

        # Processor (converts Inputs --&gt; EngineCoreRequests).
        self.processor = Processor(..., tokenizer...)

        # OutputProcessor (converts EngineCoreOutputs --&gt; RequestOutput).
        self.output_processor = OutputProcessor(self.tokenizer, ...)

        # EngineCore (starts the engine in background process).
        self.engine_core = EngineCoreClient.make_client(...)

        self.output_handler: Optional[asyncio.Task] = None

    @classmethod
    def from_engine_args(cls, ...) -&gt; "AsyncLLM":
        executor_class = Executor.get_class(vllm_config)
        return cls(...)

    async def add_request(self, request_id, prompt, params, ...) -&gt; asyncio.Queue[RequestOutput]:
        # 1) Create a new output queue for the request.
        queue: asyncio.Queue[RequestOutput] = asyncio.Queue()

        # 2) Convert Input --&gt; Request.
        request = self.processor.process_inputs(request_id, prompt, params, ...)

        # 3) Add the request to OutputProcessor (this process).
        self.output_processor.add_request(request, queue)

        # 4) Add the EngineCoreRequest to EngineCore (separate process).
        await self.engine_core.add_request_async(request)

        return queue

    async def generate(self, prompt, sampling_params, ...) -&gt; AsyncGenerator[RequestOutput, None]:
        """
        Main function called by the API server to kick off a request
            * 1) Making an AsyncStream corresponding to the Request.
            * 2) Processing the Input.
            * 3) Adding the Request to the Detokenizer.
            * 4) Adding the Request to the EngineCore (separate process).

        A separate output_handler loop runs in a background AsyncIO task, 
        pulling outputs from EngineCore and putting them into the 
        per-request AsyncStream.

        The caller of generate() iterates the returned AsyncGenerator,
        returning the RequestOutput back to the caller.
        """

        try:
            # We start the output_handler on the first call to generate() so
            # we can call __init__ before the event loop, which enables us
            # to handle startup failure gracefully in the OpenAI server.
            self.output_handler = asyncio.create_task(self._run_output_handler())

            q = await self.add_request(request_id, prompt, sampling_params, ...)

            # The output_handler task pushes items into the queue.
            # This task pulls from the queue and yields to caller.
            finished = False
            while not finished:
                out = q.get_nowait() if not q.empty() else await q.get()

                while not q.empty():
                    next_out = q.get_nowait()
                    if sampling_params.output_kind == RequestOutputKind.DELTA:
                        out.add(next_out)
                    else:
                        out = next_out

                finished = out.finished
                yield out

    async def _run_output_handler(self):
        """Background loop: pulls from EngineCore and pushes to AsyncStreams."""

        try:
            while True:
                # 1) Pull EngineCoreOutputs from the EngineCore.
                outputs = await self.engine_core.get_output_async()

                iteration_stats = IterationStats() if self.log_stats else None

                # Split outputs into chunks of at most
                # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the
                # event loop for too long.
                num_outputs = len(outputs.outputs)
                if num_outputs &lt;= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:
                    slices = (outputs.outputs, )
                else:
                    slices = np.array_split(
                        outputs.outputs,
                        cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))

                for i, outputs_slice in enumerate(slices):
                    # 2) Process EngineCoreOutputs.
                    processed_outputs = self.output_processor.process_outputs(
                        outputs_slice, outputs.timestamp, iteration_stats)
                    # NOTE: RequestOutputs are pushed to their queues.
                    assert not processed_outputs.request_outputs

                    # Allow other asyncio tasks to run between chunks
                    if i + 1 &lt; len(slices):
                        await asyncio.sleep(0)

                    # 3) Abort any reqs that finished due to stop strings.
                    await self.engine_core.abort_requests_async(
                        processed_outputs.reqs_to_abort)

                # 4) Logging.
                # TODO(rob): make into a coroutine and launch it in
                # background thread once Prometheus overhead is non-trivial.
                self._log_stats(
                    scheduler_stats=outputs.scheduler_stats,
                    iteration_stats=iteration_stats,
                )
</code></pre>
<h2 id="llmengine"><a class="header" href="#llmengine">LLMEngine</a></h2>
<h2 id="entrypoint"><a class="header" href="#entrypoint">Entrypoint</a></h2>
<h2 id="excutor"><a class="header" href="#excutor">Excutor</a></h2>
<p>Executor</p>
<pre><code class="language-python"># vllm/v1/executor/abstract.py

class Executor(ExecutorBase):
    @staticmethod
    def get_class(vllm_config: VllmConfig) -&gt; Type["Executor"]:
        executor_class = RayDistributedExecutor
        executor_class = MultiprocExecutor
        executor_class = UniProcExecutor
        executor_class = ExecutorWithExternalLauncher
        return executor_class

    def initialize(self, kv_cache_configs: List[KVCacheConfig]) -&gt; None:
        self.collective_rpc("initialize_cache", args=(kv_cache_configs, ))
        self.collective_rpc("compile_or_warm_up_model")

    def execute_model(
        self,
        scheduler_output,
    ) -&gt; Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
        output = self.collective_rpc("execute_model", args=(scheduler_output, ))
        return output[0]

</code></pre>
<pre><code class="language-python"># vllm/executor/executor_base.py
class ExecutorBase(ABC):
    def __init__(self, vllm_config: VllmConfig) -&gt; None:
        self._init_executor()
</code></pre>
<p>MultiprocExecutor</p>
<pre><code class="language-python"># vllm/v1/executor/multiproc_executor.py

class MultiprocExecutor(Executor):
	def _init_executor(self) -&gt; None:
        self.rpc_broadcast_mq = MessageQueue(self.world_size, self.world_size)
        scheduler_output_handle = self.rpc_broadcast_mq.export_handle()

        self.workers: List[WorkerProcHandle] = []
        for rank in range(self.world_size):
            worker = WorkerProc.make_worker_process(self.vllm_config, ...)
            self.workers.append(worker)

class WorkerProc:
    def __init__(self, vllm_config, local_rank, rank, ...):
		wrapper = WorkerWrapperBase(vllm_config=vllm_config, ...)
		wrapper.init_worker(all_kwargs)
		self.worker = wrapper.worker

		self.rpc_broadcast_mq = MessageQueue.create_from_handle(...)
		self.worker_response_mq = MessageQueue(1, 1)

		self.worker.init_device()
		self.worker.load_model()

    @staticmethod
    def make_worker_process(vllm_config, ...) -&gt; WorkerProcHandle:
        proc = context.Process(target=WorkerProc.worker_main, ..., daemon=True)
        proc.start()
        return WorkerProcHandle(proc, rank, ready_path, worker_response_mq)

    @staticmethod
    def worker_main(*args, **kwargs):
        try:
            worker = WorkerProc(*args, **kwargs)
            worker.worker_busy_loop()

    def worker_busy_loop(self):
        while True:
            method, args, kwargs = self.rpc_broadcast_mq.dequeue()
            func = getattr(self.worker, method)
            output = func(*args, **kwargs)
            self.worker_response_mq.enqueue((SUCCESS, output))
</code></pre>
<p>RayDistributedExecutor</p>
<pre><code class="language-python"># vllm/v1/executor/ray_distributed_executor.py

class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
    def execute_model(
        self,
        scheduler_output,
    ) -&gt; Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
        self.forward_dag = self._compiled_ray_dag(enable_asyncio=False)
        refs = self.forward_dag.execute(scheduler_output)
        return refs[0].get()
</code></pre>
<p>UniProcExecutor
ExecutorWithExternalLauncher</p>
<pre><code class="language-python"># vllm/executor/uniproc_executor.py

class UniProcExecutor(ExecutorBase):
    def _init_executor(self) -&gt; None:
        self.driver_worker = WorkerWrapperBase(vllm_config=self.vllm_config, rpc_rank=0)
        self.collective_rpc("init_worker", args=([kwargs], ))
        self.collective_rpc("init_device")
        self.collective_rpc("load_model")

    def collective_rpc(self,
                       method: Union[str, Callable],
                       timeout: Optional[float] = None,
                       args: Tuple = (),
                       kwargs: Optional[Dict] = None) -&gt; List[Any]:
        answer = run_method(self.driver_worker, method, args, kwargs)
        return [answer]

class ExecutorWithExternalLauncher(UniProcExecutor):
</code></pre>
<p>WorkerWrapperBase</p>
<pre><code>"""
This class represents one process in an executor/engine. It is responsible
for lazily initializing the worker and handling the worker's lifecycle.
We first instantiate the WorkerWrapper, which remembers the worker module
and class name. Then, when we call `update_environment_variables`, and the
real initialization happens in `init_worker`.
"""

    """
    Initialize the worker wrapper with the given vllm_config and rpc_rank.
    Note: rpc_rank is the rank of the worker in the executor. In most cases,
    it is also the rank of the worker in the distributed group. However,
    when multiple executors work together, they can be different.
    e.g. in the case of SPMD-style offline inference with TP=2,
    users can launch 2 engines/executors, each with only 1 worker.
    All workers have rpc_rank=0, but they have different ranks in the TP
    group.
    """
</code></pre>
<pre><code class="language-python"># vllm/worker/worker_base.py

class WorkerWrapperBase:
    def __init__(self, vllm_config, rpc_rank) -&gt; None:
        self.worker: Optional[WorkerBase] = None
        init_cached_hf_modules()

    def init_worker(self, all_kwargs: List[Dict[str, Any]]) -&gt; None:
        worker_class = resolve_obj_by_qualname(self.vllm_config.parallel_config.worker_cls)
        worker_class = cloudpickle.loads(self.vllm_config.parallel_config.worker_cls)
        self.worker = worker_class(**kwargs)

    def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        return run_method(target, method, args, kwargs)
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../vllm/overview.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../chronicles/2024mar.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../vllm/overview.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../chronicles/2024mar.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
