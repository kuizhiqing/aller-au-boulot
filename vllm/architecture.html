<!DOCTYPE HTML>
<html lang="en" class="latte" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>architecture - Aller au boulot</title>


        <!-- Custom HTML head -->

        <meta name="description" content="Projects excelling">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href=".././theme/catppuccin.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "latte";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('latte')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="index.html"><strong aria-hidden="true">1.</strong> welcome</a></li><li class="chapter-item "><a href="vllm/overview.html"><strong aria-hidden="true">2.</strong> vllm</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="vllm/architecture.html"><strong aria-hidden="true">2.1.</strong> architecture</a></li></ol></li><li class="chapter-item "><a href="somewhat/llama.html"><strong aria-hidden="true">3.</strong> llama</a></li><li class="chapter-item "><a href="chronicles/2024mar.html"><strong aria-hidden="true">4.</strong> chronicles</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="chronicles/2024feb.html"><strong aria-hidden="true">4.1.</strong> feb 2024</a></li></ol></li><li class="chapter-item "><a href="projects/projects.html"><strong aria-hidden="true">5.</strong> projects</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="projects/copilot.html"><strong aria-hidden="true">5.1.</strong> copilot</a></li><li class="chapter-item "><a href="projects/library.html"><strong aria-hidden="true">5.2.</strong> library</a></li><li class="chapter-item "><a href="projects/rag.html"><strong aria-hidden="true">5.3.</strong> RAG</a></li></ol></li><li class="chapter-item "><a href="survey/papers.html"><strong aria-hidden="true">6.</strong> survey</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/pollux.html"><strong aria-hidden="true">6.1.</strong> pollux</a></li><li class="chapter-item "><a href="survey/adasum.html"><strong aria-hidden="true">6.2.</strong> adasum</a></li><li class="chapter-item "><a href="survey/adaptation_learning.html"><strong aria-hidden="true">6.3.</strong> adaptation_learning</a></li><li class="chapter-item "><a href="survey/gradient_descent.html"><strong aria-hidden="true">6.4.</strong> gradient_descent</a></li><li class="chapter-item "><a href="survey/auto_parallel.html"><strong aria-hidden="true">6.5.</strong> auto_parallel</a></li><li class="chapter-item "><a href="survey/scheduling.html"><strong aria-hidden="true">6.6.</strong> scheduling</a></li><li class="chapter-item "><a href="survey/gradient_compression/gradient_compression.html"><strong aria-hidden="true">6.7.</strong> gradient_compression</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="survey/gradient_compression/dgc.html"><strong aria-hidden="true">6.7.1.</strong> dgc</a></li><li class="chapter-item "><a href="survey/gradient_compression/csc.html"><strong aria-hidden="true">6.7.2.</strong> csc</a></li></ol></li><li class="chapter-item "><a href="survey/flash_attention.html"><strong aria-hidden="true">6.8.</strong> flash attention</a></li><li class="chapter-item "><a href="survey/lora.html"><strong aria-hidden="true">6.9.</strong> LoRA</a></li></ol></li><li class="chapter-item "><a href="mathematics/topics.html"><strong aria-hidden="true">7.</strong> mathematics</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="mathematics/basic.html"><strong aria-hidden="true">7.1.</strong> basic</a></li><li class="chapter-item "><a href="mathematics/entropy.html"><strong aria-hidden="true">7.2.</strong> entropy</a></li><li class="chapter-item "><a href="mathematics/newton.html"><strong aria-hidden="true">7.3.</strong> newton</a></li><li class="chapter-item "><a href="mathematics/regression.html"><strong aria-hidden="true">7.4.</strong> regression</a></li><li class="chapter-item "><a href="mathematics/conjugate_descent.html"><strong aria-hidden="true">7.5.</strong> conjugate descent</a></li><li class="chapter-item "><a href="mathematics/gradient_descent.html"><strong aria-hidden="true">7.6.</strong> gradient descent</a></li><li class="chapter-item "><a href="mathematics/pca.html"><strong aria-hidden="true">7.7.</strong> pca</a></li><li class="chapter-item "><a href="mathematics/support_vector.html"><strong aria-hidden="true">7.8.</strong> support vector</a></li><li class="chapter-item "><a href="mathematics/differentiation.html"><strong aria-hidden="true">7.9.</strong> differentiation</a></li><li class="chapter-item "><a href="mathematics/fourier.html"><strong aria-hidden="true">7.10.</strong> fourier</a></li><li class="chapter-item "><a href="mathematics/kmeans_cos.html"><strong aria-hidden="true">7.11.</strong> kmeans</a></li></ol></li><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">8.</strong> wavelets</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="wavelets/plan.html"><strong aria-hidden="true">8.1.</strong> plan</a></li><li class="chapter-item "><a href="wavelets/preliminary.html"><strong aria-hidden="true">8.2.</strong> preliminary</a></li><li class="chapter-item "><a href="wavelets/haar.html"><strong aria-hidden="true">8.3.</strong> haar wavelet</a></li><li class="chapter-item "><a href="wavelets/fourier.html"><strong aria-hidden="true">8.4.</strong> fourier analysis</a></li><li class="chapter-item "><a href="wavelets/uncertainty_principle.html"><strong aria-hidden="true">8.5.</strong> uncertainty principle</a></li><li class="chapter-item "><a href="wavelets/multiresolution.html"><strong aria-hidden="true">8.6.</strong> multiresolution</a></li></ol></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">9.</strong> models</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="llm/llm.html"><strong aria-hidden="true">9.1.</strong> llm</a></li><li class="chapter-item "><a href="llm/falcon.html"><strong aria-hidden="true">9.2.</strong> falcon</a></li><li class="chapter-item "><a href="llm/llama.html"><strong aria-hidden="true">9.3.</strong> llama</a></li><li class="chapter-item "><a href="llm/peft.html"><strong aria-hidden="true">9.4.</strong> peft</a></li><li class="chapter-item "><a href="llm/transformer.html"><strong aria-hidden="true">9.5.</strong> transformer</a></li><li class="chapter-item "><a href="llm/models.html"><strong aria-hidden="true">9.6.</strong> models</a></li></ol></li><li class="chapter-item "><a href="megatron/megatron.html"><strong aria-hidden="true">10.</strong> megatron</a></li><li class="chapter-item "><a href="deepspeed/deepspeed.html"><strong aria-hidden="true">11.</strong> deepspeed</a></li><li class="chapter-item "><a href="pytorch/overview.html"><strong aria-hidden="true">12.</strong> pytorch</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="pytorch/tensor.html"><strong aria-hidden="true">12.1.</strong> tensor</a></li><li class="chapter-item "><a href="pytorch/autograd.html"><strong aria-hidden="true">12.2.</strong> autograd</a></li><li class="chapter-item "><a href="pytorch/operator.html"><strong aria-hidden="true">12.3.</strong> operator</a></li><li class="chapter-item "><a href="pytorch/profiler.html"><strong aria-hidden="true">12.4.</strong> profiler</a></li><li class="chapter-item "><a href="pytorch/hook.html"><strong aria-hidden="true">12.5.</strong> hook</a></li><li class="chapter-item "><a href="pytorch/elastic.html"><strong aria-hidden="true">12.6.</strong> elastic</a></li><li class="chapter-item "><a href="pytorch/patch.html"><strong aria-hidden="true">12.7.</strong> patch</a></li><li class="chapter-item "><a href="pytorch/misc.html"><strong aria-hidden="true">12.8.</strong> misc</a></li></ol></li><li class="chapter-item "><a href="paddle/paddle.html"><strong aria-hidden="true">13.</strong> paddle</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="paddle/ps-code-overview.html"><strong aria-hidden="true">13.1.</strong> ps</a></li><li class="chapter-item "><a href="paddle/framework.html"><strong aria-hidden="true">13.2.</strong> framework</a></li><li class="chapter-item "><a href="paddle/cinn.html"><strong aria-hidden="true">13.3.</strong> cinn</a></li><li class="chapter-item "><a href="paddle/dataloader.html"><strong aria-hidden="true">13.4.</strong> dataloader</a></li></ol></li><li class="chapter-item "><a href="horovod/horovod.html"><strong aria-hidden="true">14.</strong> horovod</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="horovod/run.html"><strong aria-hidden="true">14.1.</strong> run</a></li><li class="chapter-item "><a href="horovod/workflow.html"><strong aria-hidden="true">14.2.</strong> workflow</a></li><li class="chapter-item "><a href="horovod/object.html"><strong aria-hidden="true">14.3.</strong> object</a></li><li class="chapter-item "><a href="horovod/develop.html"><strong aria-hidden="true">14.4.</strong> develop</a></li><li class="chapter-item "><a href="horovod/pytorch.html"><strong aria-hidden="true">14.5.</strong> pytorch</a></li><li class="chapter-item "><a href="horovod/tensorflow.html"><strong aria-hidden="true">14.6.</strong> tensorflow</a></li><li class="chapter-item "><a href="horovod/elastic.html"><strong aria-hidden="true">14.7.</strong> elastic</a></li></ol></li><li class="chapter-item "><a href="ray/ray.html"><strong aria-hidden="true">15.</strong> ray</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="ray/overview.html"><strong aria-hidden="true">15.1.</strong> overview</a></li><li class="chapter-item "><a href="ray/gcs.html"><strong aria-hidden="true">15.2.</strong> gcs</a></li><li class="chapter-item "><a href="ray/raylet.html"><strong aria-hidden="true">15.3.</strong> raylet</a></li><li class="chapter-item "><a href="ray/api.html"><strong aria-hidden="true">15.4.</strong> api</a></li><li class="chapter-item "><a href="ray/survey.html"><strong aria-hidden="true">15.5.</strong> survey</a></li></ol></li><li class="chapter-item "><a href="python/python.html"><strong aria-hidden="true">16.</strong> python</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="python/concurrent.html"><strong aria-hidden="true">16.1.</strong> concurrent execution</a></li><li class="chapter-item "><a href="python/multiprocessing.html"><strong aria-hidden="true">16.2.</strong> multiprocessing</a></li><li class="chapter-item "><a href="python/decorator.html"><strong aria-hidden="true">16.3.</strong> decorator</a></li></ol></li><li class="chapter-item "><a href="tips/tips.html"><strong aria-hidden="true">17.</strong> tips</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="tips/enable_shared_from_this.html"><strong aria-hidden="true">17.1.</strong> enable_shared_from_this</a></li><li class="chapter-item "><a href="tips/ip_local_port_range.html"><strong aria-hidden="true">17.2.</strong> ip_local_port_range</a></li><li class="chapter-item "><a href="tips/golang_error.html"><strong aria-hidden="true">17.3.</strong> golang error</a></li></ol></li><li class="chapter-item "><a href="infra/overview.html"><strong aria-hidden="true">18.</strong> infrastructure</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="infra/pki.html"><strong aria-hidden="true">18.1.</strong> pki</a></li><li class="chapter-item "><a href="infra/cache.html"><strong aria-hidden="true">18.2.</strong> linux cache</a></li></ol></li><li class="chapter-item "><a href="kubernetes/kubernetes.html"><strong aria-hidden="true">19.</strong> kubernetes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="kubernetes/concepts.html"><strong aria-hidden="true">19.1.</strong> concepts</a></li><li class="chapter-item "><a href="kubernetes/scheduler.html"><strong aria-hidden="true">19.2.</strong> scheduler</a></li><li class="chapter-item "><a href="kubernetes/operator.html"><strong aria-hidden="true">19.3.</strong> operator</a></li><li class="chapter-item "><a href="kubernetes/device_plugin.html"><strong aria-hidden="true">19.4.</strong> device plugin</a></li><li class="chapter-item "><a href="kubernetes/docker.html"><strong aria-hidden="true">19.5.</strong> docker</a></li><li class="chapter-item "><a href="kubernetes/install.html"><strong aria-hidden="true">19.6.</strong> install</a></li><li class="chapter-item "><a href="kubernetes/api_service.html"><strong aria-hidden="true">19.7.</strong> api-service</a></li><li class="chapter-item "><a href="kubernetes/controller.html"><strong aria-hidden="true">19.8.</strong> controller</a></li></ol></li><li class="chapter-item "><a href="nccl/nccl.html"><strong aria-hidden="true">20.</strong> nccl</a></li><li class="chapter-item "><a href="nvidia/nvidia.html"><strong aria-hidden="true">21.</strong> cuda</a></li><li class="chapter-item "><a href="somewhat/todo.html"><strong aria-hidden="true">22.</strong> todo</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="somewhat/gloo.html"><strong aria-hidden="true">22.1.</strong> gloo</a></li><li class="chapter-item "><a href="somewhat/mpi.html"><strong aria-hidden="true">22.2.</strong> mpi</a></li><li class="chapter-item "><a href="somewhat/jax.html"><strong aria-hidden="true">22.3.</strong> jax</a></li><li class="chapter-item "><a href="somewhat/tvm.html"><strong aria-hidden="true">22.4.</strong> tvm</a></li><li class="chapter-item "><a href="somewhat/github.html"><strong aria-hidden="true">22.5.</strong> llm</a></li></ol></li><li class="chapter-item "><a href="notes/index.html"><strong aria-hidden="true">23.</strong> notes</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="notes/influence_and_persuasion.html"><strong aria-hidden="true">23.1.</strong> influence and persuasion</a></li><li class="chapter-item "><a href="notes/feynman_technique.html"><strong aria-hidden="true">23.2.</strong> freynman technique</a></li><li class="chapter-item "><a href="notes/wavelet_tour_signal_processing_sparse.html"><strong aria-hidden="true">23.3.</strong> wavelet signal processing</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">Aller au boulot</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/kuizhiqing/aller-au-boulot" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="architecture"><a class="header" href="#architecture">architecture</a></h1>
<h2 id="online-serving"><a class="header" href="#online-serving">online serving</a></h2>
<p>服务启动命令</p>
<pre><code class="language-bash">vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
</code></pre>
<p>命令在 <code>pyproject.toml</code> 中定义</p>
<pre><code class="language-toml">[project.scripts]
vllm = "vllm.entrypoints.cli.main:main"
</code></pre>
<p>入口
<code>vllm/entrypoints/cli/main.py</code>
dispatch 到真实启动的入口
<code>vllm.entrypoints.cli.serve</code>.</p>
<pre><code class="language-python"># vllm/entrypoints/cli/serve.py

class ServeSubcommand(CLISubcommand):
    def __init__(self):
        self.name = "serve"

    @staticmethod
    def cmd(args: argparse.Namespace) -&gt; None:
        args.model = args.model_tag
        uvloop.run(run_server(args))

    def subparser_init(self, subparsers):
        serve_parser = subparsers.add_parser("serve", usage="vllm serve &lt;model_tag&gt; [options]")
        serve_parser.add_argument("model_tag", ...)
        serve_parser.add_argument("--config", ...) # YAML config file
        return make_arg_parser(serve_parser)
</code></pre>
<p>可以看到，<code>serve</code> 命令的入口是 <code>vllm.entrypoints.cli.serve.ServeSubcommand.cmd</code>，它调用 <code>vllm.entrypoints.cli.serve.run_server</code>，而 <code>run_server</code> 会创建 <code>uvicorn</code> 服务。</p>
<pre><code class="language-python"># vllm/entrypoints/openai/api_server.py

@asynccontextmanager
async def build_async_engine_client(args) -&gt; AsyncIterator[EngineClient]:
    engine_args = AsyncEngineArgs.from_cli_args(args)
    engine_client = AsyncLLMEngine.from_engine_args(engine_args, ...)
    yield engine_client

def build_app(args: Namespace) -&gt; FastAPI:
    app = FastAPI(lifespan=lifespan)
    app.include_router(router)
    return app

async def run_server(args, **uvicorn_kwargs) -&gt; None:
    async with build_async_engine_client(args) as engine_client:
        app = build_app(args)
        model_config = await engine_client.get_model_config()
        await init_app_state(engine_client, model_config, app.state, args)
        await serve_http(app, ...  **uvicorn_kwargs)


if __name__ == "__main__":
    parser = FlexibleArgumentParser(...)
    args = parser.parse_args()
    uvloop.run(run_server(args))
</code></pre>
<p><code>run_server</code> 是个异步函数，它完成了 2 个主要任务：</p>
<ul>
<li>创建异步 engine : <code>AsyncLLMEngine</code></li>
<li>创建 FastAPI 应用，承接 http 请求</li>
</ul>
<p>这里通过 <code>init_app_state</code> 初始化了 app.state，即把 vllm LLMEngine 设置给了 FastAPI。</p>
<p>其中 <code>serve_http</code> 通过 uvicorn 实现了 http 服务的启动。</p>
<pre><code class="language-python"># vllm/entrypoints/launcher.py

async def serve_http(app: FastAPI, sock: Optional[socket.socket], **uvicorn_kwargs: Any):
    config = uvicorn.Config(app, **uvicorn_kwargs)
    server = uvicorn.Server(config)
    loop = asyncio.get_running_loop()
    server_task = loop.create_task(
        server.serve(sockets=[sock] if sock else None))
</code></pre>
<p>同时可以看到 <code>vllm serve</code> 的等价启动方式是</p>
<pre><code class="language-bash">python3 -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
</code></pre>
<pre><code class="language-python"># vllm/entrypoints/openai/api_server.py

router = APIRouter()

@router.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest, raw_request: Request):
    handler = chat(raw_request)
    generator = await handler.create_chat_completion(request, raw_request)
    if isinstance(generator, ChatCompletionResponse):
        return JSONResponse(content=generator.model_dump())

    return StreamingResponse(content=generator, media_type="text/event-stream")

def chat(request: Request) -&gt; Optional[OpenAIServingChat]:
    return request.app.state.openai_serving_chat


async def init_app_state(engine_client, model_config, state, args):
    state.openai_serving_chat = OpenAIServingChat(
        engine_client, model_config, state.openai_serving_models, ...
    )
</code></pre>
<p>请求 <code>/v1/chat/completions</code> 接口时，会调用 <code>OpenAIServingChat</code> 的 <code>create_chat_completion</code> 方法。</p>
<pre><code class="language-python"># vllm/entrypoints/openai/serving_chat.py

class OpenAIServingChat(OpenAIServing):

    async def create_chat_completion(self, ChatCompletionRequest, Request):

        tokenizer = await self.engine_client.get_tokenizer(lora_request)

        (conversation, request_prompts, engine_prompts) = 
            await self._preprocess_chat(request, tokenizer, request.messages, ...)

        for i, engine_prompt in enumerate(engine_prompts):
            sampling_params = request.to_sampling_params(...)

            generator = self.engine_client.generate(engine_prompt, sampling_params, request_id,)

            generators.append(generator)

        result_generator, = generators

        if request.stream:
            return self.chat_completion_stream_generator(
                request, result_generator, request_id, model_name,
                conversation, tokenizer, request_metadata)

        return await self.chat_completion_full_generator(
            request, result_generator, request_id, model_name,
            conversation, tokenizer, request_metadata)

    async def chat_completion_full_generator(self, request, result_generator, ...):
        async for res in result_generator:
            final_res = res

        role = self.get_chat_request_role(request)

        for output in final_res.outputs:
            logprobs = self._create_chat_logprobs(...)

            message = ChatMessage(role=role, content=output.text)

            choice_data = ChatCompletionResponseChoice(
                index=output.index,
                message=message,
                logprobs=logprobs, ...)
            choices.append(choice_data)

        response = ChatCompletionResponse(
            choices=choices,
            prompt_logprobs=final_res.prompt_logprobs,
        )

        return response

    def _create_chat_logprobs(self, token_ids,) -&gt; ChatCompletionLogProbs:
        for i, token_id in enumerate(token_ids):
            step_top_logprobs = top_logprobs[i]
            token = tokenizer.decode(token_id)
            logprobs_content.append(ChatCompletionLogProbsContent(token=token,))

        return ChatCompletionLogProbs(content=logprobs_content)

</code></pre>
<blockquote>
<p>generate 返回的 generator 中的 <code>output.text</code> 是已经经过 tokenizer decode 的。</p>
</blockquote>
<p><code>create_chat_completion</code> 的主要逻辑是调用 engine 的 <code>generate</code> 方法，生成结果，
其中 prompt 由 <code>OpenAIServing._preprocess_chat</code> 进行预处理，包括 template 化和 tokenization.</p>
<p>注意 <code>request_prompt -&gt; prompt_inputs -&gt; engine_prompt</code> 的过程，engine_prompt 中包含了完整 prompt 信息。</p>
<p>最后根据是否 stream 请求，chat_completion_stream_generator 和 chat_completion_full_generator 返回不同的结果。</p>
<pre><code class="language-python"># vllm/entrypoints/openai/serving_engine.py

class OpenAIServing:

    async def _preprocess_chat(...):
        conversation, mm_data_future = parse_chat_messages_futures(...)

        request_prompt = apply_hf_chat_template(tokenizer, conversation=conversation,)

        prompt_inputs = await self._tokenize_prompt_input_async(request, tokenizer, request_prompt,)

        engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs["prompt_token_ids"])

        return conversation, [request_prompt], [engine_prompt]
</code></pre>
<p>https://platform.openai.com/docs/api-reference/chat/create</p>
<h2 id="offline-inference"><a class="header" href="#offline-inference">offline inference</a></h2>
<pre><code class="language-python">from vllm import LLM

prompts = "Hello, my name is"

llm = LLM(model="facebook/opt-125m")

outputs = llm.generate(prompts)
</code></pre>
<pre><code class="language-python"># vllm/__init__.py

from vllm.entrypoints.llm import LLM
</code></pre>
<p>LLM</p>
<pre><code class="language-python"># vllm/entrypoints/llm.py

class LLM:
    def __init__(self, model, ...) -&gt; None:
        worker_cls = kwargs["worker_cls"]
        engine_args = EngineArgs(model, task, tokenizer)
        self.engine_class = self.get_engine_class()
        self.llm_engine = self.engine_class.from_engine_args(
            engine_args, usage_context=UsageContext.LLM_CLASS)

    @staticmethod
    def get_engine_class() -&gt; Type[LLMEngine]:
        if envs.VLLM_USE_V1:
            from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine
            return V1LLMEngine
        return LLMEngine

    def get_tokenizer(self) -&gt; AnyTokenizer:
        return self.llm_engine.get_tokenizer_group(TokenizerGroup).tokenizer

    @overload
    def generate(self, prompts, sampling_params):
        outputs = self._run_engine(use_tqdm=use_tqdm)
        return self.engine_class.validate_outputs(outputs, RequestOutput)

    def collective_rpc(self, ...):
        executor = self.llm_engine.model_executor
        return executor.collective_rpc(method, timeout, args, kwargs)

    def apply_model(self, func: Callable[[nn.Module], _R]) -&gt; list[_R]:
        executor = self.llm_engine.model_executor
        return executor.apply_model(func)
</code></pre>
<h2 id="engine"><a class="header" href="#engine">Engine</a></h2>
<h2 id="asyncllmengine"><a class="header" href="#asyncllmengine">AsyncLLMEngine</a></h2>
<p>V1 版本的 AsyncLLMEngine 定义于 <code>vllm.v1.engine.async_llm</code> 的 AsyncLLM</p>
<pre><code class="language-python"># vllm/engine/async_llm_engine.py

if envs.VLLM_USE_V1:
    from vllm.v1.engine.async_llm import AsyncLLM
    AsyncLLMEngine = AsyncLLM
</code></pre>
<p>AsyncLLM 定义一个异步的 LLM，其关键代码如下，</p>
<pre><code class="language-python"># vllm/v1/engine/async_llm.py

class AsyncLLM(EngineClient):

    def __init__(self, vllm_config, executor_class):
        # Tokenizer (+ ensure liveness if running in another process).
        self.tokenizer = init_tokenizer_from_configs(...)
        self.tokenizer.ping()

        # Processor (converts Inputs --&gt; EngineCoreRequests).
        self.processor = Processor(..., tokenizer...)

        # OutputProcessor (converts EngineCoreOutputs --&gt; RequestOutput).
        self.output_processor = OutputProcessor(self.tokenizer, ...)

        # EngineCore (starts the engine in background process).
        self.engine_core = EngineCoreClient.make_client(..., vllm_config, executor_class, ...)

        self.output_handler: Optional[asyncio.Task] = None

    @classmethod
    def from_engine_args(cls, ...) -&gt; "AsyncLLM":
        executor_class = Executor.get_class(vllm_config)
        return cls(vllm_config, executor_class, ...)

    async def generate(self, prompt, sampling_params, ...) -&gt; AsyncGenerator[RequestOutput, None]:
        self.output_handler = asyncio.create_task(self._run_output_handler())

        q = await self.add_request(request_id, prompt, sampling_params, ...)

        while not finished:
            out = q.get_nowait() if not q.empty() else await q.get()
            yield out

    async def _run_output_handler(self):
        """Background loop: pulls from EngineCore and pushes to AsyncStreams."""

        while True:
            # 1) Pull EngineCoreOutputs from the EngineCore.
            outputs = await self.engine_core.get_output_async()

            slices = (outputs.outputs, )

            for i, outputs_slice in enumerate(slices):
                # 2) Process EngineCoreOutputs.
                processed_outputs = self.output_processor.process_outputs(
                    outputs_slice, outputs.timestamp, iteration_stats)

                # 3) Abort any reqs that finished due to stop strings.
                await self.engine_core.abort_requests_async(
                    processed_outputs.reqs_to_abort)

    async def add_request(self, request_id, prompt, params, ...) -&gt; asyncio.Queue[RequestOutput]:
        # 1) Create a new output queue for the request.
        queue: asyncio.Queue[RequestOutput] = asyncio.Queue()

        # 2) Convert Input --&gt; Request.
        request = self.processor.process_inputs(request_id, prompt, params, ...)

        # 3) Add the request to OutputProcessor (this process).
        self.output_processor.add_request(request, queue)

        # 4) Add the EngineCoreRequest to EngineCore (separate process).
        await self.engine_core.add_request_async(request)

        return queue
</code></pre>
<p>generate() 调用</p>
<pre><code>engine_core.get_output_async()
output_processor.process_outputs()
engine_core.abort_requests_async()

processor.process_inputs()
output_processor.add_request()
engine_core.add_request_async()
</code></pre>
<p>AsyncLLM 是一个异步的 LLM，系统的核心组件，它的初始化过程启动了主要组件：</p>
<ul>
<li>EngineCoreClient + Executor <a href="#executor">ref</a></li>
<li>Processor + Tokenizer</li>
<li>OutputProcessor + Tokenizer</li>
</ul>
<pre><code>"""
Main function called by the API server to kick off a request
    * 1) Making an AsyncStream corresponding to the Request.
    * 2) Processing the Input.
    * 3) Adding the Request to the Detokenizer.
    * 4) Adding the Request to the EngineCore (separate process).

A separate output_handler loop runs in a background AsyncIO task, 
pulling outputs from EngineCore and putting them into the 
per-request AsyncStream.

The caller of generate() iterates the returned AsyncGenerator,
returning the RequestOutput back to the caller.
"""
</code></pre>
<h2 id="enginecoreclient"><a class="header" href="#enginecoreclient">EngineCoreClient</a></h2>
<pre><code>engine_core.get_output_async()
engine_core.abort_requests_async()
engine_core.add_request_async()
</code></pre>
<pre><code class="language-python"># vllm/v1/engine/core_client.py

class EngineCoreClient(ABC):
    @staticmethod
    def make_client(..., vllm_config, executor_class, ...) -&gt; "EngineCoreClient":
        if multiprocess_mode and asyncio_mode:
            return AsyncMPClient(vllm_config, executor_class, log_stats)

        if multiprocess_mode and not asyncio_mode:
            return SyncMPClient(vllm_config, executor_class, log_stats)

        return InprocClient(vllm_config, executor_class, log_stats)


class InprocClient(EngineCoreClient):
    def __init__(self, *args, **kwargs):
        self.engine_core = EngineCore(*args, **kwargs)

    def get_output(self) -&gt; EngineCoreOutputs:
        return self.engine_core.step()

    def add_request(self, request: EngineCoreRequest) -&gt; None:
        self.engine_core.add_request(request)

    def abort_requests(self, request_ids: List[str]) -&gt; None:
        if len(request_ids) &gt; 0:
            self.engine_core.abort_requests(request_ids)


class MPClient(EngineCoreClient):
    def __init__(..., vllm_config, executor_class, ...):
        self.ctx =  zmq.asyncio.Context()
        self.output_socket = make_zmq_socket(self.ctx, output_path, zmq.constants.PULL)
        self.input_socket = make_zmq_socket(self.ctx, input_path, zmq.constants.PUSH)

        self.proc_handle = BackgroundProcHandle(
            input_path=input_path,
            output_path=output_path,
            target_fn=EngineCoreProc.run_engine_core,
            process_kwargs={"vllm_config": vllm_config, "executor_class": executor_class, })


class SyncMPClient(MPClient):
    ...

class AsyncMPClient(MPClient):
    ...
</code></pre>
<p>EngineCoreClient 的 3 种实现 EngineCore</p>
<ul>
<li>InprocClient: In process EngineCore (for V0-style LLMEngine use)</li>
<li>SyncMPClient: ZMQ + background proc EngineCore (for LLM)</li>
<li>AsyncMPClient: ZMQ + background proc EngineCore w/ asyncio (for AsyncLLM)</li>
</ul>
<p>核心实现逻辑由 BackgroundProcHandle 启动 <code>EngineCoreProc.run_engine_core</code> 来实现。</p>
<pre><code class="language-python">
class SyncMPClient(MPClient):
    def __init__(self, vllm_config, executor_class, ...):
        def process_outputs_socket():
            while True:
                (frame, ) = output_socket.recv_multipart(copy=False)
                outputs = decoder.decode(frame.buffer)
                outputs_queue.put_nowait(outputs)
        Thread(target=process_outputs_socket, daemon=True).start()

    def get_output(self) -&gt; EngineCoreOutputs:
        return self.outputs_queue.get()

    def _send_input(self, request_type, request) -&gt; None:
        msg = (request_type.value, self.encoder.encode(request))
        self.input_socket.send_multipart(msg, copy=False)

    def add_request(self, request: EngineCoreRequest) -&gt; None:
        self._send_input(EngineCoreRequestType.ADD, request)

    def abort_requests(self, request_ids: List[str]) -&gt; None:
        self._send_input(EngineCoreRequestType.ABORT, request_ids)
</code></pre>
<pre><code class="language-python">class AsyncMPClient(MPClient):
    async def _start_output_queue_task(self):
        self.outputs_queue = asyncio.Queue()

        async def process_outputs_socket():
            while True:
                (frame, ) = await output_socket.recv_multipart(copy=False)
                outputs: EngineCoreOutputs = decoder.decode(frame.buffer)
                outputs_queue.put_nowait(outputs)

        self.queue_task = asyncio.create_task(process_outputs_socket())

    async def get_output_async(self) -&gt; EngineCoreOutputs:
        await self._start_output_queue_task()
        return await self.outputs_queue.get()

    async def _send_input(request_type, request):
        msg = (request_type.value, self.encoder.encode(request))
        await self.input_socket.send_multipart(msg, copy=False)

    async def add_request_async(self, request: EngineCoreRequest) -&gt; None:
        await self._send_input(EngineCoreRequestType.ADD, request)

    async def abort_requests_async(self, request_ids: List[str]) -&gt; None:
        await self._send_input(EngineCoreRequestType.ABORT, request_ids)

</code></pre>
<p>SyncMPClient/AsyncMPClient 的实现</p>
<ol>
<li><code>add_request/add_request_async</code> 通过 input_socket 向 mq 中发送消息；</li>
<li>while 循环从 output_socket 中获取消息，放入 outputs_queue 中；</li>
<li><code>get_output/get_output_async</code> 从 outputs_queue 中得到消息；</li>
</ol>
<pre><code class="language-python"># vllm/v1/engine/core.py

class EngineCore:
    def __init__(self, vllm_config, executor_class):
        self.model_executor = executor_class(vllm_config)

        num_gpu_blocks, num_cpu_blocks = self._initialize_kv_caches(vllm_config)

        self.scheduler = Scheduler(scheduler_config, model_config, cache_config, ...)

        self.batch_queue_size = self.model_executor.max_concurrent_batches

        if self.batch_queue_size &gt; 1:
            self.batch_queue = queue.Queue(self.batch_queue_size)

    def _initialize_kv_caches(self, vllm_config: VllmConfig) -&gt; Tuple[int, int]:
        self.model_executor.initialize(kv_cache_configs)
        return num_gpu_blocks, num_cpu_blocks

    def add_request(self, request: EngineCoreRequest):
        req = Request.from_engine_core_request(request)

        self.scheduler.add_request(req)

    def abort_requests(self, request_ids: List[str]):
        self.scheduler.finish_requests(request_ids, RequestStatus.FINISHED_ABORTED)

    def step(self) -&gt; EngineCoreOutputs:
        scheduler_output = self.scheduler.schedule()
        output = self.model_executor.execute_model(scheduler_output)
        engine_core_outputs = self.scheduler.update_from_output(scheduler_output, output)
        return engine_core_outputs

    def step_with_batch_queue(self) -&gt; Optional[EngineCoreOutputs]:
        scheduler_output = self.scheduler.schedule()
        future = self.model_executor.execute_model(scheduler_output)
        self.batch_queue.put_nowait( (future, scheduler_output))

        future, scheduler_output = self.batch_queue.get(timeout=POLLING_TIMEOUT_S)
        model_output = future.result()
        self.batch_queue.task_done()
        engine_core_outputs = self.scheduler.update_from_output(scheduler_output, model_output)
        return engine_core_outputs


class EngineCoreProc(EngineCore):
    def __init__(self, ...):
        self.input_queue: queue.Queue[Tuple[EngineCoreRequestType, Any]] = queue.Queue()
        self.output_queue: queue.Queue[EngineCoreOutputs] = queue.Queue()

        threading.Thread(target=self.process_input_socket, args=(input_path, ), daemon=True).start()
        threading.Thread(target=self.process_output_socket, args=(output_path, ), daemon=True).start()

        ready_pipe.send({"status": "READY"})

    @staticmethod
    def run_engine_core(*args, **kwargs):
        engine_core = EngineCoreProc(*args, **kwargs)
        engine_core.run_busy_loop()

    def run_busy_loop(self):
        step_fn = (self.step if self.batch_queue is None else self.step_with_batch_queue)

        while True:
            req = self.input_queue.get_nowait()
            self._handle_client_request(*req)

            outputs = step_fn()
            self.output_queue.put_nowait(outputs)

    def _handle_client_request(self, request_type: EngineCoreRequestType, request: Any) -&gt; None:
        if request_type == EngineCoreRequestType.ADD:
            self.add_request(request)
        elif request_type == EngineCoreRequestType.ABORT:
            self.abort_requests(request)

    def process_input_socket(self, input_path: str):
        with zmq_socket_ctx(input_path, zmq.constants.PULL) as socket:
            while True:
                type_frame, data_frame = socket.recv_multipart(copy=False)
                request_type = EngineCoreRequestType(bytes(type_frame.buffer))
                request = decoder.decode(data_frame.buffer)

                self.input_queue.put_nowait((request_type, request))

    def process_output_socket(self, output_path: str):
        with zmq_socket_ctx(output_path, zmq.constants.PUSH) as socket:
            while True:
                outputs = self.output_queue.get()
                encoder.encode_into(outputs, buffer)
                socket.send_multipart((buffer, ), copy=False)
</code></pre>
<p><code>run_engine_core</code> 调用 run_busy_loop，启动 while 循环,</p>
<ol>
<li>从 input_queue 中获取 EngineCoreRequest</li>
<li>调用 _handle_client_request 将新请求放入 scheduler 中调度；</li>
<li>调用 step 处理请求: 调用 scheduler 的 schedule 获取调度信息，调用 executor 的 execute_model 处理请求；</li>
<li>最后将 EngineCoreOutputs 发送到 output_queue 中</li>
</ol>
<pre><code>    def step_with_batch_queue(self) -&gt; Optional[EngineCoreOutputs]:
        """Schedule and execute batches with the batch queue.
        Note that if nothing to output in this step, None is returned.

        The execution flow is as follows:
        1. Try to schedule a new batch if there are unscheduled requests
        and the job queue is not full. If a new batch is scheduled, directly
        return an empty engine core output. In other words, we won't check
        and return model outputs before the batch queue is full.
        2. If there is no new scheduled batch, meaning that the batch queue
        is full or no other requests can be scheduled, we block until the first
        batch in the job queue is finished.
        3. Update the scheduler from the output.
        """
</code></pre>
<pre><code>"""
MPClient: base client for multi-proc EngineCore.
    EngineCore runs in a background process busy loop, getting
    new EngineCoreRequests and returning EngineCoreOutputs

    * pushes EngineCoreRequests via input_socket
    * pulls EngineCoreOutputs via output_socket

    * AsyncMPClient subclass for AsyncLLM usage
    * SyncMPClient subclass for LLM usage
"""
</code></pre>
<h2 id="scheduler"><a class="header" href="#scheduler">Scheduler</a></h2>
<pre><code class="language-python"># vllm/v1/core/scheduler.py

class Scheduler:

    def __init__(self, scheduler_config, model_config, cache_config, ...):

        self.kv_cache_manager = KVCacheManager()

        self.requests: Dict[str, Request] = {} # req_id -&gt; Request
        self.waiting: Deque[Request] = deque()
        self.running: List[Request] = []
        self.finished_req_ids: Set[str] = set()

    def add_request(self, request: Request) -&gt; None:
        self.waiting.append(request)
        self.requests[request.request_id] = request

    def schedule(self) -&gt; "SchedulerOutput":
        num_scheduled_tokens: Dict[str, int] = {}
        token_budget = self.max_num_scheduled_tokens

        # First, schedule the RUNNING requests.
        req_index = 0
        while req_index &lt; len(self.running) and token_budget &gt; 0:
            request = self.running[req_index]

            num_new_tokens = request.num_tokens_with_spec - request.num_computed_tokens
            while True:
                new_blocks = self.kv_cache_manager.allocate_slots(request, num_new_tokens)
                if new_blocks is None:
                    preempted_req = self.running.pop()
                    self.kv_cache_manager.free(preempted_req)
                    self.waiting.appendleft(preempted_req)
                else:
                    can_schedule = True
                    break
            if not can_schedule:
                break

            scheduled_running_reqs.append(request)
            num_scheduled_tokens[request.request_id] = num_new_tokens
            token_budget -= num_new_tokens
            req_index += 1

            if request.spec_token_ids:
                scheduled_spec_decode_tokens[request.request_id] = (request.spec_token_ids)

        # Next, schedule the WAITING requests.
        if not preempted_reqs:
            while self.waiting and token_budget &gt; 0:
                request = self.waiting[0]

                computed_blocks, num_computed_tokens = self.kv_cache_manager.get_computed_blocks(request)

                num_new_tokens = request.num_tokens - num_computed_tokens

                new_blocks = self.kv_cache_manager.allocate_slots(request, num_new_tokens, computed_blocks)
                if new_blocks is None:
                    break

                self.waiting.popleft()
                self.running.append(request)
                if request.status == RequestStatus.WAITING:
                    scheduled_new_reqs.append(request)
                elif request.status == RequestStatus.PREEMPTED:
                    scheduled_resumed_reqs.append(request)

                num_scheduled_tokens[request.request_id] = num_new_tokens
                token_budget -= num_new_tokens
                request.status = RequestStatus.RUNNING
                request.num_computed_tokens = num_computed_tokens

        total_num_scheduled_tokens = sum(num_scheduled_tokens.values())

        new_reqs_data = [
            NewRequestData.from_request(req,...) for req in scheduled_new_reqs
        ]
        resumed_reqs_data = [ # CachedRequestData
            self._make_cached_request_data(...) for req in scheduled_resumed_reqs
        ]
        running_reqs_data = [ # CachedRequestData
            self._make_cached_request_data(...) for req in scheduled_running_reqs
        ]
        scheduler_output = SchedulerOutput(
            scheduled_new_reqs=new_reqs_data,
            scheduled_cached_reqs=resumed_reqs_data + running_reqs_data,
            num_scheduled_tokens=num_scheduled_tokens,
            total_num_scheduled_tokens=total_num_scheduled_tokens,
            scheduled_spec_decode_tokens=scheduled_spec_decode_tokens,
        )

        return scheduler_output


    def update_from_output(self, scheduler_output, model_runner_output) -&gt; EngineCoreOutputs:
        sampled_token_ids = model_runner_output.sampled_token_ids

        for request in self.running:
            req_id = request.request_id
            req_index = model_runner_output.req_id_to_index[req_id]
            generated_token_ids = sampled_token_ids[req_index]

            if request.num_computed_tokens &gt;= request.num_tokens:
                for output_token_id in generated_token_ids:
                    request.append_output_token_ids(output_token_id)
                    stopped = self._check_stop(request)
                    if stopped:
                        self._free_request(request)
                        break

            if not stopped:
                new_running.append(request)

        self.running = new_running
        return EngineCoreOutputs(outputs=outputs,...)
</code></pre>
<p>schedule 返回 <code>SchedulerOutput</code> 包含两个列表，</p>
<ul>
<li><code>scheduled_new_reqs</code> 来自 waiting queue，即处理新请求</li>
<li><code>scheduled_cached_reqs</code> 来自 running queue 和 resumed requests，即继续处理正在处理中的请求</li>
</ul>
<p>new_running: 由 <code>_check_stop</code> 实现</p>
<ul>
<li>num_tokens_scheduled == 0 : 本次未调度的</li>
<li>request.num_tokens &gt;= self.max_model_len</li>
<li>request.num_output_tokens &gt;= request.max_tokens</li>
<li>last_token_id == request.eos_token_id</li>
<li>last_token_id in sampling_params.stop_token_ids</li>
</ul>
<p>Executor 执行的结果通过 <code>update_from_output</code> 更新到 <code>request</code> 中，
具体而言，从 <code>model_runner_output.sampled_token_ids</code> 中取出 <code>request</code> 对应的  <code>output_token_ids</code>，
通过 <code>request.append_output_token_ids(output_token_id)</code> 更新到 <code>request</code> 中。</p>
<pre><code class="language-python"># vllm/v1/request.py 

class Request:
    def __init__(self, request_id, prompt, prompt_token_ids, ...):
        self.prompt = prompt
        self.prompt_token_ids = prompt_token_ids
        self.num_prompt_tokens = len(self.prompt_token_ids)
        self._output_token_ids: List[int] = []
        self._all_token_ids: List[int] = self.prompt_token_ids.copy()
        self.spec_token_ids: List[int] = []
        self.num_computed_tokens = 0

    @property
    def num_tokens(self) -&gt; int:
        return len(self._all_token_ids)

    @property
    def num_tokens_with_spec(self) -&gt; int:
        return len(self._all_token_ids) + len(self.spec_token_ids)

    def append_output_token_ids(self, token_ids) -&gt; None:
        self._output_token_ids.extend(token_ids)
        self._all_token_ids.extend(token_ids)
</code></pre>
<pre><code>    # NOTE(woosuk) on the scheduling algorithm:
    # There's no "decoding phase" nor "prefill phase" in the scheduler.
    # Each request just has the num_computed_tokens and
    # num_tokens_with_spec. num_tokens_with_spec =
    # len(prompt_token_ids) + len(output_token_ids) + len(spec_token_ids).
    # At each step, the scheduler tries to assign tokens to the requests
    # so that each request's num_computed_tokens can catch up its
    # num_tokens_with_spec. This is general enough to cover
    # chunked prefills, prefix caching, speculative decoding,
    # and the "jump decoding" optimization in the future.
</code></pre>
<h2 id="executor"><a class="header" href="#executor">Executor</a></h2>
<p>用法</p>
<pre><code class="language-python"># AsyncLLM
executor_class = Executor.get_class(vllm_config)

# EngineCore
self.model_executor = executor_class(vllm_config) 

self.model_executor.initialize(kv_cache_configs)

output = self.model_executor.execute_model(scheduler_output)
</code></pre>
<p>Excutor 是真正驱动进程执行 GPU 计算的模块。
从 EngineCore 的使用上可以看出，</p>
<ul>
<li>首先是选择具体的 Executor 类实现，进行默认初始化 <code>__init__</code></li>
<li>然后是 initialize 初始化 Executor</li>
<li>最后是 execute_model 执行模型返回结果</li>
</ul>
<h3 id="executorbase"><a class="header" href="#executorbase">ExecutorBase</a></h3>
<p>当 ExecutorBase 初始化的时候，会执行 _init_executor，这和 scheduler 中调用的 initialize 不是同一个函数。</p>
<ul>
<li><code>_init_executor</code> 根据不同是实现执行 <code>Worker</code> 的 <code>init_worker</code>, <code>init_device</code>, <code>load_model</code>.</li>
<li><code>initialize</code> 会执行 <code>Worker</code> 的 <code>initialize_cache</code>, <code>compile_or_warm_up_model</code>.</li>
</ul>
<p><code>execute_model</code> 根据不同的实现执行 Worker 的 <code>execute_model</code>.</p>
<pre><code class="language-python"># vllm/executor/executor_base.py

class ExecutorBase(ABC):
    def __init__(self, vllm_config: VllmConfig) -&gt; None:
        self._init_executor()

class DistributedExecutorBase(ExecutorBase):
    def execute_model(self, execute_model_req: ExecuteModelRequest,) -&gt; List[SamplerOutput]:
        if self.parallel_worker_tasks is None:
            self.parallel_worker_tasks = self._run_workers(
                "start_worker_execution_loop",
                async_run_tensor_parallel_workers_only=True)

        driver_outputs = self._driver_execute_model(execute_model_req)
        return driver_outputs

    def collective_rpc(self, ...):
        return self._run_workers(method, *args, **(kwargs or {}))
</code></pre>
<pre><code class="language-python"># vllm/v1/executor/abstract.py

class Executor(ExecutorBase):
    @staticmethod
    def get_class(vllm_config: VllmConfig) -&gt; Type["Executor"]:
        executor_class = RayDistributedExecutor
        executor_class = MultiprocExecutor
        executor_class = UniProcExecutor
        executor_class = ExecutorWithExternalLauncher
        return executor_class

    def initialize(self, kv_cache_configs: List[KVCacheConfig]) -&gt; None:
        self.collective_rpc("initialize_cache", args=(kv_cache_configs, ))
        self.collective_rpc("compile_or_warm_up_model")

    def execute_model(self, scheduler_output,
    ) -&gt; Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
        output = self.collective_rpc("execute_model", args=(scheduler_output, ))
        return output[0]

</code></pre>
<h3 id="raydistributedexecutor"><a class="header" href="#raydistributedexecutor">RayDistributedExecutor</a></h3>
<pre><code class="language-python"># vllm/v1/executor/ray_distributed_executor.py

class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
    def execute_model( self, scheduler_output,) -&gt; Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
        self.forward_dag = self._compiled_ray_dag(enable_asyncio=False)
        refs = self.forward_dag.execute(scheduler_output)
        return refs[0].get()

</code></pre>
<p>RayDistributedExecutorV0</p>
<pre><code class="language-python"># vllm/executor/ray_distributed_executor.py

# RayDistributedExecutorV0
class RayDistributedExecutor(DistributedExecutorBase):
    def _init_executor(self) -&gt; None:
        initialize_ray_cluster(self.parallel_config) # ray.init(...)
        self._init_workers_ray(placement_group)

    def _init_workers_ray(self, placement_group, **ray_remote_kwargs):

        bundle_indices = list(map(int, envs.VLLM_RAY_BUNDLE_INDICES.split(",")))
        for rank, bundle_id in enumerate(bundle_indices):
            scheduling_strategy = PlacementGroupSchedulingStrategy(placement_group, bundle_id,)

            worker = ray.remote(num_cpus=0, num_gpus, scheduling_strategy, **ray_remote_kwargs,
                )(RayWorkerWrapper).remote(vllm_config=self.vllm_config, rpc_rank=rank)
            worker_metadata.append(RayWorkerMetaData(worker=worker, created_rank=rank))

        self.workers = [item.worker for item in sorted_worker_metadata] # List[RayWorkerWrapper]

        self._run_workers("adjust_rank", rerank_mapping)
        self._run_workers("update_environment_variables", self._get_env_vars_to_be_updated())

        self._run_workers("init_worker", all_kwargs)
        self._run_workers("init_device")
        self._run_workers("load_model", max_concurrent_workers)

    def execute_model(self, execute_model_req: ExecuteModelRequest) -&gt; List[SamplerOutput]:
        outputs = ray.get(self.forward_dag.execute(execute_model_req))
        return outputs[0]

    def _run_workers(self, method, *args, **kwargs,) -&gt; Any:
        ray_worker_outputs = [worker.execute_method.remote(method, ...) for worker in ray_workers]

        if async_run_tensor_parallel_workers_only:
            return ray_worker_outputs

        ray_worker_outputs = ray.get(ray_worker_outputs)
        return ray_worker_outputs
</code></pre>
<p>V1 版本的 RayDistributedExecutor 继承 V0 版本的 RayDistributedExecutor，改写了 execute_model 方法，使用 DAG 的实现.</p>
<p>V0 版本的 RayDistributedExecutor 在 <code>__init__/_init_excutor</code> 中</p>
<ol>
<li>启动 ray 集群，然后再集群中启动 RayWorkerWrapper 实现的 worker</li>
<li>在 worker 中执行 <code>adjust_rank</code> 和 <code>update_environment_variables</code> 方法</li>
<li>在 worker 中执行 <code>init_worker</code>, init_device 和 <code>load_model</code> 方法</li>
</ol>
<p>RayDistributedExecutor 中在 worker 中执行的方法通过 <code>_run_workers</code> 方法提供，其中支持异步和同步的能力。
在 DistributedExecutorBase 中，<code>collective_rpc</code> 即等价于 <code>_run_workers</code> 方法。</p>
<h3 id="multiprocexecutor"><a class="header" href="#multiprocexecutor">MultiprocExecutor</a></h3>
<pre><code class="language-python"># vllm/v1/executor/multiproc_executor.py

class MultiprocExecutor(Executor):
	def _init_executor(self) -&gt; None:
        self.rpc_broadcast_mq = MessageQueue(self.world_size, self.world_size)
        scheduler_output_handle = self.rpc_broadcast_mq.export_handle()

        self.workers: List[WorkerProcHandle] = []
        for rank in range(self.world_size):
            worker = WorkerProc.make_worker_process(self.vllm_config, ...)
            self.workers.append(worker)

class WorkerProc:
    def __init__(self, vllm_config, local_rank, rank, ...):
		wrapper = WorkerWrapperBase(vllm_config=vllm_config, ...)
		wrapper.init_worker(all_kwargs)
		self.worker = wrapper.worker

		self.rpc_broadcast_mq = MessageQueue.create_from_handle(...)
		self.worker_response_mq = MessageQueue(1, 1)

		self.worker.init_device()
		self.worker.load_model()

    @staticmethod
    def make_worker_process(vllm_config, ...) -&gt; WorkerProcHandle:
        proc = context.Process(target=WorkerProc.worker_main, ..., daemon=True)
        proc.start()
        return WorkerProcHandle(proc, rank, ready_path, worker_response_mq)

    @staticmethod
    def worker_main(*args, **kwargs):
        try:
            worker = WorkerProc(*args, **kwargs)
            worker.worker_busy_loop()

    def worker_busy_loop(self):
        while True:
            method, args, kwargs = self.rpc_broadcast_mq.dequeue()
            func = getattr(self.worker, method)
            output = func(*args, **kwargs)
            self.worker_response_mq.enqueue((SUCCESS, output))
</code></pre>
<p>UniProcExecutor
ExecutorWithExternalLauncher</p>
<pre><code class="language-python"># vllm/executor/uniproc_executor.py

class UniProcExecutor(ExecutorBase):
    def _init_executor(self) -&gt; None:
        self.driver_worker = WorkerWrapperBase(vllm_config=self.vllm_config, rpc_rank=0)
        self.collective_rpc("init_worker", args=([kwargs], ))
        self.collective_rpc("init_device")
        self.collective_rpc("load_model")

    def collective_rpc(self, ...):
        answer = run_method(self.driver_worker, method, args, kwargs)
        return [answer]

class ExecutorWithExternalLauncher(UniProcExecutor):
</code></pre>
<p>WorkerWrapperBase</p>
<pre><code>"""
This class represents one process in an executor/engine. It is responsible
for lazily initializing the worker and handling the worker's lifecycle.
We first instantiate the WorkerWrapper, which remembers the worker module
and class name. Then, when we call `update_environment_variables`, and the
real initialization happens in `init_worker`.
"""

    """
    Initialize the worker wrapper with the given vllm_config and rpc_rank.
    Note: rpc_rank is the rank of the worker in the executor. In most cases,
    it is also the rank of the worker in the distributed group. However,
    when multiple executors work together, they can be different.
    e.g. in the case of SPMD-style offline inference with TP=2,
    users can launch 2 engines/executors, each with only 1 worker.
    All workers have rpc_rank=0, but they have different ranks in the TP
    group.
    """
</code></pre>
<pre><code class="language-python"># vllm/worker/worker_base.py

class WorkerWrapperBase:
    def __init__(self, vllm_config, rpc_rank) -&gt; None:
        self.worker: Optional[WorkerBase] = None
        init_cached_hf_modules()

    def init_worker(self, all_kwargs: List[Dict[str, Any]]) -&gt; None:
        worker_class = resolve_obj_by_qualname(self.vllm_config.parallel_config.worker_cls)
        worker_class = cloudpickle.loads(self.vllm_config.parallel_config.worker_cls)
        self.worker = worker_class(**kwargs)

    def execute_method(self, method: Union[str, bytes], *args, **kwargs):
        return run_method(target, method, args, kwargs)
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../vllm/overview.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="../somewhat/llama.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../vllm/overview.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="../somewhat/llama.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
